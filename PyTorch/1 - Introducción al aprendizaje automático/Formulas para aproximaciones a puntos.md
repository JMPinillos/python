# Formulas para aproximaciones a puntos

## Regresi贸n lineal

La **regresi贸n lineal** es una t茅cnica de modelado estad铆stico que se utiliza para describir la relaci贸n entre una variable dependiente yyy y una o m谩s variables independientes xxx. En la **regresi贸n lineal simple**, tenemos una variable dependiente y una sola variable independiente.

La ecuaci贸n general de una l铆nea recta es:
$$
y = w_1 \cdot x + w_2
$$
Donde:

- $y$ es la variable dependiente (el valor que queremos predecir).
- $x$ es la variable independiente (el valor que utilizamos para hacer la predicci贸n).
- $w_1$ es la pendiente de la l铆nea (indica cu谩nto cambia $y$ por cada cambio unitario en $x$).
- $w_2$ es la intersecci贸n en el eje $y$ (el valor de $y$ cuando $x = 0$).

El objetivo de la regresi贸n lineal es encontrar los valores de $w_1$ y $w_2$ que minimicen la diferencia entre los valores observados $y$ y los valores predichos por la l铆nea recta.



### Ejemplo de Regresi贸n Lineal Simple

Vamos a realizar una regresi贸n lineal simple usando un conjunto peque帽o de datos que describe la relaci贸n entre el n煤mero de horas de estudio y la calificaci贸n obtenida en un examen. Los datos son los siguientes:

| Horas de Estudio ($x$) | Calificaci贸n ($y$) |
| ---------------------- | ------------------ |
| 1                      | 2                  |
| 2                      | 3                  |
| 3                      | 4                  |
| 4                      | 5                  |
| 5                      | 6                  |

En este caso:
- $x$ es el n煤mero de horas de estudio (variable independiente).
- $y$ es la calificaci贸n obtenida en el examen (variable dependiente).



#### Paso 1: Calcular la pendiente $w_1$ y la intersecci贸n $w_2$

La f贸rmula para calcular la pendiente $w_1$ es:


$$
w_1 = \dfrac{n\sum xy - \sum x \sum y}{n\sum x^2 - (\sum x)^2}
$$
Donde:
- $n$ es el n煤mero de puntos de datos.
- $\sum xy$ es la suma del producto de cada $x$ y $y$.
- $\sum x$ es la suma de los valores de $x$.
- $\sum y$ es la suma de los valores de $y$.
- $\sum x^2$ es la suma de los cuadrados de los valores de $x$.



Calculemos cada uno de estos valores:

- $\sum x = 1 + 2 + 3 + 4 + 5 = 15$
- $\sum y = 2 + 3 + 4 + 5 + 6 = 20$
- $\sum xy = (1 \cdot 2) + (2 \cdot 3) + (3 \cdot 4) + (4 \cdot 5) + (5 \cdot 6) = 70$
- $\sum x^2 = 1^2 + 2^2 + 3^2 + 4^2 + 5^2 = 55$
- $n = 5$

Ahora sustituimos estos valores en la f贸rmula de $w_1$:

$w_1 = \dfrac{(5 \cdot 70) - (15 \cdot 20)}{(5  \cdot 55) - 15^2} = \dfrac{350 - 300}{275 - 225} = \dfrac{50}{50} = 1$

Por lo tanto, la pendiente $w_1 = 1$.



#### Paso 2: Calcular la intersecci贸n $w_2$

La f贸rmula para calcular la intersecci贸n $w_2$ es:

$$
w_2 = \dfrac{\sum y - w_1 \sum x}{n}
$$
Sustituyendo los valores que ya calculamos:

$w_2 = \dfrac{20 - (1 \cdot 15)}{5} = \dfrac{20 - 15}{5} = \dfrac{5}{5} = 1$

Por lo tanto, la intersecci贸n $w_2 = 1$.



#### Paso 3: Ecuaci贸n final de la recta

Sustituyendo los valores de $w_1$ y $w_2$ en la ecuaci贸n de la recta:

$$
y = 1x + 1 = x + 1
$$
Esta es la ecuaci贸n que describe la relaci贸n entre el n煤mero de horas de estudio y la calificaci贸n obtenida.



### Interpretaci贸n

La ecuaci贸n $y = 1x + 1$ nos dice que:
- Por cada hora adicional de estudio, la calificaci贸n aumenta en 1 punto.
- Si no se estudia $x = 0$, se espera que la calificaci贸n sea 1 (intersecci贸n).

Este es un ejemplo simple de c贸mo se puede calcular y aplicar una regresi贸n lineal para predecir resultados a partir de datos.



## Gradiente Descendente para la Regresi贸n Lineal (Truco Absoluto)

El **gradiente descendente** es un m茅todo de optimizaci贸n utilizado para ajustar los par谩metros de un modelo, en este caso, una l铆nea de regresi贸n lineal, con el objetivo de minimizar el error entre las predicciones y los datos observados. Este es un proceso iterativo en el que se modifican los par谩metros de la recta (la pendiente $w_1$ y la intersecci贸n $w_2$) en pasos peque帽os, determinados por una tasa de aprendizaje $\alpha$, hasta que la l铆nea se ajuste lo mejor posible a los puntos de datos.



### Elementos iniciales

- Tenemos un **punto** con coordenadas $(p, q)$.
- Una **l铆nea** que est谩 representada por la ecuaci贸n general de una recta:

$$
y = w_1 \cdot x + w_2
$$

Donde:

- $w_1$ es la pendiente de la l铆nea.
- $w_2$ es la intersecci贸n en el eje $y$.



El objetivo es ajustar la recta para que pase m谩s cerca del punto $(p, q)$. Para lograr esto, el truco absoluto realiza dos ajustes:
1. **Mover la l铆nea hacia arriba o hacia abajo** a帽adiendo un valor a la intersecci贸n con el eje $y$.
2. **Girar la l铆nea** ajustando su pendiente para acercarla m谩s al punto.



### Paso inicial (ajuste directo)

Imaginemos que para acercar la l铆nea al punto $(p, q)$, simplemente sumamos 1 a la intersecci贸n con el eje $y$ y $p$ a la pendiente. Esto da lugar a la siguiente nueva ecuaci贸n para la l铆nea:
$$
y = (w_1 + p) \cdot x + (w_2 + 1)
$$
Este ajuste hace que la l铆nea se acerque r谩pidamente al punto, pero el problema es que el cambio es **demasiado grande**, lo que puede resultar en una sobrecorrecci贸n.

### Introducci贸n de la tasa de aprendizaje $(\alpha)$

Para evitar grandes correcciones y mejorar el ajuste de la l铆nea, introducimos un peque帽o valor llamado **tasa de aprendizaje**, representado por $\alpha$. La tasa de aprendizaje nos permite hacer ajustes m谩s graduales a los par谩metros de la recta.

En lugar de sumar directamente 1 a la intersecci贸n y $p$ a la pendiente, ahora sumamos peque帽as fracciones de estos valores, multiplic谩ndolos por $\alpha$.

La nueva ecuaci贸n de la recta con tasa de aprendizaje es:

$$
y = (w_1 + p \cdot \alpha) \cdot x + (w_2 + \alpha)
$$



#### Caso cuando el punto est谩 por debajo de la l铆nea

Si el punto $(p, q)$ est谩 por **debajo** de la l铆nea, necesitamos mover la l铆nea hacia abajo. Esto se logra **restando** los t茅rminos en lugar de sumarlos. La ecuaci贸n para este caso es:

$$
y = (w_1 - p \cdot \alpha) \cdot x + (w_2 - \alpha)
$$


### Ejemplo

> Tenemos el punto $(5,15)$, la l铆nea $=2+3$ y una tasa de aprendizaje de $0,1$.



#### Primer punto: $(5, 15)$ y la l铆nea $y = 2x + 3$

1. **Ecuaci贸n inicial**:

   La l铆nea que tenemos inicialmente es $y = 2x + 3$. 
   Aqu铆, $w_1 = 2$ es la pendiente y $w_2 = 3$ es la intersecci贸n con el eje $y$.

   

2. **Actualizar la pendiente**:

   Usamos el valor $p = 5$ (la coordenada $x$ del punto $(5, 15)$) para actualizar la pendiente $w_1$.

   Seg煤n el gradiente descendente, multiplicamos $p$ por la tasa de aprendizaje $\alpha = 0.1$:
   $$
   \text{Nuevo } w_1 = w_1 + p \cdot \alpha = 2 + 5 \cdot 0,1 = 2 + 0,5 = 2,5
   $$
   

3. **Actualizar la intersecci贸n**:

   Sumamos $\alpha = 0.1$ a la intersecci贸n $w_2$:

   $$
   \text{Nuevo } w_2 = w_2 + \alpha = 3 + 0,1 = 3,1
   $$
   

4. **Nueva ecuaci贸n de la recta**:

   La nueva ecuaci贸n, despu茅s de las actualizaciones, es:

   $$
   y = 2,5x + 3,1
   $$



#### Segundo punto: $(-5, 15)$

Ahora usamos el punto $(-5, 15)$ para actualizar la l铆nea.

1. **Actualizar la pendiente**:

   Usamos el valor $p = -5$ (la coordenada $x$ del punto $(-5, 15)$). Multiplicamos $p$ por la tasa de aprendizaje $\alpha = 0.1$:

   $$
   \text{Nuevo } w_1 = w_1 + p \cdot \alpha = 2 + (-5) \cdot 0,1 = 2 - 0,5 = 1,5
   $$
   

2. **Actualizar la intersecci贸n**:

   Sumamos $\alpha = 0.1$ a la intersecci贸n $w_2$ para mover la l铆nea hacia arriba:

   $$
   \text{Nuevo } w_2 = w_2 - \alpha = 3 - 0,1 = 2,9
   $$
   

3. **Nueva ecuaci贸n de la recta**:

   La nueva ecuaci贸n, despu茅s de estas actualizaciones, es:

   $$
   y = 1,5x + 2,9
   $$



## Optimizaci贸n de m铆nimos cuadrados (Truco del cuadrado)

La **optimizaci贸n de m铆nimos cuadrados** es un m茅todo utilizado para ajustar los par谩metros de una l铆nea (pendiente $w_1$ e intersecci贸n $w_2$) con el objetivo de minimizar la **suma de los errores al cuadrado** entre los puntos observados y los valores predichos por la l铆nea. Este m茅todo tiene en cuenta no solo la direcci贸n del ajuste, sino tambi茅n la **magnitud de la diferencia** entre el punto y la l铆nea, lo que permite un ajuste proporcional a la distancia del punto a la l铆nea.



### Elementos iniciales:

- **Punto:** $(p, q)$
- **L铆nea:** Representada por la ecuaci贸n:

$$
y = w_1 \cdot x + w_2
$$



A diferencia del **truco absoluto**, que ajustaba la l铆nea de manera uniforme sin importar la distancia del punto a la l铆nea, el **truco del cuadrado** tiene en cuenta la distancia vertical entre el punto $(p, q)$ y la l铆nea, que es:
$$
q - q'
$$

Donde $q'$ es el valor predicho por la l铆nea para el punto $p$, es decir, $q' = w_1 \cdot p + w_2$.



### Regla de actualizaci贸n

Para ajustar la l铆nea, el **truco del cuadrado** sigue las siguientes reglas:

1. **Actualizar la intersecci贸n:** A帽adimos $\alpha \cdot (q - q')$ a la intersecci贸n $w_2$.

2. **Actualizar la pendiente:** A帽adimos $p \cdot \alpha \cdot (q - q')$ a la pendiente $w_1$.

Esto nos da la siguiente f贸rmula de actualizaci贸n para la recta ajustada:

$$
y = (w_1 + p \cdot (q - q') \cdot \alpha) \cdot x + (w_2 + (q - q') \cdot \alpha)
$$

Esta regla es 煤nica y se puede aplicar tanto si el punto est谩 por **encima** como si est谩 por **debajo** de la l铆nea, sin necesidad de diferentes f贸rmulas como en el truco absoluto.



### Ejemplo

> Digamos que tenemos el punto $(5,15)$, la l铆nea $=2+3$ y una tasa de aprendizaje de 0,01. 

Observa que esta tasa de aprendizaje es menor que el ejemplo que usamos para el truco absoluto.

- El valor de $q'$ para $p = 5$ se calcula como:

$$
q' = 2 \cdot 5 + 3 = 13
$$



- La distancia entre el punto y la l铆nea es:

$$
q - q' = 15 - 13 = 2
$$



1. **Actualizar la pendiente:** Usamos $p = 5$ y calculamos:

$$
\text{Nuevo } w_1 = w_1 + p \cdot (q - q') \cdot \alpha = 2 + 5 \cdot 2 \cdot 0,01 = 2 + 0,1 = 2,1
$$



2. **Actualizar la intersecci贸n:** A帽adimos $\alpha \cdot (q - q')$ a $w_2$:

$$
\text{Nuevo } w_2 = w_2 + (q - q') \cdot \alpha = 3 + 2 \cdot 0,01 = 3 + 0,02 = 3,02
$$



3. **Nueva ecuaci贸n de la recta:**

$$
y = 2,1x + 3,02
$$

