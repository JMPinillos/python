# Regresi칩n lineal

La **regresi칩n lineal** es una t칠cnica de modelado estad칤stico que se utiliza para describir la relaci칩n entre una variable dependiente yyy y una o m치s variables independientes xxx. En la **regresi칩n lineal simple**, tenemos una variable dependiente y una sola variable independiente.

La ecuaci칩n general de una l칤nea recta es:
$$
y = w_1 \cdot x + w_2
$$
Donde:

- $y$ es la variable dependiente (el valor que queremos predecir).
- $x$ es la variable independiente (el valor que utilizamos para hacer la predicci칩n).
- $w_1$ es la pendiente de la l칤nea (indica cu치nto cambia $y$ por cada cambio unitario en $x$).
- $w_2$ es la intersecci칩n en el eje $y$ (el valor de $y$ cuando $x = 0$).

El objetivo de la regresi칩n lineal es encontrar los valores de $w_1$ y $w_2$ que minimicen la diferencia entre los valores observados $y$ y los valores predichos por la l칤nea recta.



### Ejemplo de Regresi칩n Lineal Simple

Vamos a realizar una regresi칩n lineal simple usando un conjunto peque침o de datos que describe la relaci칩n entre el n칰mero de horas de estudio y la calificaci칩n obtenida en un examen. Los datos son los siguientes:

| Horas de Estudio ($x$) | Calificaci칩n ($y$) |
| ---------------------- | ------------------ |
| 1                      | 2                  |
| 2                      | 3                  |
| 3                      | 4                  |
| 4                      | 5                  |
| 5                      | 6                  |

En este caso:
- $x$ es el n칰mero de horas de estudio (variable independiente).
- $y$ es la calificaci칩n obtenida en el examen (variable dependiente).



#### Paso 1: Calcular la pendiente $w_1$ y la intersecci칩n $w_2$

La f칩rmula para calcular la pendiente $w_1$ es:


$$
w_1 = \dfrac{n\sum xy - \sum x \sum y}{n\sum x^2 - (\sum x)^2}
$$
Donde:
- $n$ es el n칰mero de puntos de datos.
- $\sum xy$ es la suma del producto de cada $x$ y $y$.
- $\sum x$ es la suma de los valores de $x$.
- $\sum y$ es la suma de los valores de $y$.
- $\sum x^2$ es la suma de los cuadrados de los valores de $x$.



Calculemos cada uno de estos valores:

- $\sum x = 1 + 2 + 3 + 4 + 5 = 15$
- $\sum y = 2 + 3 + 4 + 5 + 6 = 20$
- $\sum xy = (1 \cdot 2) + (2 \cdot 3) + (3 \cdot 4) + (4 \cdot 5) + (5 \cdot 6) = 70$
- $\sum x^2 = 1^2 + 2^2 + 3^2 + 4^2 + 5^2 = 55$
- $n = 5$

Ahora sustituimos estos valores en la f칩rmula de $w_1$:

$w_1 = \dfrac{(5 \cdot 70) - (15 \cdot 20)}{(5  \cdot 55) - 15^2} = \dfrac{350 - 300}{275 - 225} = \dfrac{50}{50} = 1$

Por lo tanto, la pendiente $w_1 = 1$.



#### Paso 2: Calcular la intersecci칩n $w_2$

La f칩rmula para calcular la intersecci칩n $w_2$ es:

$$
w_2 = \dfrac{\sum y - w_1 \sum x}{n}
$$
Sustituyendo los valores que ya calculamos:

$w_2 = \dfrac{20 - (1 \cdot 15)}{5} = \dfrac{20 - 15}{5} = \dfrac{5}{5} = 1$

Por lo tanto, la intersecci칩n $w_2 = 1$.



#### Paso 3: Ecuaci칩n final de la recta

Sustituyendo los valores de $w_1$ y $w_2$ en la ecuaci칩n de la recta:

$$
y = 1x + 1 = x + 1
$$
Esta es la ecuaci칩n que describe la relaci칩n entre el n칰mero de horas de estudio y la calificaci칩n obtenida.



### Interpretaci칩n

La ecuaci칩n $y = 1x + 1$ nos dice que:
- Por cada hora adicional de estudio, la calificaci칩n aumenta en 1 punto.
- Si no se estudia $x = 0$, se espera que la calificaci칩n sea 1 (intersecci칩n).

Este es un ejemplo simple de c칩mo se puede calcular y aplicar una regresi칩n lineal para predecir resultados a partir de datos.



## Gradiente Descendente para la Regresi칩n Lineal (Truco Absoluto)

El **gradiente descendente** es un m칠todo de optimizaci칩n utilizado para ajustar los par치metros de un modelo, en este caso, una l칤nea de regresi칩n lineal, con el objetivo de minimizar el error entre las predicciones y los datos observados. Este es un proceso iterativo en el que se modifican los par치metros de la recta (la pendiente $w_1$ y la intersecci칩n $w_2$) en pasos peque침os, determinados por una tasa de aprendizaje $\alpha$, hasta que la l칤nea se ajuste lo mejor posible a los puntos de datos.



### Elementos iniciales

- Tenemos un **punto** con coordenadas $(p, q)$.
- Una **l칤nea** que est치 representada por la ecuaci칩n general de una recta:

$$
y = w_1 \cdot x + w_2
$$

Donde:

- $w_1$ es la pendiente de la l칤nea.
- $w_2$ es la intersecci칩n en el eje $y$.



El objetivo es ajustar la recta para que pase m치s cerca del punto $(p, q)$. Para lograr esto, el truco absoluto realiza dos ajustes:
1. **Mover la l칤nea hacia arriba o hacia abajo** a침adiendo un valor a la intersecci칩n con el eje $y$.
2. **Girar la l칤nea** ajustando su pendiente para acercarla m치s al punto.



### Paso inicial (ajuste directo)

Imaginemos que para acercar la l칤nea al punto $(p, q)$, simplemente sumamos 1 a la intersecci칩n con el eje $y$ y $p$ a la pendiente. Esto da lugar a la siguiente nueva ecuaci칩n para la l칤nea:
$$
y = (w_1 + p) \cdot x + (w_2 + 1)
$$
Este ajuste hace que la l칤nea se acerque r치pidamente al punto, pero el problema es que el cambio es **demasiado grande**, lo que puede resultar en una sobrecorrecci칩n.

### Introducci칩n de la tasa de aprendizaje $(\alpha)$

Para evitar grandes correcciones y mejorar el ajuste de la l칤nea, introducimos un peque침o valor llamado **tasa de aprendizaje**, representado por $\alpha$. La tasa de aprendizaje nos permite hacer ajustes m치s graduales a los par치metros de la recta.

En lugar de sumar directamente 1 a la intersecci칩n y $p$ a la pendiente, ahora sumamos peque침as fracciones de estos valores, multiplic치ndolos por $\alpha$.

La nueva ecuaci칩n de la recta con tasa de aprendizaje es:

$$
y = (w_1 + p \cdot \alpha) \cdot x + (w_2 + \alpha)
$$



#### Caso cuando el punto est치 por debajo de la l칤nea

Si el punto $(p, q)$ est치 por **debajo** de la l칤nea, necesitamos mover la l칤nea hacia abajo. Esto se logra **restando** los t칠rminos en lugar de sumarlos. La ecuaci칩n para este caso es:

$$
y = (w_1 - p \cdot \alpha) \cdot x + (w_2 - \alpha)
$$


### Ejemplo

> Tenemos el punto $(5,15)$, la l칤nea $洧녽=2洧논+3$ y una tasa de aprendizaje de $0,1$.



#### Primer punto: $(5, 15)$ y la l칤nea $y = 2x + 3$

1. **Ecuaci칩n inicial**:

   La l칤nea que tenemos inicialmente es $y = 2x + 3$. 
   Aqu칤, $w_1 = 2$ es la pendiente y $w_2 = 3$ es la intersecci칩n con el eje $y$.

   

2. **Actualizar la pendiente**:

   Usamos el valor $p = 5$ (la coordenada $x$ del punto $(5, 15)$) para actualizar la pendiente $w_1$.

   Seg칰n el gradiente descendente, multiplicamos $p$ por la tasa de aprendizaje $\alpha = 0.1$:
   $$
   \text{Nuevo } w_1 = w_1 + p \cdot \alpha = 2 + 5 \cdot 0,1 = 2 + 0,5 = 2,5
   $$
   
3. **Actualizar la intersecci칩n**:

   Sumamos $\alpha = 0.1$ a la intersecci칩n $w_2$:

   $$
   \text{Nuevo } w_2 = w_2 + \alpha = 3 + 0,1 = 3,1
   $$
   
4. **Nueva ecuaci칩n de la recta**:

   La nueva ecuaci칩n, despu칠s de las actualizaciones, es:

   $$
   y = 2,5x + 3,1
   $$



#### Segundo punto: $(-5, 15)$

Ahora usamos el punto $(-5, 15)$ para actualizar la l칤nea.

1. **Actualizar la pendiente**:

   Usamos el valor $p = -5$ (la coordenada $x$ del punto $(-5, 15)$). Multiplicamos $p$ por la tasa de aprendizaje $\alpha = 0.1$:

   $$
   \text{Nuevo } w_1 = w_1 + p \cdot \alpha = 2 + (-5) \cdot 0,1 = 2 - 0,5 = 1,5
   $$
   
2. **Actualizar la intersecci칩n**:

   Sumamos $\alpha = 0.1$ a la intersecci칩n $w_2$ para mover la l칤nea hacia arriba:

   $$
   \text{Nuevo } w_2 = w_2 - \alpha = 3 - 0,1 = 2,9
   $$
   
3. **Nueva ecuaci칩n de la recta**:

   La nueva ecuaci칩n, despu칠s de estas actualizaciones, es:

   $$
   y = 1,5x + 2,9
   $$



## Optimizaci칩n de m칤nimos cuadrados (Truco del cuadrado)

La **optimizaci칩n de m칤nimos cuadrados** es un m칠todo utilizado para ajustar los par치metros de una l칤nea (pendiente $w_1$ e intersecci칩n $w_2$) con el objetivo de minimizar la **suma de los errores al cuadrado** entre los puntos observados y los valores predichos por la l칤nea. Este m칠todo tiene en cuenta no solo la direcci칩n del ajuste, sino tambi칠n la **magnitud de la diferencia** entre el punto y la l칤nea, lo que permite un ajuste proporcional a la distancia del punto a la l칤nea.



### Elementos iniciales:

- **Punto:** $(p, q)$
- **L칤nea:** Representada por la ecuaci칩n:

$$
y = w_1 \cdot x + w_2
$$



A diferencia del **truco absoluto**, que ajustaba la l칤nea de manera uniforme sin importar la distancia del punto a la l칤nea, el **truco del cuadrado** tiene en cuenta la distancia vertical entre el punto $(p, q)$ y la l칤nea, que es:
$$
q - q'
$$

Donde $q'$ es el valor predicho por la l칤nea para el punto $p$, es decir, $q' = w_1 \cdot p + w_2$.



### Regla de actualizaci칩n

Para ajustar la l칤nea, el **truco del cuadrado** sigue las siguientes reglas:

1. **Actualizar la intersecci칩n:** A침adimos $\alpha \cdot (q - q')$ a la intersecci칩n $w_2$.

2. **Actualizar la pendiente:** A침adimos $p \cdot \alpha \cdot (q - q')$ a la pendiente $w_1$.

Esto nos da la siguiente f칩rmula de actualizaci칩n para la recta ajustada:

$$
y = (w_1 + p \cdot (q - q') \cdot \alpha) \cdot x + (w_2 + (q - q') \cdot \alpha)
$$

Esta regla es 칰nica y se puede aplicar tanto si el punto est치 por **encima** como si est치 por **debajo** de la l칤nea, sin necesidad de diferentes f칩rmulas como en el truco absoluto.



### Ejemplo

> Digamos que tenemos el punto $(5,15)$, la l칤nea $洧녽=2洧논+3$ y una tasa de aprendizaje de 0,01. 

Observa que esta tasa de aprendizaje es menor que el ejemplo que usamos para el truco absoluto.

- El valor de $q'$ para $p = 5$ se calcula como:

$$
q' = 2 \cdot 5 + 3 = 13
$$



- La distancia entre el punto y la l칤nea es:

$$
q - q' = 15 - 13 = 2
$$



1. **Actualizar la pendiente:** Usamos $p = 5$ y calculamos:

$$
\text{Nuevo } w_1 = w_1 + p \cdot (q - q') \cdot \alpha = 2 + 5 \cdot 2 \cdot 0,01 = 2 + 0,1 = 2,1
$$



2. **Actualizar la intersecci칩n:** A침adimos $\alpha \cdot (q - q')$ a $w_2$:

$$
\text{Nuevo } w_2 = w_2 + (q - q') \cdot \alpha = 3 + 2 \cdot 0,01 = 3 + 0,02 = 3,02
$$



3. **Nueva ecuaci칩n de la recta:**

$$
y = 2,1x + 3,02
$$



## Descenso de Gradiente 

El **descenso de gradiente** es un m칠todo de optimizaci칩n utilizado para ajustar los par치metros de un modelo, en este caso, una l칤nea de regresi칩n, con el objetivo de minimizar el **error** entre las predicciones y los datos observados. En otras palabras, el algoritmo busca la mejor soluci칩n para una funci칩n ajustando gradualmente los par치metros (como la pendiente y la intersecci칩n de una recta) en pasos peque침os, llamados **tasa de aprendizaje** ($\alpha$), en la direcci칩n que reduce el error.



### Proceso paso a paso 

1. **Dibuja una l칤nea inicial aleatoria.**
2. **Calcula el error** entre la l칤nea y los puntos observados. Este error es una medida de qu칠 tan lejos est치n los puntos de la l칤nea, como el **error cuadr치tico medio**:

$$
  L(w_1, w_2) = \frac{1}{2n} \sum_{i=1}^{n} (y_i - (w_1 \cdot x_i + w_2))^2
$$



3. **Ajusta los par치metros** de la l칤nea moviendo la pendiente $w_1$ y la intersecci칩n $w_2$ en la direcci칩n que reduce el error. Esto se hace usando las derivadas parciales del error:

   - **Actualizaci칩n de la pendiente**:
     $$
        w_1 = w_1 - \alpha \cdot \frac{\partial L}{\partial w_1}
     $$

   - **Actualizaci칩n de la intersecci칩n**:
     $$
     w_2 = w_2 - \alpha \cdot \frac{\partial L}{\partial w_2}
     $$

 

4. **Repite el proceso**: en cada iteraci칩n, los par치metros se ajustan de manera que se reduce el error hasta que no haya mejoras significativas.

 

### Ejemplo

Supongamos que tenemos los puntos $(1, 2), (2, 3), (3, 4)$ y una l칤nea inicial aleatoria $y = 0.5x + 1$. Vamos a aplicar el descenso de gradiente:

1. **Calculamos el error inicial** usando la l칤nea aleatoria y el error cuadr치tico medio.
2. **Ajustamos la pendiente $w_1$ y la intersecci칩n $w_2$**. Despu칠s de calcular el gradiente, determinamos que debemos aumentar la pendiente y reducir la intersecci칩n.
3. **Recalculamos el error** y seguimos ajustando, movi칠ndonos en la direcci칩n que minimiza el error.
4. **Repetimos el proceso** hasta que la l칤nea se ajuste a los puntos. El modelo final podr칤a ser algo cercano a $y = 1x + 1$, que ajusta los puntos mejor.

 

### Aplicaci칩n del Algoritmo 

El descenso de gradiente permite encontrar la **mejor l칤nea** que se ajusta a los datos mediante ajustes iterativos, reduciendo gradualmente el error en cada paso. Es un m칠todo fundamental en la optimizaci칩n de modelos de aprendizaje autom치tico.

 



## Error Absoluto Medio (EAM) 

El **error absoluto medio** (EAM) es una m칠trica utilizada en la regresi칩n lineal para medir qu칠 tan lejos est치n las predicciones de los valores reales. En lugar de considerar los errores al cuadrado (como en el error cuadr치tico medio), el EAM toma la **diferencia absoluta** entre las predicciones y los valores observados, y luego hace el promedio de estas diferencias.

 

### F칩rmula del Error Absoluto Medio

Dado un conjunto de datos con puntos $(x_i, y_i)$ y una l칤nea que representa las predicciones $\hat{y}$, el **error absoluto medio** se calcula de la siguiente forma:
$$
\text{Error absoluto medio} = \frac{1}{m} \sum_{i=1}^{m} | y_i - \hat{y}_i |
$$
 Donde:

- $y_i$ son los valores reales de los puntos.
- $\hat{y}_i$ son las predicciones de la l칤nea.
- $m$ es el n칰mero total de puntos.



El **error absoluto medio** mide qu칠 tan lejos est치n, en promedio, las predicciones de los valores reales. Al usar el valor absoluto, se asegura que las diferencias negativas no cancelen las positivas, lo que har칤a que la m칠trica no reflejara correctamente el error total.

 

### <strong style="color:green">Ventajas del Error Absoluto Medio</strong>

- **F치cil de interpretar**: Es intuitivo porque mide el error en las mismas unidades que los datos originales.
- **No penaliza los grandes errores de manera tan fuerte** como el error cuadr치tico medio, lo cual puede ser una ventaja cuando no queremos que los grandes errores tengan demasiado peso.

 

### Ejemplo

> Calcule el **error absoluto medio** para la siguiente l칤nea y puntos:
>
> - **L칤nea**: $y=1,2洧논+2$
> - **Puntos**: $(2, -2)$, $(5, 6)$, $(-4, -4)$, $(-7, 1)$, $(8, 14)$



#### Paso 1: Calcular las predicciones $ \hat{y} $

Para cada punto, vamos a calcular el valor de $ \hat{y} $ usando la ecuaci칩n de la recta $ y = 1,2x + 2 $.

1. Para el punto $ (2, -2) $:

   $ \hat{y} = 1,2 \cdot 2 + 2 = 2,4 + 2 = 4,4 $

2. Para el punto $ (5, 6) $:

   $ \hat{y} = 1,2 \cdot 5 + 2 = 6 + 2 = 8 $

3. Para el punto $ (-4, -4) $:

   $ \hat{y} = 1,2 \cdot (-4) + 2 = -4,8 + 2 = -2,8 $

4. Para el punto $ (-7, 1) $:

   $ \hat{y} = 1,2 \cdot (-7) + 2 = -8,4 + 2 = -6,4 $

5. Para el punto $ (8, 14) $:

   $ \hat{y} = 1,2 \cdot 8 + 2 = 9,6 + 2 = 11,6 $



#### Paso 2: Calcular los errores absolutos

Ahora calculamos el error absoluto entre los valores reales $ y $ y las predicciones $ \hat{y} $ para cada punto:

1. Para el punto $ (2, -2) $:

   $ | y - \hat{y} | = | -2 - 4,4 | = | -6,4 | = 6,4 $

2. Para el punto $ (5, 6) $:

   $ | y - \hat{y} | = | 6 - 8 | = | -2 | = 2 $

3. Para el punto $ (-4, -4) $:

   $ | y - \hat{y} | = | -4 - (-2,8) | = | -4 + 2,8 | = | -1,2 | = 1,2 $

4. Para el punto $ (-7, 1) $:

   $ | y - \hat{y} | = | 1 - (-6,4) | = | 1 + 6,4 | = | 7,4 | = 7,4 $

5. Para el punto $ (8, 14) $:

   $ | y - \hat{y} | = | 14 - 11,6 | = | 2,4 | = 2,4 $

 

#### Paso 3: Calcular el error absoluto medio (EAM)

El error absoluto medio se calcula promediando todos los errores absolutos:
$$
\text{EAM} = \dfrac{6,4 + 2 + 1,2 + 7,4 + 2,4}{5} = \dfrac{19,4}{5} = 3,88
$$


Este m칠todo de error es simple y efectivo, pero al no penalizar tanto los grandes errores como el error cuadr치tico medio, puede ser m치s adecuado para ciertos tipos de problemas en los que se busca un ajuste general m치s que uno extremadamente preciso.

 

## Error Cuadr치tico Medio (ECM) 

 El **error cuadr치tico medio** (ECM) es una m칠trica utilizada en la regresi칩n lineal para medir qu칠 tan lejos est치n las predicciones de los valores reales. A diferencia del **error absoluto medio**, el ECM toma la diferencia entre la predicci칩n y el valor observado, la eleva al cuadrado y luego calcula el promedio de esos valores cuadrados.

 

### F칩rmula del Error Cuadr치tico Medio

Dado un conjunto de datos con puntos $(x_i, y_i)$ y una l칤nea que representa las predicciones $\hat{y}_i$, el error cuadr치tico medio se calcula de la siguiente manera:
$$
\text{Error\ cuadr치tico\ medio} = \frac{1}{m} \sum_{i=1}^{m} (y_i - \hat{y}_i)^2
$$
Donde:

- $y_i$ son los valores reales de los puntos.
- $\hat{y}_i$ son las predicciones de la l칤nea.
- $m$ es el n칰mero total de puntos.

 

El **ECM** mide el error al cuadrado, lo que significa que **penaliza m치s los errores grandes**. Esta m칠trica es 칰til porque amplifica los errores grandes, ayudando a detectar desviaciones importantes en las predicciones. Como los errores se elevan al cuadrado, el ECM nunca ser치 negativo y siempre ser치 cero cuando las predicciones sean perfectas.



### Ejemplo

> Calcule el **error absoluto medio** para la siguiente l칤nea y puntos:
>
> - **L칤nea**: $y=1,2洧논+2$
> - **Puntos**: $(2, -2)$, $(5, 6)$, $(-4, -4)$, $(-7, 1)$, $(8, 14)$



#### Paso 1: Calcular las predicciones $ \hat{y} $

Para cada punto, vamos a calcular el valor de $ \hat{y} $ usando la ecuaci칩n de la recta $ y = 1,2x + 2 $.

1. Para el punto $ (2, -2) $:

   $ \hat{y} = 1,2 \cdot 2 + 2 = 2,4 + 2 = 4,4 $

2. Para el punto $ (5, 6) $:

   $ \hat{y} = 1,2 \cdot 5 + 2 = 6 + 2 = 8 $

3. Para el punto $ (-4, -4) $:

   $ \hat{y} = 1,2 \cdot (-4) + 2 = -4,8 + 2 = -2,8 $

4. Para el punto $ (-7, 1) $:

   $ \hat{y} = 1,2 \cdot (-7) + 2 = -8,4 + 2 = -6,4 $

5. Para el punto $ (8, 14) $:

   $ \hat{y} = 1,2 \cdot 8 + 2 = 9,6 + 2 = 11,6 $



#### Paso 2: Calcular los errores cuadrados

Ahora calculamos el error cuadr치tico entre los valores reales $ y $ y las predicciones $ \hat{y} $ para cada punto:

1. Para el punto $ (2, -2) $:

   $ (y - \hat{y})^2 = (-2 - 4,4)^2 = (-6,4)^2 = 40,96 $

2. Para el punto $ (5, 6) $:

   $ (y - \hat{y})^2 = (6 - 8)^2 = (-2)^2 = 4 $

3. Para el punto $ (-4, -4) $:

   $ (y - \hat{y})^2 = (-4 - (-2,8))^2 = (-1,2)^2 = 1,44 $

4. Para el punto $ (-7, 1) $:

   $ (y - \hat{y})^2 = (1 - (-6,4))^2 = (7,4)^2 = 54,76 $

5. Para el punto $ (8, 14) $:

   $ (y - \hat{y})^2 = (14 - 11,6)^2 = (2,4)^2 = 5,76 $

 

#### Paso 3: Calcular el error cuadr치tico medio (ECM)

El error cuadr치tico medio se calcula promediando todos los errores cuadrados:
$$
\text{ECM} = \dfrac{40,96 + 4 + 1,44 + 54,76 + 5,76}{5} = \dfrac{106,92}{5} = 21,384
$$


A diferencia del **error absoluto medio (EAM)**, que toma la distancia absoluta entre las predicciones y los valores reales, el ECM amplifica los errores m치s grandes, lo que puede ser 칰til cuando se desea penalizar m치s las predicciones err칩neas.
