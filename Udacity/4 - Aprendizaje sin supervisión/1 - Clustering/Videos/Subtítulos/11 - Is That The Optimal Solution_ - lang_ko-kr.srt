1

00:00:00,000  -->  00:00:05,500
k-평균 알고리즘의

2

00:00:05,500  -->  00:00:10,330
작동 방식에 대해 지금 알고 있는 내용에서 몇 가지 사항에 대해 이야기해야 합니다.

3

00:00:10,330  -->  00:00:14,179
여전히 고려해야 할 두 가지 관심 영역이 있습니다.

4

00:00:14,179  -->  00:00:16,370
먼저 이런 생각이 들 수 있습니다.

5

00:00:16,370  -->  00:00:19,480
"알고리즘의 임의 시작점이 중요한가요?"라고 말이죠.

6

00:00:19,480  -->  00:00:21,830
그 대답은 '예'입니다.

7

00:00:21,829  -->  00:00:25,539
둘째, 시작점이 어디에 있든 클러스터의 종료 결과가

8

00:00:25,539  -->  00:00:29,320
동일하다고 확신할 수 있을까요?

9

00:00:29,320  -->  00:00:31,260
그 대답은 '아니오'입니다.

10

00:00:31,260  -->  00:00:34,929
아쉽게도 임의 시작점에 따라

11

00:00:34,929  -->  00:00:37,750
클러스터의 종료 결과가 다를 수 있습니다.

12

00:00:37,750  -->  00:00:40,890
시작 값 세트가 데이터의 동일한 그룹화로

13

00:00:40,890  -->  00:00:44,100
끝날 것이라고 보장할 수 없기 때문에

14

00:00:44,100  -->  00:00:48,870
실제로 최상의 데이터 클러스터링을 찾는 일반적인 방법은 k-평균 알고리즘을

15

00:00:48,869  -->  00:00:54,119
다른 출발점에 대해 각각 여러 번 실행하는 것입니다.

16

00:00:54,119  -->  00:00:57,079
사실, scikit-learn이 이 작업을 수행합니다.

17

00:00:57,079  -->  00:01:01,570
무작위 시작점은 서로 실질적으로 달라야 합니다.

18

00:01:01,570  -->  00:01:06,704
우리는 각 시도에서 거의 똑같은 시작점을 원하지 않습니다.

19

00:01:06,704  -->  00:01:11,664
K-평균의 각 실행에 대해 여전히 동일한 그룹화로 끝난다면

20

00:01:11,665  -->  00:01:16,025
그 그룹화에 대해 확신을 가질 수 있기 때문에 좋습니다.

21

00:01:16,025  -->  00:01:20,130
또는 시작점을 변경할 때 다른 그룹화가 있는 경우

22

00:01:20,129  -->  00:01:25,064
최상의 그룹화를 선택해야 합니다.

23

00:01:25,064  -->  00:01:28,920
설정된 클러스터 수 k에 대한 최상의 그룹화는

24

00:01:28,920  -->  00:01:32,234
그룹화 또는 점에서 중심까지의 평균 거리로 정의되며

25

00:01:32,234  -->  00:01:36,359
가장 작습니다.
