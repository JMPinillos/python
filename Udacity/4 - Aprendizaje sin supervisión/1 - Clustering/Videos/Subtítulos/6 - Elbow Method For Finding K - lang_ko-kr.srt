1

00:00:00,000  -->  00:00:07,214
이제 점들의 클러스터링을 명확하게

2

00:00:07,214  -->  00:00:09,300
볼 수 있을 때와 클러스터 수에 대한 선입견이 있을 때

3

00:00:09,300  -->  00:00:12,530
데이터세트에서 클러스터 수를

4

00:00:12,529  -->  00:00:17,884
식별하는 연습을 했습니다.

5

00:00:17,885  -->  00:00:22,365
하지만 데이터에 얼마나 많은 클러스터가 있는지 모를 때는 어떻게 해야 할까요?

6

00:00:22,364  -->  00:00:26,509
이러한 경우 k-평균의 k를 어떻게 결정할 수 있을까요?

7

00:00:26,510  -->  00:00:31,100
이를 위해 엘보우 방법이라는 방법을 소개하려고 합니다.

8

00:00:31,100  -->  00:00:34,679
이것의 작동 원리를 소개하자면

9

00:00:34,679  -->  00:00:38,189
모든 데이터세트에 대해 서로 다른 k 수로

10

00:00:38,189  -->  00:00:41,250
k-평균을 맞춘 후 각 k 값에 대해

11

00:00:41,250  -->  00:00:46,210
클러스터 중심점에서 각 지점의 평균 거리를 측정할 수 있다는 것입니다.

12

00:00:46,210  -->  00:00:50,490
k의 각 값에 대한 평균 거리를 추적하면

13

00:00:50,490  -->  00:00:52,825
다음과 같은 플롯을 만들 수 있습니다.

14

00:00:52,825  -->  00:00:58,315
x축에는 클러스터 수 또는 k가 있음을 알 수 있습니다.

15

00:00:58,314  -->  00:01:01,375
y축에는 모든 클러스터 곳곳에 클러스터 중심에서

16

00:01:01,375  -->  00:01:05,640
각 점에 대한 평균 거리가 있습니다.

17

00:01:05,640  -->  00:01:09,510
여기에서 클러스터 수가 증가함에 따라

18

00:01:09,510  -->  00:01:13,859
각 점에서 중심까지의 평균 거리가 감소하는 것을 볼 수 있습니다.

19

00:01:13,859  -->  00:01:16,804
그러나 센터가 추가될 때마다

20

00:01:16,805  -->  00:01:19,945
평균 거리에 미치는 영향은 동일하지 않습니다.

21

00:01:19,944  -->  00:01:21,769
k 값이 작을수록

22

00:01:21,769  -->  00:01:25,875
각각의 추가 중심으로 인해 평균 거리가 크게 줄어듭니다.

23

00:01:25,875  -->  00:01:30,064
k 값이 클수록 영향은 점점 줄어듭니다.

24

00:01:30,064  -->  00:01:34,579
이 플롯에서 주요 관심 포인트는 Elbow로 알려진 것입니다.

25

00:01:34,579  -->  00:01:38,039
이 시점에서 우리는 각 지점에서 클러스터 중심까지의

26

00:01:38,040  -->  00:01:42,695
거리를 크게 줄이는 여러 클러스터를 발견했습니다.

27

00:01:42,694  -->  00:01:47,879
동시에 k 증가는 실질적인 영향을 미치지 않는 것으로 보입니다.

28

00:01:47,879  -->  00:01:51,449
Elbow를 찾는 것은 편파적 결정이 될 수 있습니다.

29

00:01:51,450  -->  00:01:54,920
k를 증가시켜도 더 이상 중심으로부터의

30

00:01:54,920  -->  00:01:58,335
거리가 크게 감소하지 않는 지점을 찾고 있습니다.

31

00:01:58,334  -->  00:02:00,799
이것으로 연습을 해보겠습니다.
