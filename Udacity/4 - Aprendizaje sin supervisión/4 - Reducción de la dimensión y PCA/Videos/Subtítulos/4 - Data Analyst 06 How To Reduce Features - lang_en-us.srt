1

00:00:01,000  -->  00:00:06,480
You now have an idea of how the many features of a survey or home,

2

00:00:06,480  -->  00:00:09,740
can actually fall into just a few main buckets.

3

00:00:09,740  -->  00:00:13,400
Because these buckets aren't explicitly in the data,

4

00:00:13,400  -->  00:00:16,000
we call them latent features.

5

00:00:16,000  -->  00:00:20,840
The question is, how do we actually take all of the data we have from

6

00:00:20,840  -->  00:00:25,360
all of these features and condense it to only a few number of features,

7

00:00:25,360  -->  00:00:27,930
that hold the most amount of information.

8

00:00:27,930  -->  00:00:30,840
One option is that we could just choose

9

00:00:30,840  -->  00:00:34,220
a subset of the features for each latent variable,

10

00:00:34,220  -->  00:00:38,340
that best captures the essence of that latent variable.

11

00:00:38,340  -->  00:00:41,260
Maybe the square footage of the home best

12

00:00:41,260  -->  00:00:44,340
captures the latent variable related to home size.

13

00:00:44,340  -->  00:00:48,200
And we could drop these other features related to the number of bedrooms,

14

00:00:48,200  -->  00:00:51,850
the number of bathrooms, garages, et cetera.

15

00:00:51,850  -->  00:00:55,600
We could perhaps do this with the other latent features as well,

16

00:00:55,600  -->  00:00:59,840
but it seems like we're losing a lot of potentially helpful information,

17

00:00:59,840  -->  00:01:02,360
by simply dropping all of these features.

18

00:01:02,360  -->  00:01:05,420
There has to be a better way we can reduce the number of

19

00:01:05,420  -->  00:01:08,840
features in our dataset without simply dropping all of them.

20

00:01:08,840  -->  00:01:13,300
Well, in fact, there is a way we can preserve more of the information.

21

00:01:13,300  -->  00:01:16,540
And one of the main methods for doing this is PCA,

22

00:01:16,540  -->  00:01:19,280
the very topic of this lesson.
