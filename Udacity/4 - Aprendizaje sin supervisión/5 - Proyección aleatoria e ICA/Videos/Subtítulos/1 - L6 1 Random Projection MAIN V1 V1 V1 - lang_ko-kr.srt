1

00:00:00,000  -->  00:00:02,555
안녕하세요 J입니다. 이번 시간에는

2

00:00:02,555  -->  00:00:05,175
차원 축소에 대해 알아보겠습니다.

3

00:00:05,174  -->  00:00:08,910
이 단원에서 살펴볼 첫 번째 방법은 PCA보다 계산적으로

4

00:00:08,910  -->  00:00:11,460
더 효율적인 강력한 차원 감소 방법인

5

00:00:11,460  -->  00:00:14,255
무작위 투영입니다.

6

00:00:14,255  -->  00:00:17,010
데이터세트에 PCA가 직접 계산되기에는

7

00:00:17,010  -->  00:00:20,405
너무 많은 차원이 있는 경우에 일반적으로 사용됩니다.

8

00:00:20,405  -->  00:00:25,250
애플리케이션이 계산 리소스가 제한된 시스템에서 실행 중이거나

9

00:00:25,250  -->  00:00:29,329
PCA가 특정 상황에 너무 부담스럽다고 가정해 보겠습니다.

10

00:00:29,329  -->  00:00:31,289
PCA와 마찬가지로 데이터세트를 사용합니다.

11

00:00:31,289  -->  00:00:34,685
이것이 차원이 d인 데이터세트이고

12

00:00:34,685  -->  00:00:38,469
1,000이고 특정 수의 샘플 또는 행이 n이라고 가정해 봅시다.

13

00:00:38,469  -->  00:00:40,320
이들은 열입니다.

14

00:00:40,320  -->  00:00:42,905
따라서 데이터세트를 가져와

15

00:00:42,905  -->  00:00:47,564
훨씬 적은 수의 열에 있는 변환을 생성합니다.

16

00:00:47,564  -->  00:00:50,164
좋습니다. 예를 들어 50개라고 가정해 보겠습니다.

17

00:00:50,164  -->  00:00:52,899
하지만 샘플 수는 같고 각 열이 여기 있는 위치에서

18

00:00:52,899  -->  00:00:55,869
여러 열의 정보를 캡처합니다.

19

00:00:55,869  -->  00:00:59,119
2차원에서 1차원으로 데이터세트의 차원을 줄이는

20

00:00:59,119  -->  00:01:02,989
지나치게 단순화된 예를 살펴보겠습니다.

21

00:01:02,990  -->  00:01:05,379
여기서 PCA는 분산을 최대화하려고 합니다.

22

00:01:05,379  -->  00:01:09,679
따라서 분산을 최대화하는 벡터 또는 방향을 찾아

23

00:01:09,680  -->  00:01:11,570
2차원에서 1차원으로 데이터를 투영할 때

24

00:01:11,569  -->  00:01:14,539
정보 손실을 최소화합니다.

25

00:01:14,540  -->  00:01:16,430
따라서 그 선은 이와 같을 것이고

26

00:01:16,430  -->  00:01:18,170
이것은 투영이 될 것입니다.

27

00:01:18,170  -->  00:01:19,980
1차원에서

28

00:01:19,980  -->  00:01:21,770
데이터세트는 이렇게 보일 것입니다.

29

00:01:21,769  -->  00:01:23,989
무작위 투영, 특히 많은 차원에 대해 이야기하는 경우

30

00:01:23,989  -->  00:01:26,629
PCA가 수행한 이 계산으로 일정량의

31

00:01:26,629  -->  00:01:28,409
리소스를 소비합니다.

32

00:01:28,409  -->  00:01:30,619
무작위 투영은 말 그대로 선을 선택하고

33

00:01:30,620  -->  00:01:34,689
아무 선이나 선택하면 그 위에 투영을 할 것입니다. 이것이 우리의 데이터세트입니다.

34

00:01:34,689  -->  00:01:37,640
따라서 2차원에서 1차원으로의 지나치게 단순화된

35

00:01:37,640  -->  00:01:40,730
시나리오에서는 별 의미가 없지만 실제로 작동하고

36

00:01:40,730  -->  00:01:44,210
더 높은 차원에서 정말 잘 작동하고

37

00:01:44,209  -->  00:01:47,029
고성능 방식으로 작동합니다.

38

00:01:47,030  -->  00:01:50,000
무작위 투영의 기본 전제는 이와 같은

39

00:01:50,000  -->  00:01:52,790
임의 행렬을 곱하여 데이터세트의 차원 수를

40

00:01:52,790  -->  00:01:56,725
간단히 줄일 수 있다는 것입니다.

41

00:01:56,724  -->  00:02:02,629
따라서 d, 우리는 데이터세트에 d를 갖지만

42

00:02:02,629  -->  00:02:05,509
k는 우리가 계산하거나 우리가 원하는 것이고

43

00:02:05,510  -->  00:02:08,640
k에 대한 보수적이거나 좋은 추정치를 계산하는 방법이 있습니다.

44

00:02:08,639  -->  00:02:10,244
따라서 이것은 결과 데이터세트가 됩니다.

45

00:02:10,245  -->  00:02:12,375
임의 행렬을 곱하면

46

00:02:12,375  -->  00:02:14,884
그것은 어떤 면에서 모든 무작위 투영입니다.

47

00:02:14,884  -->  00:02:17,254
여기서 구체적인 예를 들어 보겠습니다.

48

00:02:17,254  -->  00:02:18,840
이것이 우리의 데이터세트이고

49

00:02:18,840  -->  00:02:21,770
여기에 12,000개의 차원이 있고

50

00:02:21,770  -->  00:02:26,570
그것이 우리의 d이고 1,500개의 행 또는 샘플이 있다고 가정해 보겠습니다.

51

00:02:26,569  -->  00:02:29,870
이것을 scikit-learn에 제공하고, scikit-learn이라고 부르면

52

00:02:29,870  -->  00:02:34,150
기본값만 사용하여 이 데이터세트에 대해 무작위 투영을 수행할 수 있을까요?

53

00:02:34,150  -->  00:02:36,950
6,200개의 차원과 동일한 수의 샘플에 있는

54

00:02:36,949  -->  00:02:42,174
이 데이터세트를 반환합니다.

55

00:02:42,175  -->  00:02:46,450
그렇다면 그것이 작동하고 k는 어디에서 오는지 어떻게 알 수 있을까요?

56

00:02:46,449  -->  00:02:50,419
무작위 투영에 대한 이론적 토대는 Johnson-Lindenstrauss 보조정리라고

57

00:02:50,419  -->  00:02:53,799
하는 이 아이디어로,

58

00:02:53,800  -->  00:02:57,635
고차원 공간에서 N개의 점으로 구성된 데이터세트를 말합니다.

59

00:02:57,634  -->  00:03:00,659
따라서 이 데이터세트, N개의 점,

60

00:03:00,659  -->  00:03:03,509
고차원 공간, 12,000은 상당히 높고

61

00:03:03,509  -->  00:03:04,954
매핑할 수 있습니다.

62

00:03:04,955  -->  00:03:06,890
이 임의 행렬을 곱하면

63

00:03:06,889  -->  00:03:12,789
이 좁은 데이터세트인 훨씬 더 낮은 차원의 공간까지 내려갑니다.

64

00:03:12,789  -->  00:03:14,280
이것이 우리에게 정말 중요한 이유입니다.

65

00:03:14,280  -->  00:03:15,770
점 사이의 거리를 상당 부분

66

00:03:15,770  -->  00:03:18,855
보존하는 방식으로 수행할 수 있습니다.

67

00:03:18,854  -->  00:03:21,699
따라서 특정 방식으로 보존되는 투영 후

68

00:03:21,699  -->  00:03:23,629
각 두 점 사이의 거리,

69

00:03:23,629  -->  00:03:26,990
이러한 데이터세트의 각 점 쌍입니다.

70

00:03:26,990  -->  00:03:29,469
대부분 또는 많은 지도 학습 및

71

00:03:29,469  -->  00:03:32,050
비지도 학습에서 알고리즘은 점 사이의 거리에

72

00:03:32,050  -->  00:03:34,765
실제로 관심을 갖기 때문에 이는 정말 중요합니다.

73

00:03:34,764  -->  00:03:37,789
따라서 우리는 이러한 거리가 약간 왜곡되지만

74

00:03:37,789  -->  00:03:41,914
보존될 수 있다는 보장 수준을 설정했습니다.

75

00:03:41,914  -->  00:03:43,750
이것은 어떻게 보존할 수 있을까요?

76

00:03:43,750  -->  00:03:45,389
어떤 종류의 보증이 있을까요?

77

00:03:45,389  -->  00:03:48,619
간단한 예를 들어 실제로 계산해 봅시다.

78

00:03:48,620  -->  00:03:52,140
자, 여기서 처음 두 행을 취한다고 가정해봅시다.

79

00:03:52,139  -->  00:03:53,604
그래서 이것과 이것,

80

00:03:53,604  -->  00:03:58,609
우리 데이터세트의 처음 두 점 그리고 이것이 투영 후의 값입니다.

81

00:03:58,610  -->  00:04:02,210
동일한 샘플이지만 차원 수준이 다릅니다.

82

00:04:02,210  -->  00:04:05,430
이 Johnson-Lindentrauss 기본형이 우리에게 알려주는 것은

83

00:04:05,430  -->  00:04:11,314
투영 제곱에서 두 점 사이의 거리가 압착되어 있다는 것입니다.

84

00:04:11,314  -->  00:04:14,810
따라서 원래 데이터세트의 두 점 사이 거리의

85

00:04:14,810  -->  00:04:19,259
제곱 곱하기 1 빼기 엡실론보다 큽니다.

86

00:04:19,259  -->  00:04:22,009
따라서 엡실론은 투영에서

87

00:04:22,009  -->  00:04:25,050
무작위 투영이 허용하는 오류 수준입니다.

88

00:04:25,050  -->  00:04:28,100
따라서 투영된 데이터세트의 두 점 사이의 거리는

89

00:04:28,100  -->  00:04:31,910
원래 데이터세트의 두 점 거리의

90

00:04:31,910  -->  00:04:35,270
1 빼기 엡실론 곱하기 제곱보다 크고

91

00:04:35,269  -->  00:04:39,454
1 더하기 엡실론 거리의 제곱보다 작습니다.

92

00:04:39,454  -->  00:04:41,430
실제로 그 숫자를 계산한 결과

93

00:04:41,430  -->  00:04:45,805
이 두 점 사이의 거리는 125.6입니다.

94

00:04:45,805  -->  00:04:47,694
그 값을 여기에 넣습니다.

95

00:04:47,694  -->  00:04:50,719
기본값인 엡실론은 아무것도 변경하지 않았기 때문에

96

00:04:50,720  -->  00:04:53,765
scikit-learn의 기본값은 0.1입니다.

97

00:04:53,764  -->  00:04:56,019
0-1 사이의 값을 가질 수 있습니다.

98

00:04:56,019  -->  00:04:58,625
이렇게 하면

99

00:04:58,625  -->  00:05:01,819
이 두 점 사이의 거리는 125.8이 됩니다.

100

00:05:01,819  -->  00:05:06,409
따라서 이 거리는 이 거리의 0.9배보다 크고

101

00:05:06,410  -->  00:05:11,785
이 거리의 제곱의 1.1배보다 작습니다.

102

00:05:11,785  -->  00:05:15,235
그래서 엡실론이 간과 같다고 볼 수 있습니다.

103

00:05:15,235  -->  00:05:18,650
이것은 얼마나 많은 열이 생산되는지 계산에 들어가고

104

00:05:18,649  -->  00:05:21,319
왜곡이 이러한 차원 감소를 갖도록

105

00:05:21,319  -->  00:05:25,480
허용하는 것은 오류 수준입니다.

106

00:05:25,480  -->  00:05:30,660
따라서 이 보장은 데이터세트의 모든 점 쌍 사이의 거리를 유지합니다.

107

00:05:30,660  -->  00:05:32,840
그래서, 그것은 단지 하나와 둘이 아니라,

108

00:05:32,839  -->  00:05:34,909
하나와 모든 다른 지점이고,

109

00:05:34,910  -->  00:05:36,090
둘과 모든 다른 지점입니다.

110

00:05:36,089  -->  00:05:38,064
그 보장이 그곳에 있습니다.

111

00:05:38,064  -->  00:05:41,404
엡실론은 이 정도로 거리를 보존하는 데 사용할 수 있는

112

00:05:41,404  -->  00:05:45,269
함수에 대한 입력일 뿐입니다.
