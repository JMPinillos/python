1

00:00:00,000  -->  00:00:02,109
어서 오세요. 이 동영상에서는,

2

00:00:02,109  -->  00:00:06,394
독립 성분 분석 알고리즘에 대해 더 이야기를 나눠보겠습니다.

3

00:00:06,394  -->  00:00:08,369
매우 높은 수준에서 이야기할 것이므로

4

00:00:08,369  -->  00:00:10,329
수학 얘기는 깊이 다루지 않을 겁니다.

5

00:00:10,330  -->  00:00:11,960
알맞으면서도

6

00:00:11,960  -->  00:00:15,580
알고 있는 것이 좋은 내용을 소개합니다.

7

00:00:15,580  -->  00:00:17,789
작동 방식이 어떻게 되는지,

8

00:00:17,789  -->  00:00:21,219
이를 사용하려 할 때 어떤 가정을 하는지에 대한 일반적인 내용을 살펴봅니다.

9

00:00:21,219  -->  00:00:24,964
우리가 갖고 있는 데이터세트를 X라고 하죠.

10

00:00:24,964  -->  00:00:29,769
X는 곱으로 생성되었습니다.

11

00:00:29,769  -->  00:00:32,060
우리가 혼합 행렬이라고 부르는 것은

12

00:00:32,060  -->  00:00:34,844
소스 신호에 의한 A이며

13

00:00:34,844  -->  00:00:36,140
우리는 이를 보유하고 있지 않습니다.

14

00:00:36,140  -->  00:00:38,365
A가 없으니 S도 없죠.

15

00:00:38,365  -->  00:00:41,140
하지만 S는 우리가 계산하려고 하는 값입니다.

16

00:00:41,140  -->  00:00:44,734
그러므로 X가 A와 S를 곱한 값이라고 하면

17

00:00:44,734  -->  00:00:48,424
우리가 여기서 원하는 소스 목표인 S는 W이며

18

00:00:48,424  -->  00:00:50,404
이는 A의 역수임을 알 수 있습니다.

19

00:00:50,405  -->  00:00:52,439
따라서 A가 혼합 행렬이라면

20

00:00:52,439  -->  00:00:54,659
W는 비혼합 행렬이라고 부를 수 있습니다.

21

00:00:54,659  -->  00:00:56,659
우리가 가진 데이터세트,

22

00:00:56,659  -->  00:00:58,889
우리가 가진 원본 레코드에 X를 곱하죠.

23

00:00:58,890  -->  00:01:00,660
이것이 공식입니다.

24

00:01:00,659  -->  00:01:02,234
X는 우리가 입력할 수 있는 값이고

25

00:01:02,234  -->  00:01:05,269
W는 우리가 계산하려는 값이며

26

00:01:05,269  -->  00:01:06,450
S가 그 결과입니다.

27

00:01:06,450  -->  00:01:09,769
따라서 프로세스의 ICA 알고리즘은 W의 근사치를 내거나

28

00:01:09,769  -->  00:01:13,759
원본 신호를 생성하기 위한 데이터세트인

29

00:01:13,760  -->  00:01:17,660
X로 곱할 수 있는 가장 적합한 W를 찾는 것입니다.

30

00:01:17,659  -->  00:01:21,200
ICA 알고리즘에 대해서는

31

00:01:21,200  -->  00:01:25,460
Independent Component Analysis: Algorithms and Applications라는 논문에 명확하게 설명되어 있습니다.

32

00:01:25,459  -->  00:01:28,129
여기에서 모든 것이 파생되었죠.

33

00:01:28,129  -->  00:01:32,174
이는 알고리즘의 서로 다른 부분을 계산하는 몇 가지 방법을 보여주지만

34

00:01:32,174  -->  00:01:37,304
FastICA라는 알고리즘에 대한 매우 높은 수준의 보기가 있는 경우에만 해당합니다.

35

00:01:37,305  -->  00:01:38,420
이것이 한 가지 방법인데요.

36

00:01:38,420  -->  00:01:41,140
scikit-learn에서 실제로 구현된 것입니다.

37

00:01:41,140  -->  00:01:43,459
먼저 데이터세트인 X가 있습니다.

38

00:01:43,459  -->  00:01:46,069
이것을 중앙에 놓고 흰색으로 만듭니다.

39

00:01:46,069  -->  00:01:49,129
그런 다음 초기 무작위 가중치 행렬을 선택합니다.

40

00:01:49,129  -->  00:01:51,179
이것을 W라고 하죠. 세 번째 단계에서는

41

00:01:51,180  -->  00:01:55,180
W를 벡터를 포함하는 행렬이라고 추정합니다.

42

00:01:55,180  -->  00:01:57,875
벡터의 수, 각각은 가중치 벡터입니다.

43

00:01:57,875  -->  00:02:02,355
이를 추정한 후 상관 관계를 해제합니다. 상관 관계는

44

00:02:02,355  -->  00:02:07,165
W1과 W2가 같은 값으로 변환되는 것을 방지합니다.

45

00:02:07,165  -->  00:02:10,010
따라서 우리는 이들이 다른 값으로 수렴되길 원하죠.

46

00:02:10,009  -->  00:02:14,069
그런 다음 3부터 시작해 수렴할 때까지 반복합니다.

47

00:02:14,069  -->  00:02:17,120
우리가 만족하는 W의 값을 찾을 때까지요.

48

00:02:17,120  -->  00:02:19,580
대부분은 여기에서 3단계에 도달합니다.

49

00:02:19,580  -->  00:02:23,450
그렇다면 추정은 어떻게 이루어질까요?

50

00:02:23,449  -->  00:02:28,264
이것은 여기에서 각 벡터를 추정하기 위한 공식입니다.

51

00:02:28,264  -->  00:02:30,709
E는 예상 값이고

52

00:02:30,710  -->  00:02:32,594
X는 데이터세트이며

53

00:02:32,594  -->  00:02:36,104
G는 그냥 2차가 아닌 함수죠.

54

00:02:36,104  -->  00:02:38,794
우리는 다수를 선택할 수 있습니다.

55

00:02:38,794  -->  00:02:40,599
일반적으로 사용되는 것은 무엇이고,

56

00:02:40,599  -->  00:02:43,310
scikit-learn이 사용하는 것은 무엇이고, 논문에서 제안하는 것은 무엇인가요?

57

00:02:43,310  -->  00:02:44,990
옵션 중 하나가 tanh이므로

58

00:02:44,990  -->  00:02:51,159
하이퍼 탄젠트 함수와 상관 관계 해제는 다음과 같이 계산됩니다.

59

00:02:51,159  -->  00:02:53,090
자, 이 부분에 대해 좀 더 이야기해 볼까요.

60

00:02:53,090  -->  00:02:55,020
조금 아리송해 보일지도 모르겠습니다.

61

00:02:55,020  -->  00:02:57,830
ICA는 몇 가지를 가정하죠.

62

00:02:57,830  -->  00:03:01,400
ICA는 구성 요소가 통계적으로 독립적이라 가정하며

63

00:03:01,400  -->  00:03:06,760
논문에서는 이것이 의미하는 바를 통계적 언어로 설명합니다.

64

00:03:06,759  -->  00:03:11,914
또한 구성 요소에 비가우스 분포가 있어야 한다고 가정합니다.

65

00:03:11,914  -->  00:03:14,939
여기에서는 비가우스라는 점이 무척 중요합니다.

66

00:03:14,939  -->  00:03:18,419
이 부분이 실제로 ICA를 추정하는 데 핵심적인 역할을 하며

67

00:03:18,419  -->  00:03:19,939
이 부분이 없으면 계산을 할 수 없으며

68

00:03:19,939  -->  00:03:23,844
가우스인 경우에는 원래 신호를 복원할 수 없을 것입니다.

69

00:03:23,844  -->  00:03:25,504
그러므로 여기서부터 시작해서

70

00:03:25,504  -->  00:03:27,609
중심 극한 정리는 우리에게

71

00:03:27,610  -->  00:03:31,705
독립 변수 합계의 분포는

72

00:03:31,705  -->  00:03:34,570
가우스 분포로 향하는 경향이 있음을 알려줍니다.

73

00:03:34,569  -->  00:03:36,935
이를 안 상태에서 우리는 W를

74

00:03:36,935  -->  00:03:38,539
여기의 가중치 행렬로 여깁니다.

75

00:03:38,539  -->  00:03:45,034
우리는 이를 W 전치 X의 비가우스성을 극대화하는 행렬로 간주합니다.

76

00:03:45,034  -->  00:03:48,329
따라서 여기서도 비가우스성이 중요하죠.

77

00:03:48,330  -->  00:03:51,600
하지만 여기서 우리가 해야할 건 정확히 비가우스성을 계산하는 것입니다.

78

00:03:51,599  -->  00:03:55,659
그것이 이 전체 알고리즘이 극대화하려는 항이기 때문입니다.

79

00:03:55,659  -->  00:03:59,710
그렇다면, 비가우스성을 계산하는 방법엔 무엇이 있을까요?

80

00:03:59,710  -->  00:04:03,930
여기 있는 이 항은 음 엔트로피라고 불리는 것의 근사치입니다.

81

00:04:03,930  -->  00:04:08,230
음 엔트로피란 엔트로피라는 아이디어가 나온

82

00:04:08,229  -->  00:04:13,609
정보 이론에서 비롯된 것으로, 이것의 근사치를 내는 방법입니다.

83

00:04:13,610  -->  00:04:15,395
자세한 내용을 모두 알 필요는 없습니다.

84

00:04:15,395  -->  00:04:18,819
비가우스성이라는 가정을 알고 있는 한 말이죠.

85

00:04:18,819  -->  00:04:23,139
독립 성분은 말 그대로 독립적이어야 합니다.

86

00:04:23,139  -->  00:04:24,829
그렇지 않으면 찾을 수 없을 테니까요.

87

00:04:24,829  -->  00:04:26,664
이러한 조건을 알고 있으며

88

00:04:26,665  -->  00:04:31,050
iCA가 작동해야 하는 상황을 알고 있다면 중요한 부분입니다.

89

00:04:31,050  -->  00:04:35,129
세부 사항을 알아보고 싶거나 파생 항목을 보려면

90

00:04:35,129  -->  00:04:38,759
아래 코멘트에 논문의 링크가 있으며 동영상 아래에는 설명이 있습니다.
