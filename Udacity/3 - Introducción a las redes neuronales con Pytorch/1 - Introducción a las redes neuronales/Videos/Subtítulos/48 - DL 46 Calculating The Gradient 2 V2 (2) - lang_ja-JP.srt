1
00:00:00,000 --> 00:00:04,305
重みや入力を扱った ニューラルネットワークに戻りましょう

2
00:00:04,305 --> 00:00:08,730
W¹は 第１層に

3
00:00:08,730 --> 00:00:12,775
W²は 第２層に属することを思い出してください

4
00:00:12,775 --> 00:00:15,390
バイアスは bと呼ばれず

5
00:00:15,390 --> 00:00:16,960
今は 便宜上W₃₁ W₃₂と呼ばれ

6
00:00:16,960 --> 00:00:19,665
マトリックスノーテーションで すべてを表現できるようにしています

7
00:00:19,665 --> 00:00:22,830
入力はどうなっているでしょうか？

8
00:00:22,830 --> 00:00:25,280
フィードフォーワード処理を行ってみましょう

9
00:00:25,280 --> 00:00:28,218
第１層で

10
00:00:28,218 --> 00:00:29,910
入力を選択し

11
00:00:29,910 --> 00:00:34,620
重みをかけると

12
00:00:34,620 --> 00:00:37,860
入力と重みの線形関数である h₁を得られます

13
00:00:37,860 --> 00:00:39,600
h２も同じです

14
00:00:39,600 --> 00:00:42,085
この式で求められます

15
00:00:42,085 --> 00:00:43,350
第２層では

16
00:00:43,350 --> 00:00:46,485
このh₁とh₂ そして新しいバイアスを選択し

17
00:00:46,485 --> 00:00:48,645
シグモイド関数を適用してから

18
00:00:48,645 --> 00:00:52,980
重みを掛けて hの値を得るように追加することで

19
00:00:52,980 --> 00:00:57,345
線形関数を適用します

20
00:00:57,345 --> 00:00:58,530
最後に 第３層で

21
00:00:58,530 --> 00:01:02,100
hのシグモイド関数を選択して

22
00:01:02,100 --> 00:01:07,540
０と１の間の 推測または確率 ŷ を得ます

23
00:01:07,540 --> 00:01:11,070
より簡潔な表記で これを読み取ることができます

24
00:01:11,070 --> 00:01:15,244
第１層に一致する列をW¹

25
00:01:15,244 --> 00:01:19,930
第２層に一致する列を W²とすることによってです

26
00:01:19,930 --> 00:01:22,570
W²をW¹のシグモイドと組み合わせ

27
00:01:22,570 --> 00:01:26,260
入力xに適用する

28
00:01:26,260 --> 00:01:33,540
これが フィードフォーワードです

29
00:01:33,540 --> 00:01:35,890
フィードフォーワードの逆である
誤差逆伝播法をやってみたいと思います

30
00:01:35,890 --> 00:01:39,010
連鎖律で ラベルの各重みの
誤差関数の微分係数を計算します

31
00:01:39,010 --> 00:01:40,930
連鎖律で ラベルの各重みの
誤差関数の微分係数を計算します

32
00:01:40,930 --> 00:01:44,890
誤差関数は この式であり
予測ŷの関数だと思い出してください

33
00:01:44,890 --> 00:01:52,611
予測は 重みwijのすべての関数であるため

34
00:01:52,611 --> 00:01:59,110
誤差関数は wijのすべての関数と
見なすことができます

35
00:01:59,110 --> 00:02:02,760
したがって 勾配は 単に各重みに関する誤差関数Eの

36
00:02:02,760 --> 00:02:07,015
全偏微分係数で形成されるベクトルです

37
00:02:07,015 --> 00:02:13,540
ここで微分係数の１つを計算してみましょう

38
00:02:13,540 --> 00:02:16,960
W₁₁⁽¹⁾に関して Eの微分係数を計算してみましょう

39
00:02:16,960 --> 00:02:23,500
予測は 単純な関数で構成されていて

40
00:02:23,500 --> 00:02:25,196
連鎖律によって これに関する微分係数が
全偏微分係数の積であると分かります

41
00:02:25,196 --> 00:02:31,210
この場合 W₁₁に関する微分係数Eは

42
00:02:31,210 --> 00:02:35,140
ŷ x h x h₁ x W₁₁に
関する微分係数で求めることができます

43
00:02:35,140 --> 00:02:37,750
複雑に思えるかもしれませんが

44
00:02:37,750 --> 00:02:41,650
４つの偏微分係数を 掛けるだけで
複雑な構成の微分係数を 計算できることは

45
00:02:41,650 --> 00:02:44,710
すばらしいと言えます

46
00:02:44,710 --> 00:02:48,617
１つ目を既に計算しています

47
00:02:48,617 --> 00:02:52,480
ŷに関する微分係数Eです

48
00:02:52,480 --> 00:02:57,650
覚えていると思いますが ŷ - yです

49
00:02:57,650 --> 00:03:01,345
他のものを計算してみましょう

50
00:03:01,345 --> 00:03:03,790
確認のために 多層パーセプトロンの１つだけを見てみましょう

51
00:03:03,790 --> 00:03:06,370
入力は 前の計算から求められる値であるh₁とh₂です

52
00:03:06,370 --> 00:03:10,235
バイアスユニットに一致するh₁, h₂, 1に
シグモイド関数と線形関数を適用すると

53
00:03:10,235 --> 00:03:12,360
hという結果を得られます

54
00:03:12,360 --> 00:03:14,767
h₁に関するhの微分係数は 何でしょうか？

55
00:03:14,767 --> 00:03:16,430
hは３つの和で １つしかh₁を含みません

56
00:03:16,430 --> 00:03:20,095
したがって２つ目と３つ目は 微分係数が０になります

57
00:03:20,095 --> 00:03:25,193
１つ目は W₁₁⁽²⁾になります

58
00:03:25,193 --> 00:03:28,665
定数だからです

59
00:03:28,665 --> 00:03:30,955
これにh₁に関する シグモイド関数の微分係数を掛けます

60
00:03:30,955 --> 00:03:34,905
インストラクターのコメントの下に 計算したものがあります

61
00:03:34,905 --> 00:03:39,045
シグモイド関数には すばらしい微分係数があります

62
00:03:39,045 --> 00:03:41,550
つまり hのシグモイドの微分係数は

63
00:03:41,550 --> 00:03:44,670
hx1-h のシグモイドとなるのです

64
00:03:44,670 --> 00:03:51,130
インストラクターのコメントの下に 展開したものがあります

65
00:03:51,130 --> 00:03:55,940
１日分のコースが終了しますので
クイズでこれをコーディングしてみましょう

66
00:03:55,940 --> 00:04:03,205
あとは 式をコーディングして それをずっと使用するだけです

67
00:04:03,205 --> 00:04:08,715
それが ニューラルネットワークをトレーニングする方法です

68
00:04:08,715 --> 00:04:12,615

69
00:04:12,615 --> 00:04:15,960

70
00:04:15,960 --> 00:04:19,200

71
00:04:19,200 --> 00:04:24,660

72
00:04:24,660 --> 00:04:27,600

73
00:04:27,600 --> 00:04:31,275

74
00:04:31,275 --> 00:04:35,635

75
00:04:35,635 --> 00:04:37,020

