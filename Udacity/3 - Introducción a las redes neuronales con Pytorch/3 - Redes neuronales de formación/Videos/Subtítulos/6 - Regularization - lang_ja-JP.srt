1
00:00:00,000 --> 00:00:04,852
まず第一に
どちらの方程式も同じ直線

2
00:00:04,852 --> 00:00:07,500
つまり X1+X2=0の直線を作る
ものです

3
00:00:07,500 --> 00:00:10,664
その理由は 解答２は実際には

4
00:00:10,664 --> 00:00:15,839
解答１のスカラー倍数だからです
では 見てみましょう

5
00:00:15,839 --> 00:00:18,929
​予測値は 一次関数のシグモイド
であることを思い出してください

6
00:00:18,929 --> 00:00:20,579
最初の例では 0.11の場合

7
00:00:20,579 --> 00:00:22,788
1+1のシグモイドで

8
00:00:22,789 --> 00:00:27,600
２のシグモイドで 0.88です

9
00:00:27,600 --> 00:00:29,370
これは 点が青で

10
00:00:29,370 --> 00:00:31,268
ラベルが１なので 悪くありません

11
00:00:31,268 --> 00:00:32,460
点（-1, -1）の場合

12
00:00:32,460 --> 00:00:35,880
予測値は-1+-1のシグモイド

13
00:00:35,880 --> 00:00:40,304
つまり -2のシグモイドであり
0.12です

14
00:00:40,304 --> 00:00:45,359
また ラベルは赤 つまり０なので
ベストではありません

15
00:00:45,359 --> 00:00:48,009
では ２つ目のモデルを
見てみましょう

16
00:00:48,009 --> 00:00:51,335
点(1, 1)の予測シグモイドは

17
00:00:51,335 --> 00:00:55,469
10x1+10x1で
20のシグモイドです

18
00:00:55,469 --> 00:01:01,920
これは0.999999979で

19
00:01:01,920 --> 00:01:04,109
１に限りなく近く

20
00:01:04,109 --> 00:01:06,015
素晴らしい予測と言えます

21
00:01:06,015 --> 00:01:08,400
点（-1, -1）の予測シグモイドは

22
00:01:08,400 --> 00:01:13,350
-1+1の10倍 つまり

23
00:01:13,349 --> 00:01:16,837
-20のシグモイドとなり

24
00:01:16,837 --> 00:01:23,409
0.0000000021となります

25
00:01:23,409 --> 00:01:27,256
これは０に近い値なので
素晴らしい予測です

26
00:01:27,256 --> 00:01:30,143
質問の答えは２つ目のモデルで

27
00:01:30,143 --> 00:01:32,150
２つ目は正確です

28
00:01:32,150 --> 00:01:33,765
この方が優れている
ということです

29
00:01:33,765 --> 00:01:35,909
過剰適合を暗示しているので

30
00:01:35,909 --> 00:01:38,909
躊躇しているかもしれません

31
00:01:38,909 --> 00:01:40,954
その勘は当たっています

32
00:01:40,954 --> 00:01:43,814
問題は過剰適合で 非常に微妙です

33
00:01:43,814 --> 00:01:46,259
なぜ最初のモデルの方が

34
00:01:46,260 --> 00:01:49,206
誤差が大きくても
良いのかを説明します

35
00:01:49,206 --> 00:01:54,150
x₁+x₂のような小さな値に
シグモイドを適用すると

36
00:01:54,150 --> 00:01:59,484
左のような関数になり
最急降下法の傾斜が見られます

37
00:01:59,484 --> 00:02:07,594
一次関数を10倍にして
10x₁+10x₂のシグモイドにすると

38
00:02:07,594 --> 00:02:11,919
予測値が０と１に近くなり
より良い結果が得られます

39
00:02:11,919 --> 00:02:17,737
関数の傾きは急になり

40
00:02:17,737 --> 00:02:19,960
導関数は ほとんどが０に近く

41
00:02:19,960 --> 00:02:24,960
曲線の中央に来ると
非常に大きくなります

42
00:02:24,960 --> 00:02:27,939
最急降下法を適切に行うには

43
00:02:27,939 --> 00:02:33,585
右よりも左のようなモデルが
必要になります

44
00:02:33,585 --> 00:02:35,200
概念的には

45
00:02:35,199 --> 00:02:37,259
右のモデルは安定しすぎて

46
00:02:37,259 --> 00:02:41,454
最急降下法を適用する余地が
ほとんどありません

47
00:02:41,455 --> 00:02:42,550
右のモデルで

48
00:02:42,550 --> 00:02:45,910
誤って分類されたポイントは

49
00:02:45,909 --> 00:02:51,250
大きな誤差を発生させるので
修正には調整が必要です

50
00:02:51,250 --> 00:02:53,379
有名な哲学者・数学者である

51
00:02:53,379 --> 00:02:58,264
バートランド・ラッセルの
言葉に集約されます

52
00:02:58,264 --> 00:03:00,819
人工知能の問題点は

53
00:03:00,819 --> 00:03:04,194
悪いモデルのほうが確実で

54
00:03:04,194 --> 00:03:07,634
良いモデルは疑いに
満ちていることです

55
00:03:07,634 --> 00:03:08,764
問題は

56
00:03:08,764 --> 00:03:12,422
このような過剰適合を
どのようにして防ぐかです

57
00:03:12,423 --> 00:03:16,775
悪いモデルの方は誤差が小さく
簡単ではなさそうです

58
00:03:16,775 --> 00:03:20,865
しかし エラー関数を
少し調整すればいいのです

59
00:03:20,865 --> 00:03:24,250
基本的には 高い係数を
ペナルティにします

60
00:03:24,250 --> 00:03:25,969
従来の誤差関数の

61
00:03:25,969 --> 00:03:32,164
重みが大きいときに
大きくなる項を加えます

62
00:03:32,164 --> 00:03:34,569
これには２つ方法があります

63
00:03:34,569 --> 00:03:40,935
１つは 重みの絶対値の合計に
定数ラムダを加えたものです

64
00:03:40,935 --> 00:03:46,490
もう１つは 重みの二乗の合計に
同じ定数を加えたものです

65
00:03:46,490 --> 00:03:52,082
重みが大きくなれば
この２つも大きくなります

66
00:03:52,082 --> 00:03:56,430
ラムダパラメータで 係数に
対するペナルティが決まります

67
00:03:56,430 --> 00:03:57,467
ラムダが大きい場合

68
00:03:57,467 --> 00:03:59,085
ペナルティも大きくなります

69
00:03:59,085 --> 00:04:02,875
ラムダが小さい場合は
ペナルティも小さくなります

70
00:04:02,875 --> 00:04:05,775
最後に 絶対値を選んだ場合は

71
00:04:05,775 --> 00:04:09,138
L1正則化を行い

72
00:04:09,138 --> 00:04:10,944
正方形を選ぶ場合は

73
00:04:10,944 --> 00:04:13,969
L2正則化を行います

74
00:04:13,969 --> 00:04:15,294
どちらも一般的で

75
00:04:15,294 --> 00:04:18,069
目的や用途に応じて

76
00:04:18,069 --> 00:04:20,724
どちらかを
適用することになります

77
00:04:20,725 --> 00:04:27,185
L1正則化とL2正則化を決定するための
一般的なガイドラインを紹介します

78
00:04:27,185 --> 00:04:32,634
L1を適用すると ベクトルが
疎になる傾向があります

79
00:04:32,634 --> 00:04:36,449
つまり 小さな重みは
０になる傾向があります

80
00:04:36,449 --> 00:04:40,449
そのため 重みの数を減らして
小さなセットにしたい場合は

81
00:04:40,449 --> 00:04:41,944
L1を使用します

82
00:04:41,944 --> 00:04:44,550
これは特徴量選択にも最適で

83
00:04:44,550 --> 00:04:47,550
時には 何百もの特徴量選択を
持つ問題があります

84
00:04:47,550 --> 00:04:52,379
L1正則化は
重要な選択に役立ち

85
00:04:52,379 --> 00:04:55,129
残りを０にしてくれます

86
00:04:55,129 --> 00:04:56,939
一方 L2は

87
00:04:56,939 --> 00:04:59,519
すべての重みを均一に
小さくするため

88
00:04:59,519 --> 00:05:02,894
疎ベクトルを好まない傾向が
あります

89
00:05:02,894 --> 00:05:06,329
通常のトレーニングでは

90
00:05:06,329 --> 00:05:09,854
こちらの方が良い結果が得られます

91
00:05:09,855 --> 00:05:13,710
ではなぜ L1正則化では
重みが疎ベクトルになり

92
00:05:13,709 --> 00:05:18,495
L2正則化ではベクトルの重みが
小さく均質になるのでしょうか

93
00:05:18,495 --> 00:05:20,545
その理由を考えてみましょう

94
00:05:20,545 --> 00:05:23,030
ベクトル（1, 0）の場合

95
00:05:23,029 --> 00:05:26,384
重みの絶対値の和は１で

96
00:05:26,384 --> 00:05:30,267
重みの二乗の和も１になります

97
00:05:30,267 --> 00:05:35,187
しかし ベクトル（0.5, 0.5）では

98
00:05:35,187 --> 00:05:39,298
重みの絶対値の和は
１のままですが

99
00:05:39,298 --> 00:05:46,024
二乗の和は
0.25+0.25で0.5になります

100
00:05:46,024 --> 00:05:51,245
L2正則化では
ベクトル（1, 0）よりも

101
00:05:51,245 --> 00:05:53,599
ベクトル点（0.5, 0.5）の方が

102
00:05:53,600 --> 00:05:57,020
二乗和が小さくなるので
好まれます

103
00:05:57,019 --> 00:06:00,000
つまり 生成される関数が
小さくなります

