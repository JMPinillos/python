1
00:00:00,000 --> 00:00:04,419
これを解決する最善の方法は
活性化関数です

2
00:00:04,419 --> 00:00:07,615
これとは別に 双曲線正接関数も役立ちます

3
00:00:07,615 --> 00:00:10,224
次のような式です

4
00:00:10,224 --> 00:00:15,795
(eˣ - e⁻ˣ)/(eˣ + e⁻ˣ)

5
00:00:15,795 --> 00:00:18,004
これはシグモイドに似ていますが

6
00:00:18,004 --> 00:00:20,914
範囲が-１～１です

7
00:00:20,914 --> 00:00:23,089
微分係数はこれより大きいです

8
00:00:23,089 --> 00:00:24,554
信じられないかもしれませんが
この微妙な違いにより

9
00:00:24,554 --> 00:00:27,589
ニューラルネットワークは
目覚ましい進歩を遂げています

10
00:00:27,589 --> 00:00:33,115
その他に 人気のある活性化関数は
正規化線形ユニット（ReLU）です

11
00:00:33,115 --> 00:00:35,995
これは非常にシンプルな関数です

12
00:00:35,994 --> 00:00:38,824
正の場合にのみ情報を伝え

13
00:00:38,825 --> 00:00:40,670
同じ値を返し

14
00:00:40,670 --> 00:00:44,395
負の場合には０を返します

15
00:00:44,395 --> 00:00:48,575
別の見方をすれば
xと０の間の最大値です

16
00:00:48,575 --> 00:00:52,270
この関数はシグモイドの
代わりに多用されていて

17
00:00:52,270 --> 00:00:55,585
精度であまり妥協せずに
トレーニングを向上できます

18
00:00:55,585 --> 00:00:59,850
数字が正の場合に 微分係数が
１であるためです

19
00:00:59,850 --> 00:01:02,609
線形性をほとんど
逸脱しないこの関数が

20
00:01:02,609 --> 00:01:06,495
複雑な非線形の解決策
につながるのは すばらしいことです

21
00:01:06,495 --> 00:01:08,469
より適切な活性化関数では

22
00:01:08,469 --> 00:01:12,605
微分係数を掛けて 任意の種類の
重みの微分係数を得る場合

23
00:01:12,605 --> 00:01:14,260
和は微分係数を少し小さくする

24
00:01:14,260 --> 00:01:18,715
少し大きい数字になり

25
00:01:18,715 --> 00:01:21,340
勾配降下法を行えるようになります

26
00:01:21,340 --> 00:01:25,510
その関数を描くことで
ReLUユニットを示します

27
00:01:25,510 --> 00:01:30,730
ここで多数のReLU活性化ユニットに
関する複数層パーセプトロンの例を挙げます

28
00:01:30,730 --> 00:01:33,405
最後のユニットがシグモイド
であることに注意してください

29
00:01:33,405 --> 00:01:38,385
まだ最終出力が０と１の間の確率
である必要があるためです

30
00:01:38,385 --> 00:01:40,969
しかし最終ユニットをReLUにすると

