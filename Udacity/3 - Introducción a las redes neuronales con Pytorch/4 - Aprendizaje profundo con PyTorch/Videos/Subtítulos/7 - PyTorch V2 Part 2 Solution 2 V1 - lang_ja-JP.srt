1
00:00:00,000 --> 00:00:04,919
おかえりなさい

2
00:00:04,919 --> 00:00:07,214
ソフトマックス関数についての解法です

3
00:00:07,214 --> 00:00:08,849
分子があって

4
00:00:08,849 --> 00:00:12,179
指数関数をとりたいので

5
00:00:12,179 --> 00:00:14,519
ここは torch.expを使えば
極めて簡単になります

6
00:00:14,519 --> 00:00:16,469
入力テンソルである ｘの指数関数を使います

7
00:00:16,469 --> 00:00:20,099
分母で行いたいことは

8
00:00:20,100 --> 00:00:23,150
指数関数つまり torch.expを実行して

9
00:00:23,149 --> 00:00:25,739
すべての値の和を得ることです

10
00:00:25,739 --> 00:00:29,939
１つの行全体の和を求めたいのを
頭に入れておきましょう

11
00:00:29,940 --> 00:00:33,914
つまり 各例の１つの行の列ごとの和です

12
00:00:33,914 --> 00:00:36,375
例えば この値を合計したいとします

13
00:00:36,375 --> 00:00:38,159
ここのtorch.sumの中で

14
00:00:38,159 --> 00:00:40,169
dim=１を使います

15
00:00:40,170 --> 00:00:44,195
これは 基本的に列全体の和を出します

16
00:00:44,195 --> 00:00:46,984
ここで torch.sumが行うことは

17
00:00:46,984 --> 00:00:50,344
テンソルを教えることで

18
00:00:50,344 --> 00:00:53,530
単に64のエレメントのベクトルです

19
00:00:53,530 --> 00:00:54,870
ここで問題なのは

20
00:00:54,869 --> 00:00:56,879
64×10の場合で

21
00:00:56,880 --> 00:01:00,115
これが64の長さのベクトルである場合

22
00:01:00,115 --> 00:01:06,575
このテンソルの中の各エレメントを
64の値すべてで

23
00:01:06,575 --> 00:01:11,450
割ろうとすることです

24
00:01:11,450 --> 00:01:13,355
64×64のテンソルが得られますが
求めたかったものとは違います

25
00:01:13,355 --> 00:01:20,090
出力は 64×10であってほしいのです

26
00:01:20,090 --> 00:01:22,344
実際に必要なことは
このテンソルを変えて 64行にして

27
00:01:22,344 --> 00:01:23,519
各行の値を１つだけにすることです

28
00:01:23,519 --> 00:01:27,670
何をするかというと

29
00:01:27,670 --> 00:01:31,504
ここはこのテンソルの中の各行を見るので

30
00:01:31,504 --> 00:01:35,179
このテンソルの中の同じ行を見ます

31
00:01:35,180 --> 00:01:40,550
このテンソルの各行は
値を１つしか持たないので

32
00:01:40,549 --> 00:01:42,700
この指数関数を
分母のテンソルの値で割ります

33
00:01:42,700 --> 00:01:49,278
かなり扱いにくいのですが

34
00:01:49,278 --> 00:01:52,099
PyTorchでブロードキャストが
どのように機能するのか

35
00:01:52,099 --> 00:01:55,509
正しい形状や正しい演算で
これらのテンソルをどう一致させ

36
00:01:55,510 --> 00:01:57,525
すべてを正しく行う方法を
理解するのは とても重要です

37
00:01:57,525 --> 00:02:01,340
このように状況を確認したら

38
00:02:01,340 --> 00:02:02,390
出力をsoftmax関数に渡して

39
00:02:02,390 --> 00:02:05,284
確率を得ます

40
00:02:05,284 --> 00:02:07,700
形状を見ると 64×10なので

41
00:02:07,700 --> 00:02:08,765
各行全体の和をとると

42
00:02:08,764 --> 00:02:11,339
結局１になります

43
00:02:11,340 --> 00:02:17,870
適切な確率分布を
使っていればそうなるはずです

44
00:02:17,870 --> 00:02:22,939
ではニューラルネットワークの構築に
このnn.Moduleをどう使うのかを見てみましょう

45
00:02:22,939 --> 00:02:28,564
実際には ものすごく単純で
強力だと分かるでしょう

46
00:02:28,564 --> 00:02:30,215
同じフレームワークで構築するニューラル
ネットワークをどんどん拡大できます

47
00:02:30,215 --> 00:02:32,375
一般的に機能する方法として
新しいクラスを生成し

48
00:02:32,375 --> 00:02:34,189
これをニューラルネットワークと呼んでも

49
00:02:34,189 --> 00:02:35,300
何でも構いませんが

50
00:02:35,300 --> 00:02:36,800
分類子でも

51
00:02:36,800 --> 00:02:38,430
MNISTでも構いません

52
00:02:38,430 --> 00:02:40,920
どう呼ぶかは重要ではありませんが

53
00:02:40,919 --> 00:02:46,125
これをnn.Moduleから
サブクラス化する必要があります

54
00:02:46,125 --> 00:02:51,360
initメソッドの中で つまりこの__init メソッドですが

55
00:02:51,360 --> 00:02:54,925
superで呼び出して nn.Moduleのinitメソッドを実行します

56
00:02:54,925 --> 00:02:57,120
これを行う必要があるのは

57
00:02:57,120 --> 00:02:59,284
このニューラルネットワークに入れることになる

58
00:02:59,284 --> 00:03:00,770
様々なレイヤーや演算を登録する必要があるのを
PyTorchに認識させるためです

59
00:03:00,770 --> 00:03:03,290
この部分を行わないと

60
00:03:03,289 --> 00:03:04,969
ニューラルネットワークに追加するものを 
追跡できず単純に機能しません

61
00:03:04,969 --> 00:03:06,740
ここでnn.Linearで 隠れ層を生成できます

62
00:03:06,740 --> 00:03:08,680
何をするかというと

63
00:03:08,680 --> 00:03:14,025
線型変換のための演算を作成します

64
00:03:14,025 --> 00:03:15,230
入力ｘをとって重みをかけて
バイアス項を足したら

65
00:03:15,229 --> 00:03:19,250
それが線型変換です

66
00:03:19,250 --> 00:03:24,370
何をするかというと nn.Linearの呼び出しで

67
00:03:24,370 --> 00:03:26,205
オブジェクトを作成し

68
00:03:26,205 --> 00:03:29,110
それ自体が重みと バイアスのパラメータを生成して

69
00:03:29,110 --> 00:03:32,330
その後テンソルを隠れ層へ渡す時に

70
00:03:32,330 --> 00:03:35,620
オブジェクトが 代わりに
線型変換を自動的に計算します

71
00:03:35,620 --> 00:03:39,270
行うべきことは 入力のサイズと

72
00:03:39,270 --> 00:03:43,540
出力のサイズを伝えることです

73
00:03:43,539 --> 00:03:47,539
784×256ということは

74
00:03:47,539 --> 00:03:48,769
ここでは256の出力を使います

75
00:03:48,770 --> 00:03:52,219
前に見たニューラルネットワークの
再構築のようなものです

76
00:03:52,219 --> 00:03:54,590
同じように 隠れユニットと出力の間にも

77
00:03:54,590 --> 00:03:58,104
別の線型変換がほしいと考えています

78
00:03:58,104 --> 00:04:04,114
ここでも 256の隠れユニットがあり

79
00:04:04,115 --> 00:04:06,695
10の出力 10の出力ユニットがあります

80
00:04:06,694 --> 00:04:09,689
そこでself.outputという名前の
出力層を生成して

81
00:04:09,689 --> 00:04:14,120
この線型変換の演算を作成します

82
00:04:14,120 --> 00:04:17,530
私たちはまた 有効化のために
シグモイド演算を作成し

83
00:04:17,529 --> 00:04:21,469
出力のソフトマックス関数も作成して

84
00:04:21,470 --> 00:04:23,150
この確率分布を得たいと考えています

85
00:04:23,149 --> 00:04:25,134
今からforwardメソッドを生成します

86
00:04:25,134 --> 00:04:28,814
forwardでは基本的に

87
00:04:28,814 --> 00:04:31,110
テンソルをニューラルネットワークに渡すと

88
00:04:31,110 --> 00:04:34,889
すべての演算を経て

89
00:04:34,889 --> 00:04:36,519
最終的に出力が得られます

90
00:04:36,519 --> 00:04:37,944
ここでは 引数のｘがテンソルの入力になって

91
00:04:37,944 --> 00:04:41,120
引数ｘを隠れ層に渡します

92
00:04:41,120 --> 00:04:43,560
これは上の行で定義した
このような線型変換で

93
00:04:43,560 --> 00:04:47,095
シグモイド活性化関数を経ることになります

94
00:04:47,095 --> 00:04:50,510
その後 出力層もしくは上の行にある
出力層変換を経て

95
00:04:50,509 --> 00:04:55,664
シグモイド関数を経ます

96
00:04:55,665 --> 00:04:57,924
最後に ソフトマックス関数の出力を戻します

97
00:04:57,923 --> 00:05:02,060
これで生成できます

98
00:05:02,060 --> 00:05:03,725
このあたりを見てみると 出力していて

99
00:05:03,725 --> 00:05:07,140
演算を示します

100
00:05:07,139 --> 00:05:10,399
オーダとは限りませんが

101
00:05:10,399 --> 00:05:14,844
少なくとも このニューラルネットワークに
対して定義された演算が分かります

102
00:05:14,845 --> 00:05:20,375
シグモイドやソフトマックスといったものの
関数的定義も利用します

103
00:05:20,375 --> 00:05:24,949
これでクラス 皆さんのコードの
書き方が少しすっきりします

104
00:05:24,949 --> 00:05:27,784
これは torch.nn.functionalから得られます

105
00:05:27,785 --> 00:05:32,689
よく目にする書き方は
“import torch.nn.functional as F”でしょう

106
00:05:32,689 --> 00:05:35,545
PyTorchコードの
決まりごとのようなものです

107
00:05:35,545 --> 00:05:38,970
ここでも線型変換 self.hiddenや
self.output を定義しますが

108
00:05:38,970 --> 00:05:43,335
forwardメソッドにおいては

109
00:05:43,334 --> 00:05:47,909
隠れ層の値を得るために
self.hiddenを呼び出せます

110
00:05:47,910 --> 00:05:49,785
ただし その値をシグモイド関数
F.sigmoidに渡して

111
00:05:49,785 --> 00:05:53,310
出力層にも同じようにします

112
00:05:53,310 --> 00:05:56,990
これでこの出力の線型変換が得られましたので

113
00:05:56,990 --> 00:05:59,509
ソフトマックス演算に渡します

114
00:05:59,509 --> 00:06:01,865
これが可能な理由は

115
00:06:01,865 --> 00:06:04,444
線型変換を生成した際に

116
00:06:04,444 --> 00:06:09,665
重みとバイアスの行列を
自ら生成しているためです

117
00:06:09,665 --> 00:06:11,870
しかし シグモイド関数とソフト
マックス関数にとっては要素ごとの演算に過ぎず

118
00:06:11,870 --> 00:06:14,329
このような演算を行うために
追加のパラメータや

119
00:06:14,329 --> 00:06:17,120
追加の行列を生成する必要がなく

120
00:06:17,120 --> 00:06:19,550
オブジェクトやクラスの類を生成する
必要のない純粋な関数として扱うことができます

121
00:06:19,550 --> 00:06:22,353
ただしそれらの関数は平等です

122
00:06:22,353 --> 00:06:25,699
このニューラルネットワーク構築方法は
上の行に対して平等ですが

123
00:06:25,699 --> 00:06:28,384
こういった関数パターンを使う際は

124
00:06:28,384 --> 00:06:33,439
もう少し簡潔です

125
00:06:33,439 --> 00:06:36,259
今のところ 活性化関数として
シグモイド関数だけを使っていますが

126
00:06:36,259 --> 00:06:39,490
使いたい関数が 他にもたくさんあるでしょう

127
00:06:39,490 --> 00:06:44,150
ただ１つの要件は

128
00:06:44,149 --> 00:06:45,364
これらの活性化関数は
通常非線型でなければならないことです

129
00:06:45,365 --> 00:06:47,660
ニューラルネットワークが非線型の相関関係
およびパターンを学習できるようにして

130
00:06:47,660 --> 00:06:49,700
出力を非線型にしたい場合は

131
00:06:49,699 --> 00:06:53,300
隠れ層の非線型の
活性化関数を使う必要があります

132
00:06:53,300 --> 00:07:00,259
シグモイド関数は例の１つです

133
00:07:00,259 --> 00:07:02,745
別の例には 双曲線正接関数があります

134
00:07:02,745 --> 00:07:07,009
常によく使われていて

135
00:07:07,009 --> 00:07:09,800
活性化関数および隠れ層として
ほぼ独占的なのが

136
00:07:09,800 --> 00:07:12,064
ランプ（ReLU）関数です

137
00:07:12,064 --> 00:07:14,509
基本的に使用できる
最も単純な非線型関数です

138
00:07:14,509 --> 00:07:17,879
ランプ関数を使用すると
シグモイド関数や双曲線正接関数に比べ

139
00:07:17,879 --> 00:07:20,670
ニューラルネットワークの訓練が
かなり速くなる傾向があります

140
00:07:20,670 --> 00:07:24,845
そのため 通常ランプ関数を使用します

141
00:07:24,845 --> 00:07:28,330
より大きなニューラルネットワークを
構築しようとしています

142
00:07:28,329 --> 00:07:32,810
今回は 隠れ層が２つあり

143
00:07:32,810 --> 00:07:35,464
各自の隠れ層でランプ活性化関数
を使用することになります

144
00:07:35,464 --> 00:07:40,159
.module内のオブジェクト
指向のクラスメソッドで

145
00:07:40,160 --> 00:07:43,170
このような形のニューラルネットワークを構築してください

146
00:07:43,170 --> 00:07:49,009
784の入力ユニットがあり

147
00:07:49,009 --> 00:07:55,519
１つ目の隠れ層には 128のユニット

148
00:07:55,519 --> 00:07:59,134
２つ目の隠れ層には 64のユニット

149
00:07:59,134 --> 00:08:00,935
そして10の出力ユニットを持つものです

150
00:08:00,935 --> 00:08:03,560
それでは！

