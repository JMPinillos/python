1
00:00:00,000 --> 00:00:04,530
こんにちは

2
00:00:04,530 --> 00:00:05,910
転移学習の練習に対する解決策です

3
00:00:05,910 --> 00:00:09,900
少し変わっているので

4
00:00:09,900 --> 00:00:14,565
この授業で学習した内容を組み合わせて

5
00:00:14,564 --> 00:00:16,785
プロセスを理解できるように書き出してみます

6
00:00:16,785 --> 00:00:18,899
利用できるGPUがある場合に 最初にすることは

7
00:00:18,899 --> 00:00:23,894
GPUを使用できるようにコードを
アグノスティックに書きます

8
00:00:23,894 --> 00:00:30,254
したがってdevice = torch.device
とし"cuda"になります

9
00:00:30,254 --> 00:00:40,289
torch.cuda.is_available() else "cpu"
の場合に GPUを実行します

10
00:00:40,289 --> 00:00:42,240
これが 実行するのは

11
00:00:42,240 --> 00:00:44,565
GPUが利用可能の場合

12
00:00:44,564 --> 00:00:50,329
これはTrueになり ここに"cuda"を返し
その他の場合は "cpu"になります

13
00:00:50,329 --> 00:00:53,989
deviceをすべてのテンソルと
モデルに受け渡せるようになり

14
00:00:53,990 --> 00:00:57,700
利用可能であれば GPUに自動的に移動します

15
00:00:57,700 --> 00:01:02,490
次に 事前にトレーニングされている
modelを取得します

16
00:01:02,490 --> 00:01:04,439
ここでは resnetを使用します

17
00:01:04,439 --> 00:01:07,289
これには model.modelとします

18
00:01:07,290 --> 00:01:09,140
torchvisionからmodelを 既にインポートしていて

19
00:01:09,140 --> 00:01:12,849
あるものすべてを確認できます

20
00:01:12,849 --> 00:01:14,564
ここにresnetがあります

21
00:01:14,564 --> 00:01:16,545
resnet50という 比較的小さなものを使用します

22
00:01:16,545 --> 00:01:22,905
pretrained=Trueとし これでmodelが得られます

23
00:01:22,905 --> 00:01:24,629
確認すると

24
00:01:24,629 --> 00:01:27,649
このように出力され

25
00:01:27,650 --> 00:01:31,490
様々なオペレーションと層 
実行されているすべてが伝えられます

26
00:01:31,489 --> 00:01:36,259
下にスクロールすると 終わりの
部分に fcがあるのが分かります

27
00:01:36,260 --> 00:01:38,300
したがって これが最後の層で

28
00:01:38,299 --> 00:01:40,840
分類子のように 機能しているフル接続の層です

29
00:01:40,840 --> 00:01:42,480
この層は 2,048の入力が予想され

30
00:01:42,480 --> 00:01:47,659
out_featuresは 1,000だとわかります

31
00:01:47,659 --> 00:01:51,829
これがImageNetで トレーニング
されていると思い出してください

32
00:01:51,829 --> 00:01:56,609
ImageNetは 通常1,000の異なる
画像クラスでトレーニングされます

33
00:01:56,609 --> 00:01:58,719
ここでは ネコとイヌだけを使用していますので

34
00:01:58,719 --> 00:02:02,034
機能を出力し 分類子を
入力する必要があるだけです

35
00:02:02,034 --> 00:02:06,409
このようなmodelを読み込み modelの周囲が

36
00:02:06,409 --> 00:02:08,629
フリーズされていることを確認し

37
00:02:08,629 --> 00:02:11,615
トレーニングしている時に 更新されないようにします

38
00:02:11,615 --> 00:02:14,275
したがって 必ずうまくいくように実行します

39
00:02:14,275 --> 00:02:19,325
modelを読み込み 勾配をオフにできます

40
00:02:19,324 --> 00:02:21,979
modelに対して 勾配をオフにします

41
00:02:21,979 --> 00:02:24,965
次の手順は トレーニングする
新しい分類子を定義することです

42
00:02:24,965 --> 00:02:29,974
ここでは かなりシンプルにできます

43
00:02:29,974 --> 00:02:31,715
model = nn.Sequentialです

44
00:02:31,715 --> 00:02:34,849
これは様々な方法で定義できますので
nn.Sequentialを使用します

45
00:02:34,849 --> 00:02:36,439
１つ目の層は Linearです

46
00:02:36,439 --> 00:02:39,219
2,048の入力が必要であることを思い出してください

47
00:02:39,219 --> 00:02:42,145
ReLUの層では 512に下がります
Dropoutです

48
00:02:42,145 --> 00:02:47,585
次に出力層で

49
00:02:47,585 --> 00:02:53,080
(512, 2)でLogSoftmaxを行います

50
00:02:53,080 --> 00:02:54,840
分類子になるよう 変更する必要があります

51
00:02:54,840 --> 00:03:01,950
これで分類子を見つけることができ
modelに割り当てられます

52
00:03:01,949 --> 00:03:05,849
model.fc = classifierと
言えるようにするためです

53
00:03:05,849 --> 00:03:09,590
modelを再度確認すると

54
00:03:09,590 --> 00:03:13,580
ここで下にスクロールできます

55
00:03:13,580 --> 00:03:16,639
このフル接続のモジュール層を
確認できますが

56
00:03:16,639 --> 00:03:19,270
Sequentialな分類子Linearなオペレーション
ReLUがあります　dropout

57
00:03:19,270 --> 00:03:23,960
別のLinearのトランスフォーメーション
そしてLogSoftmaxです

58
00:03:23,960 --> 00:03:28,550
次に行うのは 損失であるcriterionを定義することです

59
00:03:28,550 --> 00:03:32,740
これは損失を被ったような 負のログになります

60
00:03:32,740 --> 00:03:36,740
次にoptimizerである
optim.Adamを定義します

61
00:03:36,740 --> 00:03:40,700
ここでは fcである分類子のパラメーターを
使用してから 学習率を設定します

62
00:03:40,699 --> 00:03:47,935
最後に modelを利用可能な
任意のデバイスに移動します

63
00:03:47,935 --> 00:03:52,129
すべてのmodelを設定したので トレーニングします

64
00:03:52,129 --> 00:03:57,159
最初に トレーニング中に使用する
いくつかの変数を定義します

65
00:03:57,159 --> 00:04:03,639
例えば epochsを設定します
１つここでやってみます

66
00:04:03,639 --> 00:04:07,824
トレーニングの手順の数を
追跡することになりますので

67
00:04:07,824 --> 00:04:10,458
これを０に設定し

68
00:04:10,459 --> 00:04:13,430
損失を追跡することになりますので

69
00:04:13,430 --> 00:04:16,579
これも０に設定しましょう

70
00:04:16,579 --> 00:04:19,129
最後に 検証の損失を出力する前に

71
00:04:19,129 --> 00:04:20,555
実行する手順数のループを
設定する必要があります

72
00:04:20,555 --> 00:04:22,644
そこでエポックを通じてループします

73
00:04:22,644 --> 00:04:26,214
epoch in range(epochs)に対してです

74
00:04:26,214 --> 00:04:29,269
images, labels in trainloader
の累積手順のデータをループします

75
00:04:29,269 --> 00:04:33,819
これらのバッチのいずれかを利用する度に
手順が増えることになります

76
00:04:33,819 --> 00:04:36,959
しがたって 画像とラベルのある今

77
00:04:36,959 --> 00:04:41,729
利用可能であれば それらをGPUに移動します

78
00:04:41,730 --> 00:04:46,245
ですから 今行うのは
images.to (device) labels.to(device)です

79
00:04:46,245 --> 00:04:52,004
トレーニングループを書き出します

80
00:04:52,004 --> 00:04:54,500
最初にやるべきなのは 勾配を０にすることです

81
00:04:54,500 --> 00:04:56,589
非常に重要ですので 忘れないでください

82
00:04:56,589 --> 00:04:58,849
次にログの確率のある画像で

83
00:04:58,850 --> 00:05:04,160
受け渡されるmodelから確率を得てください

84
00:05:04,160 --> 00:05:13,685
ラベルの基準から 損失を得ることができます

85
00:05:13,685 --> 00:05:17,819
次に バックワードパスを実行し
オプティマイザーで手順を実行します

86
00:05:17,819 --> 00:05:22,680
ここで増分できるのは running_lossのようです

87
00:05:22,680 --> 00:05:25,170
より多くのデータを利用する際に
トレーニングの損失の追跡できます

88
00:05:25,170 --> 00:05:28,730
これがトレーニングループです

89
00:05:28,730 --> 00:05:33,170
時折 それぞれの変数の出力のように設定されるのです

90
00:05:33,170 --> 00:05:37,520
ここでトレーニングのループを
ドロップアウトして

91
00:05:37,519 --> 00:05:42,664
テストデータセットのネットワークの
精度と損失をテストしたいと思います

92
00:05:42,665 --> 00:05:47,629
print_every変数では

93
00:05:47,629 --> 00:05:50,029
これは０で次に検証ループに入ります

94
00:05:50,029 --> 00:05:52,439
最初にやることは model.evalです

95
00:05:52,439 --> 00:05:54,074
modelは 評価推論モードに代わり
dropoutをオフにします

96
00:05:54,074 --> 00:05:58,729
ですから test_lossと accuracyを得る代わりに

97
00:05:58,730 --> 00:06:02,685
ネットワークを正確に
使用して 予測を行うことができます

98
00:06:02,685 --> 00:06:07,399
テストデータから画像とラベルを
得られるようになりました

99
00:06:07,399 --> 00:06:12,394
検証ループを実行します

100
00:06:12,394 --> 00:06:15,859
したがって modelでは画像を受け渡します

101
00:06:15,860 --> 00:06:18,480
これらはテストセットの画像です

102
00:06:18,480 --> 00:06:24,965
テストセットからlogpsを得るため

103
00:06:24,964 --> 00:06:28,669
criterionのlossを得て

104
00:06:28,670 --> 00:06:34,259
test_loss += loss.itemの損失を追跡します

105
00:06:34,259 --> 00:06:40,310
これらの検証ルールを通じて

106
00:06:40,310 --> 00:06:42,410
テスト損失を追跡し
続けられるようになります

107
00:06:42,410 --> 00:06:47,030
次に 精度を計算したいと思います

108
00:06:47,029 --> 00:06:49,479
確率(ps)は torch.ex(logps)です

109
00:06:49,480 --> 00:06:53,835
modelが LogSoftmaxを
返していることを思い出してください

110
00:06:53,834 --> 00:07:01,279
これはクラスのログの確率であり

111
00:07:01,279 --> 00:07:04,279
実際の確率を得るには torch.exを利用します

112
00:07:04,279 --> 00:07:08,199
トップの確率と トップのクラスを
ps.topk(1)から得ます

113
00:07:08,199 --> 00:07:12,019
これにより 確率の最初の最大値が得られます

114
00:07:12,019 --> 00:07:17,389
ここでは次元を１に必ず設定して
それが列に沿ってトップの確率を

115
00:07:17,389 --> 00:07:20,789
実際に探すようにする必要があります

116
00:07:20,790 --> 00:07:25,460
トップのクラスに移動し

117
00:07:25,459 --> 00:07:27,379
ラベルでequalityをチェックしてから

118
00:07:27,379 --> 00:07:34,230
精度を更新できます

119
00:07:34,230 --> 00:07:39,950
ここでは equalityから精度を
計算できることを覚えておいてください

120
00:07:39,949 --> 00:07:43,490
FloatTensorへ変更するとtorch.meanを実行し

121
00:07:43,490 --> 00:07:47,215
精度を得ることができます

122
00:07:47,214 --> 00:07:48,759
増分累積の一種を この精度
の変数に利用するだけでいいです

123
00:07:48,759 --> 00:07:56,300
ループの中にいるので このforの
手順を各print_everyに実行します

124
00:07:56,300 --> 00:07:58,280
基本的に トレーニング
の損失のrunning_lossがあり

125
00:07:58,279 --> 00:08:03,154
modelを通じた テストデータに合格し
精度の損失を測定したテストの損失があります

126
00:08:03,154 --> 00:08:05,659
これらすべてを出力し

127
00:08:05,660 --> 00:08:08,210
コピーペーストします
入力する多数のデータがあるためです

128
00:08:08,209 --> 00:08:13,319
ここでは エポックを出力しているだけです

129
00:08:13,319 --> 00:08:18,305
自分の場所を 追跡で把握でき
それを追跡することができます

130
00:08:18,305 --> 00:08:23,375
running_loss/print_everyは

131
00:08:23,375 --> 00:08:26,240
基本的には トレーニングの損失の平均です

132
00:08:26,240 --> 00:08:29,139
出力する度に 平均をとるだけなのです

133
00:08:29,139 --> 00:08:30,854
次に test_loss/len(testloader)です

134
00:08:30,855 --> 00:08:33,580
testloaderの長さは いくつのバッチが
testloaderから得ている

135
00:08:33,580 --> 00:08:37,070
テストデータセットに実際にあるかを示しています

136
00:08:37,070 --> 00:08:40,475
各バッチに対して すべての損失を
合計しているため

137
00:08:40,475 --> 00:08:43,610
合計の損失を選択して バッチ数で割ると

138
00:08:43,610 --> 00:08:46,639
平均の損失が得られます

139
00:08:46,639 --> 00:08:47,855
精度でも同じことをします

140
00:08:47,855 --> 00:08:49,610
各バッチの精度を ここで合計してから

141
00:08:49,610 --> 00:08:52,909
バッチの合計数で割ると

142
00:08:52,909 --> 00:08:56,629
テストセットの平均の精度が得られます

143
00:08:56,629 --> 00:09:00,539
最後に running_lossの設定を０に戻してから

144
00:09:00,539 --> 00:09:03,949
modelを トレーニングモードに戻します

145
00:09:03,950 --> 00:09:05,750
 トレーニングコードは 終了です
うまく機能するか確認します

146
00:09:05,750 --> 00:09:07,909
ここはforではなく ifにするべきです

147
00:09:07,909 --> 00:09:09,814
忘れがちなことです

148
00:09:09,815 --> 00:09:13,820
テンソルをGPUに移転するのを忘れています

149
00:09:13,820 --> 00:09:18,540
これでうまくいくといいですね

150
00:09:18,539 --> 00:09:19,879
かなり素早く 95%を上回る確率で
テストの精度を得られます

151
00:09:19,879 --> 00:09:23,600
５つの手順ごとに これを出力し

152
00:09:23,600 --> 00:09:29,095
全部で15のバッチにすることを 忘れないでください

153
00:09:29,095 --> 00:09:34,475
モデルで更新されたトレーニングバッチです

154
00:09:34,475 --> 00:09:38,409
これらの分類子をトップで微調整し

155
00:09:38,409 --> 00:09:41,944
データセットで95%を上回る精度を
得るのは簡単です

