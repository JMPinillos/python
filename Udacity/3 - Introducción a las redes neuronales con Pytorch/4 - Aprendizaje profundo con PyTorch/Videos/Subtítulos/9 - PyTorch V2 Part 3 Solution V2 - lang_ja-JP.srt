1
00:00:00,000 --> 00:00:05,955
またお会いしましたね

2
00:00:05,955 --> 00:00:09,914
LogSoftmax出力使用のモデルの回答です

3
00:00:09,914 --> 00:00:12,359
インデックスシーケンシャルで
構築したものとかなり似ています

4
00:00:12,359 --> 00:00:14,160
線形変換 ReLU 線形変換 ReLU

5
00:00:14,160 --> 00:00:16,859
別の線形変換を出力に
使用してからこれを

6
00:00:16,859 --> 00:00:20,054
LogSoftmaxモジュールに渡します

7
00:00:20,054 --> 00:00:22,410
ここでは
LogSoftmaxに対して次元を

8
00:00:22,410 --> 00:00:25,500
必ず１に設定します

9
00:00:25,500 --> 00:00:31,214
これで 行ではなく 列をまたがって
関数が計算されます

10
00:00:31,214 --> 00:00:34,969
行は例に一致するのを覚えていますか

11
00:00:34,969 --> 00:00:37,640
ネットワークに渡す例が多く

12
00:00:37,640 --> 00:00:40,750
各行はこれらの例の１つです

13
00:00:40,750 --> 00:00:42,799
必ずソフトマックス関数を

14
00:00:42,799 --> 00:00:46,159
各例にまたがって

15
00:00:46,159 --> 00:00:52,239
網羅させます

16
00:00:52,240 --> 00:00:56,060
バッチの個別の機能は
網羅しないようにする必要があります

17
00:00:56,060 --> 00:00:58,490
ここでは損失または基準を負のログの

18
00:00:58,490 --> 00:01:01,250
可能性損失として定義しているだけです

19
00:01:01,250 --> 00:01:04,310
trainloaderからimagesとlabelsを取得し

20
00:01:04,310 --> 00:01:07,379
それらをフラット化してモデルを
通じて渡してlogitsを得ます

21
00:01:07,379 --> 00:01:09,843
これは実はもう
最大ではありません

22
00:01:09,843 --> 00:01:11,449
これはログの確率のようなものです

23
00:01:11,450 --> 00:01:14,344
ですからこれをlogpsと呼びます

24
00:01:14,344 --> 00:01:17,319
進めましょう

25
00:01:17,319 --> 00:01:19,968
さて損失をうまく得ました

26
00:01:19,968 --> 00:01:23,194
損失の計算方法が分かりましたが

27
00:01:23,194 --> 00:01:26,089
実際にどのように使用して
誤差逆伝播法を行うのでしょうか？

28
00:01:26,090 --> 00:01:30,460
PyTorchには Autogradという
すばらしいモジュールがあります

29
00:01:30,459 --> 00:01:32,549
テンソルの勾配を自動的に計算します

30
00:01:32,549 --> 00:01:35,209
仕組みは次の通りです

31
00:01:35,209 --> 00:01:39,634
PyTorchがテンソルで行うすべての
操作の追跡を行い

32
00:01:39,635 --> 00:01:41,630
バックワードパスで
逆向きの受け渡しをして

33
00:01:41,629 --> 00:01:47,015
入力パラメータに関して勾配を計算します

34
00:01:47,015 --> 00:01:53,825
一般的に特定のテンソルで

35
00:01:53,825 --> 00:01:59,025
Autogradを使用する
PyTorchを把握する必要があります

36
00:01:59,025 --> 00:02:00,590
この場合x = torch.zerosのような
テンソルを作成します

37
00:02:00,590 --> 00:02:03,844
ただスカラーにするためにです

38
00:02:03,844 --> 00:02:09,620
１としてrequires_grad = Trueとします

39
00:02:09,620 --> 00:02:13,539
これがPyTorchにこのテンソルxでの
操作を追跡するように伝えます

40
00:02:13,539 --> 00:02:16,099
勾配を得る場合には計算されます

41
00:02:16,099 --> 00:02:19,310
一般的にはテンソルを作成し

42
00:02:19,310 --> 00:02:21,789
勾配を計算しない場合には

43
00:02:21,789 --> 00:02:26,194
必ずこれをfalseに設定してください

44
00:02:26,194 --> 00:02:28,400
このコンテキストtorch.no_gradを利用して

45
00:02:28,400 --> 00:02:31,254
すべての勾配が行っている全操作を
シャットオフすることもできます

46
00:02:31,254 --> 00:02:35,000
このコンテキスト内の場合にです

47
00:02:35,000 --> 00:02:39,060
それからtorch.set_grad_enabledで
勾配をオン/オフにして

48
00:02:39,060 --> 00:02:44,520
TrueまたはFalseにすることもできます
何をするかにより異なります

49
00:02:44,520 --> 00:02:49,290
これがPyTorchと機能する方法は
基本的にテンソルと作成することです

50
00:02:49,289 --> 00:02:53,359
requires_grad = Trueを設定してから
いくつかの操作を実行するだけです

51
00:02:53,360 --> 00:02:56,060
操作を完了したら
.backwardsを入力します

52
00:02:56,060 --> 00:02:59,000
xを使用したら テンソルxになり

53
00:02:59,000 --> 00:03:04,594
他のテンソルzを計算します

54
00:03:04,594 --> 00:03:07,370
z.backwardを行ったら

55
00:03:07,370 --> 00:03:12,319
操作を通じて戻り
xの勾配すべてを計算します

56
00:03:12,319 --> 00:03:15,889
例えば ランダムなテンソルを
作成するだけの場合

57
00:03:15,889 --> 00:03:18,319
ランダムな(２, ２)のテンソルで
このように２乗にできます

58
00:03:18,319 --> 00:03:20,405
yを見ると確認できます

59
00:03:20,405 --> 00:03:25,125
yはセカンダリのものあるいは
２乗のテンソルです

60
00:03:25,125 --> 00:03:28,340
y.grad_fnを見ると

61
00:03:28,340 --> 00:03:31,914
このgrad関数はべき関数であると
示されています

62
00:03:31,914 --> 00:03:36,334
したがってPyTorchはこれを追跡するだけで

63
00:03:36,335 --> 00:03:40,145
最後に行われた操作がべき乗の操作
だったと分かります

64
00:03:40,145 --> 00:03:42,710
yの平均を取り別の
テンソルzを得ます

65
00:03:42,710 --> 00:03:45,849
これはスカラーテンソルなので
yを減らしています

66
00:03:45,849 --> 00:03:49,219
yは(２, ２)のマトリックスで

67
00:03:49,219 --> 00:03:53,870
(２, ２)の配列で
その平均をとってzを得ます

68
00:03:53,870 --> 00:03:56,330
テンソルの構成要素が
この属性gradに示されます

69
00:03:56,330 --> 00:03:59,410
テンソルxの勾配を
これで確認できます

70
00:03:59,409 --> 00:04:01,275
これをフォワードパスだけで行っています

71
00:04:01,275 --> 00:04:05,550
まだ勾配を実際には計算しておらず
今行いました

72
00:04:05,550 --> 00:04:07,430
次にz.backwardを行うと

73
00:04:07,430 --> 00:04:10,795
既に行った操作のセットを通じて
後ろ向きに進みます

74
00:04:10,794 --> 00:04:12,739
べき乗して 平均値をとり

75
00:04:12,740 --> 00:04:15,290
これを通じて後ろ向きに進み
xの勾配を計算します

76
00:04:15,289 --> 00:04:18,379
数学の知識を駆使すると

77
00:04:18,379 --> 00:04:21,680
xに関するzの勾配がx/2になり

78
00:04:21,680 --> 00:04:26,435
勾配を見ると

79
00:04:26,435 --> 00:04:29,209
x/2も視ることができ
それらは同じです

80
00:04:29,209 --> 00:04:31,430
したがって勾配は数学的に
あるべき姿と同一です

81
00:04:31,430 --> 00:04:32,810
これはAutogradやPyTorchといった
勾配の作業を行うための

82
00:04:32,810 --> 00:04:36,579
一般的なプロセスです

83
00:04:36,579 --> 00:04:42,349
これが役立つのは

84
00:04:42,350 --> 00:04:44,960
損失を計算するときに
勾配を得るのに利用できるからです

85
00:04:44,959 --> 00:04:48,169
損失が重みやバイアスのパラメータ
により異なることを思い出してください

86
00:04:48,170 --> 00:04:52,960
勾配降下法を行うには 重みの勾配が必要です

87
00:04:52,959 --> 00:04:57,229
重みを勾配の必要とする
テンソルとして設定し

88
00:04:57,230 --> 00:04:59,990
損失を計算するためにフォワードパス
を行います

89
00:04:59,990 --> 00:05:02,930
損失では 重みの勾配を計算する
バックワードパスを行ってから

90
00:05:02,930 --> 00:05:07,535
これらの勾配で勾配降下法の手順を
行えます

91
00:05:07,535 --> 00:05:10,220
コードでそれがどのように
なるかを説明します

92
00:05:10,220 --> 00:05:13,625
LogSoftmaxの出力で
前に行ったように モデルを定義します

93
00:05:13,625 --> 00:05:15,949
負のログの可能性損失を利用して

94
00:05:15,949 --> 00:05:18,800
trainloaderからimagesとlabelsを取得し

95
00:05:18,800 --> 00:05:20,930
それらを平坦化してモデルから
ログの確率を得てから

96
00:05:20,930 --> 00:05:23,750
それを基準に渡します

97
00:05:23,750 --> 00:05:29,600
これで実際の損失が得られます

98
00:05:29,600 --> 00:05:31,715
モデルの重みを見ると

99
00:05:31,714 --> 00:05:34,369
モデル０がこの１つ目の
線形変換のためのパラメータが得られます

100
00:05:34,370 --> 00:05:38,259
重みを見てから勾配を見ると

101
00:05:38,259 --> 00:05:40,569
損失から開始する
バックワードパスが行われ

102
00:05:40,569 --> 00:05:42,694
重みの勾配を再度確認できます

103
00:05:42,694 --> 00:05:45,425
バックワードパスの前に見ると

104
00:05:45,425 --> 00:05:47,629
確認できません
まだ計算していないからです

105
00:05:47,629 --> 00:05:51,714
しかし バックワードパスの後には

106
00:05:51,714 --> 00:05:54,769
勾配が計算されています

107
00:05:54,769 --> 00:05:57,829
これらの勾配を勾配降下法で使用して
ネットワークをトレーニングできます

108
00:05:57,829 --> 00:06:00,979
損失を計算する方法が分かりました

109
00:06:00,980 --> 00:06:04,850
勾配を計算するために
損失を使用する方法を学びました

110
00:06:04,850 --> 00:06:09,230
トレーニングを開始する前に
１つやり残したことがあります

111
00:06:09,230 --> 00:06:14,345
重みを更新するために
勾配の使い方を確認しましょう

112
00:06:14,345 --> 00:06:18,184
optimizerを使用します
これはPyTorchのOptimパッケージにあります

113
00:06:18,184 --> 00:06:22,395
例として optim.SGDの
確率的勾配降下法を使用します

114
00:06:22,394 --> 00:06:23,939
これを定義するには PyTorchの
このモジュールoptimを

115
00:06:23,939 --> 00:06:25,894
インポートしてからoptim.SGDを入力し

116
00:06:25,894 --> 00:06:29,854
model.parametersと入力します

117
00:06:29,855 --> 00:06:32,245
これがoptimizerに実際に更新させる
パラメータになります

118
00:06:32,245 --> 00:06:36,110
そして学習率を入力すると

119
00:06:36,110 --> 00:06:39,439
optimizerが生成されます

120
00:06:39,439 --> 00:06:43,219
したがってトレーニングパスは
４つの異なる手順で構成されています

121
00:06:43,220 --> 00:06:45,350
ますネットワークを通じてフォーワード
パスを行ってから

122
00:06:45,350 --> 00:06:48,540
そのネットワーク出力を利用して
損失を計算します

123
00:06:48,540 --> 00:06:52,715
そしてloss.のネットワークを通じて

124
00:06:52,714 --> 00:06:56,089
バックワードパスを実行します
すると勾配が計算されます

125
00:06:56,089 --> 00:07:00,409
次に重みを更新するoptimizerの
手順を実行します

126
00:07:00,410 --> 00:07:04,189
１つのトレーニング手順でこれが
どのように機能するか説明した後

127
00:07:04,189 --> 00:07:05,795
ネットワークをトレーニングするためと
ループのために書く作業があります

128
00:07:05,795 --> 00:07:08,350
trainloaderから通常行うように

129
00:07:08,350 --> 00:07:11,910
最初にimagesやlabelsを 取得するところから始め

130
00:07:11,910 --> 00:07:14,970
その後平坦化します

131
00:07:14,970 --> 00:07:18,185
次に勾配の消去を行います

132
00:07:18,185 --> 00:07:21,829
PyTorchはデフォルトで勾配を累積します

133
00:07:21,829 --> 00:07:24,629
これは 複数のフォーワードパスと
複数のバックワードパスのように

134
00:07:24,629 --> 00:07:27,100
複数のパスと複数のバックワードを

135
00:07:27,100 --> 00:07:29,850
実際に行い 勾配を計算し続けると

136
00:07:29,850 --> 00:07:33,680
これらの勾配の合算が行われ
続けることを意味します

137
00:07:33,680 --> 00:07:36,019
勾配の消去を行わないと

138
00:07:36,019 --> 00:07:38,724
現在のトレーニング手順にある前の
トレーニング手順から勾配を

139
00:07:38,725 --> 00:07:40,290
得ることになり 最終的に

140
00:07:40,290 --> 00:07:45,710
ネットワークが適切に
トレーニングされません

141
00:07:45,709 --> 00:07:48,560
ですから一般的には

142
00:07:48,560 --> 00:07:52,120
トレーニングパスごとに
zero_gradを呼び出します

143
00:07:52,120 --> 00:07:54,480
optimizer.zero_gradを入力し
optimizerの

144
00:07:54,480 --> 00:07:59,355
すべての勾配とすべてのパラメータを消去します

145
00:07:59,355 --> 00:08:00,569
これで トレーニングが適切に
行われるようになります

146
00:08:00,569 --> 00:08:03,219
これはPyTorchの中で
忘れやすい部分となっていますが

147
00:08:03,220 --> 00:08:06,090
非常に重要なので

148
00:08:06,089 --> 00:08:07,724
しっかり覚えておきましょう

149
00:08:07,725 --> 00:08:09,010
次にフォーワードパス
バックワードパスをしてから

150
00:08:09,009 --> 00:08:12,110
重みを更新します

151
00:08:12,110 --> 00:08:15,720
そして出力を得ます

152
00:08:15,720 --> 00:08:20,360
imagesでモデルを通じてフォワード
パスを行います

153
00:08:20,360 --> 00:08:23,675
次にモデルの出力とラベルで
損失を計算してから

154
00:08:23,675 --> 00:08:26,150
バックワードパスを行い

155
00:08:26,149 --> 00:08:28,009
いよいよoptimizerの手順を実行します

156
00:08:28,009 --> 00:08:30,349
最初の重みを見ると

157
00:08:30,350 --> 00:08:32,330
このように見えますので
勾配から計算できます

158
00:08:32,330 --> 00:08:35,870
勾配はこのように見えます

159
00:08:35,870 --> 00:08:39,560
optimizerの手順を実行して
重みを更新すると

160
00:08:39,559 --> 00:08:42,184
重みが変更されます

161
00:08:42,184 --> 00:08:44,059
一般的に 内容はトレーニング
セットを通じてループされ

162
00:08:44,059 --> 00:08:46,179
トレーニングセットの各バッチに対して

163
00:08:46,179 --> 00:08:51,319
同じトレーニングパスが行われます

164
00:08:51,320 --> 00:08:52,925
データを得てから

165
00:08:52,924 --> 00:08:56,284
勾配を消去し

166
00:08:56,284 --> 00:09:01,639
これらのimagesや入力をネットワークを
通じて受け渡して 出力を得ます

167
00:09:01,639 --> 00:09:05,870
labelsで損失を計算してから

168
00:09:05,870 --> 00:09:10,440
損失でバックワードパスを実行してから
重みを更新します

169
00:09:10,440 --> 00:09:11,710
さてここからはあなたがこのモデルの
トレーニングループを実装する番です

170
00:09:11,710 --> 00:09:16,000
データセットを通じてループを行う
というのがここでの考え方です

171
00:09:16,000 --> 00:09:18,190
trainloaderからimagesとlabelsを取得し
これらのバッチのそれぞれで

172
00:09:18,190 --> 00:09:20,120
トレーニングパスを行います

173
00:09:20,120 --> 00:09:23,435
そしてネットワークの出力を
計算するこのパスを行い

174
00:09:23,434 --> 00:09:26,979
損失を計算し 損失のバックワードパスを
行ってから

175
00:09:26,980 --> 00:09:30,004
重みを更新します

176
00:09:30,004 --> 00:09:31,564
トレーニングセット全体を通じた
各パスはエポックと呼ばれ

177
00:09:31,565 --> 00:09:34,880
ここでは５つのエポックが設定されています

178
00:09:34,879 --> 00:09:36,620
増やしたり減らしたりする場合は
この数字を変更できます

179
00:09:36,620 --> 00:09:38,509
損失を計算したら

180
00:09:38,509 --> 00:09:40,610
追跡できるように累積できますので

181
00:09:40,610 --> 00:09:42,065
損失を確認します

182
00:09:42,065 --> 00:09:44,825
これがrunning_lossで

183
00:09:44,825 --> 00:09:47,555
Training lossも出力します

184
00:09:47,554 --> 00:09:49,984
ですから機能していれば

185
00:09:49,985 --> 00:09:52,529
データ全体を見たときに

