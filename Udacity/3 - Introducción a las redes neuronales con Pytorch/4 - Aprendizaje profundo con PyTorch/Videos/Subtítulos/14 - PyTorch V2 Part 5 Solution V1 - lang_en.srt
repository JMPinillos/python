1
00:00:00,000 --> 00:00:03,930
Welcome back. So, here's my solution for the validation pass.

2
00:00:03,930 --> 00:00:06,870
So, here, our model has been defined,

3
00:00:06,870 --> 00:00:10,125
our loss, and our optimizer, and all this stuff.

4
00:00:10,125 --> 00:00:16,980
I've set to 30 epochs so we can see like how this trains or how the training loss drops,

5
00:00:16,980 --> 00:00:19,800
and how the validation loss changes over time.

6
00:00:19,800 --> 00:00:23,205
The way this works is that after each pass,

7
00:00:23,204 --> 00:00:25,619
after each epoch, after each pass through the training set,

8
00:00:25,620 --> 00:00:28,155
then we're going to do a validation pass.

9
00:00:28,155 --> 00:00:29,820
That's what this else here means.

10
00:00:29,820 --> 00:00:33,119
So, basically, four of this stuff and then

11
00:00:33,119 --> 00:00:36,750
else basically says after this four loop completes,

12
00:00:36,750 --> 00:00:38,060
then run this code.

13
00:00:38,060 --> 00:00:40,150
That's what this else here means.

14
00:00:40,149 --> 00:00:45,089
As seen before, we want to turn off our gradients so with torch.no_grad,

15
00:00:45,090 --> 00:00:47,990
and then we're going to get our images and labels from a test set,

16
00:00:47,990 --> 00:00:53,725
pass it into the images into our model to get our log probabilities, calculate our loss.

17
00:00:53,725 --> 00:00:57,079
So, here, I am going to just be updating our test_loss.

18
00:00:57,079 --> 00:00:59,989
So, test_loss is just an integer that's going to count

19
00:00:59,990 --> 00:01:03,230
up our loss on our test set as we're training,

20
00:01:03,229 --> 00:01:05,119
as we're doing more of these validation passes.

21
00:01:05,120 --> 00:01:06,650
So, this way, we can actually track

22
00:01:06,650 --> 00:01:10,100
the test loss over all the epochs that we're training.

23
00:01:10,099 --> 00:01:12,079
So, from the law of probabilities,

24
00:01:12,079 --> 00:01:17,030
I can get our actual probability distributions using torch.exponential.

25
00:01:17,030 --> 00:01:23,159
Again, topk1 gives us our predicted classes and we can measure,

26
00:01:23,159 --> 00:01:25,965
we can calculate the equalities.

27
00:01:25,965 --> 00:01:30,549
So, here, we can get our probabilities from our log probabilities using torch.exp,

28
00:01:30,549 --> 00:01:34,284
taking the exponential of a log gives you back into the probabilities.

29
00:01:34,284 --> 00:01:38,114
From that, we do ps.topk, so one,

30
00:01:38,114 --> 00:01:43,109
and this gives us the top_class or predicted class from the network.

31
00:01:43,109 --> 00:01:47,519
Then, using checking for equality,

32
00:01:47,519 --> 00:01:52,534
we can see where our predicted classes match with the true classes from labels.

33
00:01:52,534 --> 00:01:55,670
Again, measure our calculator accuracy.

34
00:01:55,670 --> 00:01:59,125
So, using torch.mean and changing equals into a FloatTensor.

35
00:01:59,125 --> 00:02:02,469
So, I'm going to run this and then let it run for a while,

36
00:02:02,469 --> 00:02:05,314
and then we can see what the actual training

37
00:02:05,314 --> 00:02:08,965
and validation losses look like as we were training this network.

38
00:02:08,965 --> 00:02:10,879
Now, the network is trained, we can see how

39
00:02:10,879 --> 00:02:12,590
the validation loss and the training loss actually

40
00:02:12,590 --> 00:02:16,985
changed over time like as we continue training on more and more data.

41
00:02:16,985 --> 00:02:19,790
So, we see is the training loss drops but

42
00:02:19,789 --> 00:02:23,269
the validation loss actually starts going up over time.

43
00:02:23,270 --> 00:02:26,340
There's actually a clear sign of overfitting

44
00:02:26,340 --> 00:02:29,270
so our network is getting better and better and better on the training data,

45
00:02:29,270 --> 00:02:32,530
but it's actually starting to get worse on the validation data.

46
00:02:32,530 --> 00:02:35,330
This is because as it's learning the training data,

47
00:02:35,330 --> 00:02:38,795
it's failing to generalize the data outside of that.

48
00:02:38,794 --> 00:02:43,644
Okay. So, this is what the phenomenon of overfitting looks like.

49
00:02:43,645 --> 00:02:45,480
The way that we combine it,

50
00:02:45,479 --> 00:02:47,959
the way that we try to avoid this and prevent it is by

51
00:02:47,960 --> 00:02:51,560
using regularization and specifically, dropout.

52
00:02:51,560 --> 00:02:56,245
So, deep behind dropout is that we randomly drop input units between our layers.

53
00:02:56,245 --> 00:03:00,185
What this does is it forces the network to share information between the weights,

54
00:03:00,185 --> 00:03:03,965
and so this increases this ability to generalize to new data.

55
00:03:03,965 --> 00:03:06,349
PyTorch adding dropout is pretty straightforward.

56
00:03:06,349 --> 00:03:08,814
We just use this nn.Dropout module.

57
00:03:08,814 --> 00:03:12,125
So, we can basically create our classifier like we had before

58
00:03:12,125 --> 00:03:16,384
using the linear transformations to do our hidden layers,

59
00:03:16,384 --> 00:03:18,259
and then we just add self.dropout,

60
00:03:18,259 --> 00:03:21,685
nn.Dropout, and then you give it some drop probability.

61
00:03:21,685 --> 00:03:23,460
In this case, this is 20 percent,

62
00:03:23,460 --> 00:03:26,719
so this is the probability that you'll drop a unit.

63
00:03:26,719 --> 00:03:28,655
In the forward method, it's pretty similar.

64
00:03:28,655 --> 00:03:30,259
So, we just pass in x,

65
00:03:30,259 --> 00:03:32,134
which is our input tensor,

66
00:03:32,134 --> 00:03:33,604
we're going to make sure it's flattened,

67
00:03:33,604 --> 00:03:37,590
and then we pass this tensor through each of

68
00:03:37,590 --> 00:03:41,909
our fully connected layers into an activation,

69
00:03:41,909 --> 00:03:44,264
relu activation and then through dropout.

70
00:03:44,264 --> 00:03:48,629
Our last layer is the output layer so we're not going to use dropout here.

71
00:03:48,629 --> 00:03:50,759
There's one more thing to note about this.

72
00:03:50,759 --> 00:03:53,799
So, when we're actually doing inference,

73
00:03:53,800 --> 00:03:55,700
if we're trying to make predictions with our network,

74
00:03:55,699 --> 00:03:59,389
we want to have all of our units available, right?

75
00:03:59,389 --> 00:04:00,839
So, in this case,

76
00:04:00,840 --> 00:04:03,718
we want to turn off dropout when we're doing validation,

77
00:04:03,718 --> 00:04:05,969
testing, when we're trying to make predictions.

78
00:04:05,969 --> 00:04:09,439
So, to do that, we do model.eval.

79
00:04:09,439 --> 00:04:16,269
So, model.eval will turn off dropout and this will allow us to get the most power,

80
00:04:16,269 --> 00:04:19,714
the highest performance out of our network when we're doing inference.

81
00:04:19,714 --> 00:04:22,609
Then, to go back in the train mode, use model.train.

82
00:04:22,610 --> 00:04:25,780
So, then, the validation pass looks like this now.

83
00:04:25,779 --> 00:04:27,699
So, first, we're going to turn off our gradients.

84
00:04:27,699 --> 00:04:32,360
So, with torch.no_grad, and then we set our model to evaluation mode,

85
00:04:32,360 --> 00:04:36,720
and then we do our validation pass through the test data.

86
00:04:36,720 --> 00:04:38,070
Then, after all this,

87
00:04:38,069 --> 00:04:42,545
we want to make sure the model is set back to train mode so we do model.train.

88
00:04:42,545 --> 00:04:48,585
Okay. So, now, I'm going to leave it up to you to create your new model.

89
00:04:48,584 --> 00:04:52,049
Try adding dropout to it and then try training your model with dropout.

90
00:04:52,050 --> 00:04:58,470
Then, again, checkout the training progress of validation using dropout. Cheers.

