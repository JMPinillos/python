1
00:00:00,000 --> 00:00:04,740
皆さん PyTorchの深層学習の授業へようこそ

2
00:00:04,740 --> 00:00:07,530
ここでは PyTorchでニューラルネットワークを
構築し トレーニングする方法を説明します

3
00:00:07,530 --> 00:00:11,400
構築したノートを通じて

4
00:00:11,400 --> 00:00:14,220
自分で実際のコードを書き これらの
ネットワークを構築することになります

5
00:00:14,220 --> 00:00:18,914
授業の最後には

6
00:00:18,914 --> 00:00:21,794
最先端の自分の画像分類子を
構築することになります

7
00:00:21,795 --> 00:00:25,125
まずは 基礎から始めます

8
00:00:25,125 --> 00:00:27,059
PyTorchのシンプルなニューラルネット
ワークをどう構築するのでしょうか？

9
00:00:27,059 --> 00:00:31,604
ニューラルネットワークの機能する
方法をリマインドしますが

10
00:00:31,605 --> 00:00:34,905
一般的には 入力値があります

11
00:00:34,905 --> 00:00:38,469
ここには x₁ x₂ があり いくつかの
重みwと バイアスで掛けます

12
00:00:38,469 --> 00:00:43,460
このbはバイアスで １と掛けて すべてを合計し

13
00:00:43,460 --> 00:00:47,645
値hを得ます　

14
00:00:47,645 --> 00:00:52,220
次に活性化関数と 呼ばれるものを利用します

15
00:00:52,219 --> 00:00:54,875
f(h)がここにあり

16
00:00:54,875 --> 00:00:59,829
この活性化関数を通じて
入力値hを受け渡すと 出力yが得られます

17
00:00:59,829 --> 00:01:02,929
これがニューラルネットワークの基礎です

18
00:01:02,929 --> 00:01:04,069
これらの入力があり

19
00:01:04,069 --> 00:01:06,125
いくつかの方法で掛けて合計を計算し

20
00:01:06,125 --> 00:01:10,435
いくつかの活性化関数を通じて
受け渡すと出力が得られます

21
00:01:10,435 --> 00:01:13,879
これらを積み重ねて これらのニューロンの
これらのユニットの出力が

22
00:01:13,879 --> 00:01:18,250
別の重みのセットのように別の層に
移動するようにできます

23
00:01:18,250 --> 00:01:20,834
数学的には yのようになり

24
00:01:20,834 --> 00:01:26,959
出力は 重みと入力値w xの線形結合に
バイアスbを加えたものに等しく

25
00:01:26,959 --> 00:01:30,319
活性化関数fを通じて yが得られます

26
00:01:30,319 --> 00:01:34,069
この式でも表すことができます

27
00:01:34,069 --> 00:01:36,109
w₁x₁+bです

28
00:01:36,109 --> 00:01:40,510
これで yが得られます

29
00:01:40,510 --> 00:01:42,344
これの良いところは x 入力機能値をベクトルとして

30
00:01:42,344 --> 00:01:46,640
重みを別のベクトルとして

31
00:01:46,640 --> 00:01:49,519
実際に考えられることです

32
00:01:49,519 --> 00:01:52,685
２つのベクトルの点 または内部の積は
掛け算と足し算で表します

33
00:01:52,685 --> 00:01:58,799
入力と重みを ベクトルと見なした場合

34
00:01:58,799 --> 00:02:01,864
これらの２つを点の積として利用すると

35
00:02:01,864 --> 00:02:03,859
値hが得られます　

36
00:02:03,859 --> 00:02:06,784
次に活性化関数を通じて
hを受け渡し 出力yを得ます

37
00:02:06,784 --> 00:02:09,859
重みと入力をベクトルとして考え始めた場合

38
00:02:09,860 --> 00:02:16,430
ベクトルはテンソルのインスタンスです

39
00:02:16,430 --> 00:02:20,330
テンソルは ベクトルとマトリックスを
全般化したものにすぎません

40
00:02:20,330 --> 00:02:24,350
このような値の規則構造の配置の場合

41
00:02:24,349 --> 00:02:27,949
１つの次元のみのテンソルはベクトルになります

42
00:02:27,949 --> 00:02:32,284
値について 単一の１次元の配列しかありません

43
00:02:32,284 --> 00:02:37,400
この場合 t-e-n-s-o-rという文字のマトリックスは

44
00:02:37,400 --> 00:02:42,664
２次元のテンソルで 左から右ならびに

45
00:02:42,664 --> 00:02:46,370
上から下の２つの方向の値があり

46
00:02:46,370 --> 00:02:48,830
個別の行と列を確保できます

47
00:02:48,830 --> 00:02:51,620
行に沿ったように 列全体での操作や

48
00:02:51,620 --> 00:02:54,500
列を下がるように 行全体での操作が可能です

49
00:02:54,500 --> 00:02:58,490
３次元のテンソルを用意し

50
00:02:58,490 --> 00:03:01,745
３次元テンソルとしての RGBカラーイメージ
のような画像を考えることもできます

51
00:03:01,745 --> 00:03:05,990
それぞれのピクセルに

52
00:03:05,990 --> 00:03:07,370
赤 緑 青の全チャネルのなんらかの値があり

53
00:03:07,370 --> 00:03:10,490
個別のピクセルに対してもそうです

54
00:03:10,490 --> 00:03:13,765
２次元の画像では

55
00:03:13,764 --> 00:03:15,629
３つの値があります

56
00:03:15,629 --> 00:03:17,069
これが ３次元のテンソルです

57
00:03:17,069 --> 00:03:19,155
前にも説明しましたが
テンソルは これを全般化したもので

58
00:03:19,155 --> 00:03:21,800
実はテンソルのように

59
00:03:21,800 --> 00:03:24,379
4, ５, ６次元…というようにできます

60
00:03:24,378 --> 00:03:27,709
通常使用するのは
１, ２, ３次元のテンソルです

61
00:03:27,710 --> 00:03:29,930
テンソルは PyTorchや他のニューラルネット
ワークフレームワークを使用する基本的なデータ構造です

62
00:03:29,930 --> 00:03:32,800
そのため TensorFlowはテンソルに
因んで命名されています

63
00:03:32,800 --> 00:03:36,695
これらが使用する 基本的なデータ構造のため

64
00:03:36,694 --> 00:03:40,715
よく理解する必要があります

65
00:03:40,715 --> 00:03:43,759
これにより深層学習に利用する
フレームワークの多くの部分で利用できます

66
00:03:43,759 --> 00:03:46,579
それでは テンソルを実際に作成し

67
00:03:46,580 --> 00:03:49,520
シンプルなニューラルネットワークを
構築する方法を説明します

68
00:03:49,520 --> 00:03:52,310
まず PyTorchをインポートします

69
00:03:52,310 --> 00:03:55,865
torchをここにインポートします

70
00:03:55,865 --> 00:03:59,689
ここで 活性化関数を作成しています

71
00:03:59,689 --> 00:04:03,604
これがシグモイド活性化関数です

72
00:04:03,604 --> 00:04:07,564
これは０～１の入力値を
絞り込む S字型の形状です

73
00:04:07,564 --> 00:04:09,770
これは確率を提供する上で役立ちます

74
00:04:09,770 --> 00:04:12,355
確率はこれらの値で ０～１の値になります

75
00:04:12,354 --> 00:04:17,879
ニューラルネットワークの出力を
確率にする場合は

76
00:04:17,879 --> 00:04:21,064
シグモイド活性化を利用します

77
00:04:21,064 --> 00:04:25,194
いくつかのフェイクデータとデータを作成し
重みとバイアスを生成します

78
00:04:25,194 --> 00:04:27,769
シンプルなニューラルネットワークの出力を
得るために 実際に計算を行います

79
00:04:27,769 --> 00:04:30,560
ここではmanual_seedを作成しています

80
00:04:30,560 --> 00:04:33,060
使用される乱数の生成のための
シードを設定します

81
00:04:33,060 --> 00:04:38,120
ここでは機能を作成します

82
00:04:38,120 --> 00:04:41,240
機能はネットワークの入力データ
の入力機能のようです

83
00:04:41,240 --> 00:04:44,814
torch.randnがあります

84
00:04:44,814 --> 00:04:47,519
randnは 正規変数のテンソルを作成します

85
00:04:47,519 --> 00:04:50,299
したがって正規確率変数は正規分布の
サンプルとして使用されます

86
00:04:50,300 --> 00:04:53,254
希望のサイズのタプルを提供します

87
00:04:53,254 --> 00:04:57,605
この場合は 機能をマトリックスにします

88
00:04:57,605 --> 00:05:00,150
１行と５列の２次元のテンソルです

89
00:05:00,149 --> 00:05:04,909
これを５つの要素のある行のベクトルと
考えることができます

90
00:05:04,910 --> 00:05:08,180
重みには 正規確率変数の別の
マトリックスを作成します

91
00:05:08,180 --> 00:05:11,250
今回は randn_likeを使用します

92
00:05:11,250 --> 00:05:15,605
別のテンソルを使用することで

93
00:05:15,605 --> 00:05:20,675
このテンソルの形を
確認してから作成します

94
00:05:20,675 --> 00:05:25,800
同じ形で別のテンソルを作成します

95
00:05:25,800 --> 00:05:28,610
それが この意味になります

96
00:05:28,610 --> 00:05:33,270
正規確率変数のテンソルを
機能と同じ形で作成します

97
00:05:33,269 --> 00:05:35,990
これで重みが得られます

98
00:05:35,990 --> 00:05:39,259
次にバイアスの項を作成します

99
00:05:39,259 --> 00:05:41,329
これは 正規確率変数にすぎません

100
00:05:41,329 --> 00:05:42,769
１つの値を作成しているだけです

101
00:05:42,769 --> 00:05:44,930
したがって これは１行１列です

102
00:05:44,930 --> 00:05:50,115
練習には自由に取り組んでください

103
00:05:50,115 --> 00:05:53,115
これから 機能 重み バイアス
のテンソルを利用し

104
00:05:53,115 --> 00:05:56,240
このシンプルなニューラル
ネットワークの出力を計算します

105
00:05:56,240 --> 00:05:58,699
機能や重みで内積を利用するか

106
00:05:58,699 --> 00:06:00,714
機能に重みをかけて 和を出してから

107
00:06:00,714 --> 00:06:03,349
バイアスを足して 活性化関数を
通じて受け渡すと

108
00:06:03,350 --> 00:06:06,050
ネットワークの出力が得られるはずです

109
00:06:06,050 --> 00:06:08,449
どう行ったか確認する場合は
回答ノートを確認するか

110
00:06:08,449 --> 00:06:11,029
回答を説明する次のビデオを確認してください

