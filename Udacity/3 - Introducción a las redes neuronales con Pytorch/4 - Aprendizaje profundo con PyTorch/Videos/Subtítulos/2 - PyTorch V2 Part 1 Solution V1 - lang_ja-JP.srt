1
00:00:00,000 --> 00:00:03,540
それでは ニューラルネットワークの

2
00:00:03,540 --> 00:00:08,190
出力計算演習の解答例を見ていきましょう

3
00:00:08,189 --> 00:00:12,329
やりたいことは featuresにweightsをかけることで

4
00:00:12,330 --> 00:00:14,070
(features * weights)です

5
00:00:14,070 --> 00:00:18,225
これらのテンソルは 基本的にNumPy配列と

6
00:00:18,225 --> 00:00:19,845
同様に動作します

7
00:00:19,844 --> 00:00:22,769
(features * weights)によって

8
00:00:22,769 --> 00:00:25,320
それぞれの最初の要素を取得 乗算し

9
00:00:25,320 --> 00:00:27,420
２番目以降も同様に行い

10
00:00:27,420 --> 00:00:29,895
要素ごとの乗算を備えた

11
00:00:29,894 --> 00:00:33,060
新しいテンソルが返されます

12
00:00:33,060 --> 00:00:37,020
torch.sumで合計し

13
00:00:37,020 --> 00:00:41,100
バイアス項を加えて

14
00:00:41,100 --> 00:00:44,719
活性化関数に渡すと Yが得られます

15
00:00:44,719 --> 00:00:46,295
(features * weights)の
再実行時にも これが使え

16
00:00:46,295 --> 00:00:48,710
別のテンソルが作られますが

17
00:00:48,710 --> 00:00:53,645
テンセルには .sumがあり

18
00:00:53,645 --> 00:00:56,550
テンソルのすべての値が合計されます

19
00:00:56,549 --> 00:00:58,604
この方法や torch.sum

20
00:00:58,604 --> 00:01:01,429
または sumメソッドと
上限値を用いることもできます

21
00:01:01,429 --> 00:01:04,644
それを活性化関数に渡します

22
00:01:04,644 --> 00:01:06,439
ここでは この要素ごとの乗算を行い

23
00:01:06,439 --> 00:01:11,079
別々の操作で
合計を取得しています

24
00:01:11,079 --> 00:01:13,179
乗算してから 合計していますが

25
00:01:13,180 --> 00:01:17,420
行列乗算を用いれば １回でこれを行えます

26
00:01:17,420 --> 00:01:19,519
一般的に 行列乗算が使用されます

27
00:01:19,519 --> 00:01:22,069
より効率的であり

28
00:01:22,069 --> 00:01:24,364
線形代数演算は GPUで実行される
CUDA等の最新ライブラリにより

29
00:01:24,364 --> 00:01:30,144
高速化されているためです

30
00:01:30,144 --> 00:01:32,984
２つのテンソルで
PyTorchでの行列乗算を行うには

31
00:01:32,984 --> 00:01:37,549
２つの方法があります

32
00:01:37,549 --> 00:01:40,310
torch.mm または torch.matmulです

33
00:01:40,310 --> 00:01:44,875
torch.mmでは 行列乗算はより単純で

34
00:01:44,875 --> 00:01:48,590
渡すテンソルについては
より厳密になります

35
00:01:48,590 --> 00:01:52,760
torch.matmulは ブロードキャストを
サポートしており

36
00:01:52,760 --> 00:01:56,420
奇妙なサイズ 形状のテンソルを配置すると

37
00:01:56,420 --> 00:02:02,028
予期せぬ出力が 返ってくる場合があります

38
00:02:02,028 --> 00:02:05,664
私は torch.mmをよく使いますが

39
00:02:05,665 --> 00:02:09,495
基本的に 予測通りに動作します

40
00:02:09,495 --> 00:02:12,520
問題が発生した場合

41
00:02:12,520 --> 00:02:15,710
計算は続行されず エラーが返されます

42
00:02:15,710 --> 00:02:19,085
featuresと weightsで
torch.mmを使うと

43
00:02:19,085 --> 00:02:21,665
エラーが発生します

44
00:02:21,664 --> 00:02:25,174
RuntimeError
サイズの不一致となっています

45
00:02:25,175 --> 00:02:28,975
２つのテンソルのサイズが 不一致で

46
00:02:28,974 --> 00:02:33,484
行列乗算ができないという意味で

47
00:02:33,485 --> 00:02:35,810
ここにサイズが表示されます

48
00:02:35,810 --> 00:02:39,814
最初のテンソルM1は 1x5で

49
00:02:39,814 --> 00:02:42,155
２番目のテンソルも 1x5です

50
00:02:42,155 --> 00:02:45,905
線形代数の学習を覚えていますか？

51
00:02:45,905 --> 00:02:51,360
行列乗算を行うとき

52
00:02:51,360 --> 00:02:53,425
最初の行列には

53
00:02:53,425 --> 00:02:56,689
２番目の行列の行数と 同じ数の列が必要です

54
00:02:56,689 --> 00:03:00,520
実際に必要なのは weightテンソルです

55
00:03:00,520 --> 00:03:04,245
weight行列は 1x5ではなく5x1にします

56
00:03:04,245 --> 00:03:07,765
ネットワークを構築時に

57
00:03:07,764 --> 00:03:11,338
テンソルの形状をチェックするには

58
00:03:11,338 --> 00:03:13,365
tensor.shapeを使います

59
00:03:13,365 --> 00:03:15,210
これはPyTorchだけでなく

60
00:03:15,210 --> 00:03:18,500
TensorFlowなどでも使われます

61
00:03:18,500 --> 00:03:22,889
ネットワーク構築時のエラーの多くや

62
00:03:22,889 --> 00:03:25,759
ニューラルネットワークの構造設計時の困難は

63
00:03:25,759 --> 00:03:28,909
テンソルの形状を 正しく
合わせることに関係しています

64
00:03:28,909 --> 00:03:32,240
つまり 多くのデバッグにおいて

65
00:03:32,240 --> 00:03:35,780
テンソルの形を 確認することになるでしょう

66
00:03:35,780 --> 00:03:38,014
ですから tensor.shapeをお忘れなく

67
00:03:38,014 --> 00:03:39,980
テンソルの形状変更には

68
00:03:39,979 --> 00:03:42,634
３つの選択肢があります

69
00:03:42,634 --> 00:03:45,169
reshape(形状変更)

70
00:03:45,169 --> 00:03:47,689
resize(サイズ変更)

71
00:03:47,689 --> 00:03:49,789
そしてview(表示)です

72
00:03:49,789 --> 00:03:51,859
すべてが機能する方法は

73
00:03:51,860 --> 00:03:53,045
テンソルのweights.reshapeを行ってから

74
00:03:53,044 --> 00:03:56,074
必要とする新しい形状を渡すことです

75
00:03:56,074 --> 00:03:58,204
ここでは weightを 5x1の行列に変更するため

76
00:03:58,205 --> 00:04:01,105
.reshape(5,1)と記述します

77
00:04:01,104 --> 00:04:04,804
これで形状が変更され

78
00:04:04,805 --> 00:04:05,930
weightと同じデータ

79
00:04:05,930 --> 00:04:08,900
つまり メモリ内にあるのと同じ
データを持つテンソルが返されます

80
00:04:08,900 --> 00:04:11,629
要求した形状の

81
00:04:11,629 --> 00:04:15,319
新しいテンソルを作成するだけですが

82
00:04:15,319 --> 00:04:18,750
メモリ内の実際のデータは 変更されません

83
00:04:18,750 --> 00:04:22,595
しかし クローンが返されることがあります

84
00:04:22,595 --> 00:04:24,275
データをメモリの別部分に複製してから

85
00:04:24,274 --> 00:04:27,774
メモリ部分上に テンソルを返すのです

86
00:04:27,774 --> 00:04:31,774
データを複製する場合

87
00:04:31,774 --> 00:04:33,739
複製せずにテンソルの形状を
変更した場合よりも効率が低下します

88
00:04:33,740 --> 00:04:38,569
こうしたことのために

89
00:04:38,569 --> 00:04:41,089
「_」付きのresizeが使用できます

90
00:04:41,089 --> 00:04:44,214
_は メソッドが インプレース操作
であることを意味します

91
00:04:44,214 --> 00:04:47,049
つまり データには全く触れず

92
00:04:47,050 --> 00:04:50,079
メモリ内のアドレス指定された
データ上のテンソルを変更するだけです

93
00:04:50,079 --> 00:04:54,079
resizeメソッドの問題は

94
00:04:54,079 --> 00:04:55,259
元のテンソルと 異なる形状を要求した場合

95
00:04:55,259 --> 00:04:58,050
データの一部が 失われたり

96
00:04:58,050 --> 00:05:02,884
初期化されていないメモリから得た
誤ったデータが作成されたりすることです

97
00:05:02,884 --> 00:05:04,235
この場合 必要なのは

98
00:05:04,235 --> 00:05:06,220
形状を元の要素数から 変更した場合に

99
00:05:06,220 --> 00:05:08,140
エラーを返すメソッドです

100
00:05:08,139 --> 00:05:12,574
それが.viewです

101
00:05:12,574 --> 00:05:16,435
.viewは 私もよく使いますがもつ

102
00:05:16,435 --> 00:05:20,560
メモリ内のweightと同じデータを

103
00:05:20,560 --> 00:05:23,470
新しいテンソルを返します

104
00:05:23,470 --> 00:05:26,305
メモリ内のデータには干渉せず

105
00:05:26,305 --> 00:05:30,004
新しいテンソルを返すだけです

106
00:05:30,004 --> 00:05:32,509
テンソルの要素の数が異なる

107
00:05:32,509 --> 00:05:35,779
新しいサイズ 形状を取得しよう
とするとエラーが返されます

108
00:05:35,779 --> 00:05:38,029
.viewを使用していれば

109
00:05:38,029 --> 00:05:41,554
weightの形状を変更しても

110
00:05:41,555 --> 00:05:43,644
同数の要素を取得できます

111
00:05:43,644 --> 00:05:46,870
テンソルのreshape時に使用するのは このためです

112
00:05:46,870 --> 00:05:49,535
weightを５行１列に

113
00:05:49,535 --> 00:05:52,985
reshapeする場合は

114
00:05:52,985 --> 00:05:55,345
weights.view(5,1)のように記述できます

115
00:05:55,345 --> 00:05:57,620
テンソルのreshapeと行列乗算を

116
00:05:57,620 --> 00:06:00,155
学びましたので これを利用して

117
00:06:00,154 --> 00:06:03,754
このニューラルネットワークの
出力を計算してみましょう

