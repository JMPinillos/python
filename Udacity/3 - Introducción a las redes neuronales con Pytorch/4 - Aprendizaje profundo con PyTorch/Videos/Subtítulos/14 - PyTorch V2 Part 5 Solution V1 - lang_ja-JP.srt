1
00:00:00,000 --> 00:00:03,930
では始めます

2
00:00:03,930 --> 00:00:06,870
validation pass用の私の解がこれです

3
00:00:06,870 --> 00:00:10,125
ここにモデルが定義してあり

4
00:00:10,125 --> 00:00:16,980
損失 オプティマイザ そして
その他のものがあります

5
00:00:16,980 --> 00:00:19,800
エポックを30に設定したので

6
00:00:19,800 --> 00:00:23,205
トレーニングの方法や training lossの減少

7
00:00:23,204 --> 00:00:25,619
validation lossが 時間の経過と共に
どう変化するかを確認できます

8
00:00:25,620 --> 00:00:28,155
これがどう機能するかと言うと

9
00:00:28,155 --> 00:00:29,820
各パス 各エポック トレーニングセットによる
各パスの後でvalidation passを実行します

10
00:00:29,820 --> 00:00:33,119
これが elseの意味です

11
00:00:33,119 --> 00:00:36,750
ですから この４つがあって

12
00:00:36,750 --> 00:00:38,060
これらのループが完了してから

13
00:00:38,060 --> 00:00:40,150
elseのこのコードを実行します

14
00:00:40,149 --> 00:00:45,089
これがこのelseの意味です

15
00:00:45,090 --> 00:00:47,990
前と同様に 勾配をオフにしたいので
with torch.no_gradとします

16
00:00:47,990 --> 00:00:53,725
次にテストセットから 画像とラベルを取得して

17
00:00:53,725 --> 00:00:57,079
それをモデルの画像に渡して 対数確率を
取得し 損失を計算します

18
00:00:57,079 --> 00:00:59,989
ここで test_lossのアップデートをします

19
00:00:59,990 --> 00:01:03,230
test_lossは トレーニング中に

20
00:01:03,229 --> 00:01:05,119
これらの検証パスを更に進める中で
テストセットに生じる損失を数える整数です

21
00:01:05,120 --> 00:01:06,650
このように トレーニングしている
すべてのエポックについて

22
00:01:06,650 --> 00:01:10,100
test lossを追跡することができます

23
00:01:10,099 --> 00:01:12,079
確率の法則から

24
00:01:12,079 --> 00:01:17,030
torch.exponentialを使用すれば
実際の確率分布を得ることができます

25
00:01:17,030 --> 00:01:23,159
topk1から予測されるクラスが 得られますから

26
00:01:23,159 --> 00:01:25,965
等式を計算することができます

27
00:01:25,965 --> 00:01:30,549
torch.expを使用して
対数確率から確率を取得できます

28
00:01:30,549 --> 00:01:34,284
対数を指数関数にすると 確率が得られます

29
00:01:34,284 --> 00:01:38,114
そこから ps.topkを実行して１となり

30
00:01:38,114 --> 00:01:43,109
これが top_classまたはネットワークからの
予測されたクラスとなります

31
00:01:43,109 --> 00:01:47,519
次に 等式を確認して

32
00:01:47,519 --> 00:01:52,534
予測されたクラスが どこでラベルの
真のクラスと一致するかが 確認できます

33
00:01:52,534 --> 00:01:55,670
ここでもまた 計算の精度を測定します

34
00:01:55,670 --> 00:01:59,125
torch.meanを使用し
等式をFloatTensorに変更します

35
00:01:59,125 --> 00:02:02,469
これを実行して しばらく実行させておいて

36
00:02:02,469 --> 00:02:05,314
このネットワークのトレーニング中の

37
00:02:05,314 --> 00:02:08,965
実際のトレーニングとvalidation lossが
どんなものかを確認できます

38
00:02:08,965 --> 00:02:10,879
ネットワークがトレーニングされ

39
00:02:10,879 --> 00:02:12,590
より多くのデータでトレーニングを
継続する中で validation lossと

40
00:02:12,590 --> 00:02:16,985
training lossが時間経過と共に
どのように変化したかが分かります

41
00:02:16,985 --> 00:02:19,790
training lossは減少していますが

42
00:02:19,789 --> 00:02:23,269
validation lossは時間とともに
増加し始めています

43
00:02:23,270 --> 00:02:26,340
実際には 明らかな過剰適合の
兆候がありますから

44
00:02:26,340 --> 00:02:29,270
ネットワークはトレーニングデータでは
どんどん良くなっていますが

45
00:02:29,270 --> 00:02:32,530
検証データでは悪化が始まっています

46
00:02:32,530 --> 00:02:35,330
これは トレーニングデータを
学習するにつれて

47
00:02:35,330 --> 00:02:38,795
それ以外のデータを一般化することが
できないためです

48
00:02:38,794 --> 00:02:43,644
これが過学習の現象です

49
00:02:43,645 --> 00:02:45,480
これを結合させる方法

50
00:02:45,479 --> 00:02:47,959
これを回避し 防止する方法は

51
00:02:47,960 --> 00:02:51,560
正則化 具体的には
ドロップアウトを使用するものです

52
00:02:51,560 --> 00:02:56,245
ドロップアウトの奥深くでは
層間の入力が ランダムでドロップされます

53
00:02:56,245 --> 00:03:00,185
これにより ネットワークはウェイト間の
情報を共有することになり

54
00:03:00,185 --> 00:03:03,965
新しいデータを一般化する
能力が向上します

55
00:03:03,965 --> 00:03:06,349
PyTorchのドロップアウトの追加は
非常に簡単です

56
00:03:06,349 --> 00:03:08,814
nn.Dropoutモジュールを使用するだけです

57
00:03:08,814 --> 00:03:12,125
基本的に 前のような分類子を

58
00:03:12,125 --> 00:03:16,384
線形変換で前のような分類子を作成して
隠れ層を処理し

59
00:03:16,384 --> 00:03:18,259
self.dropout nn.Dropoutを加えて

60
00:03:18,259 --> 00:03:21,685
そこに何らかのドロップ確率を与えます

61
00:03:21,685 --> 00:03:23,460
ここでは20パーセントです

62
00:03:23,460 --> 00:03:26,719
これがユニットをドロップする
確立になります

63
00:03:26,719 --> 00:03:28,655
forwardメソッドでも同じように行います

64
00:03:28,655 --> 00:03:30,259
入力テンソルである

65
00:03:30,259 --> 00:03:32,134
xに渡して

66
00:03:32,134 --> 00:03:33,604
flattenedであることを確認します

67
00:03:33,604 --> 00:03:37,590
このテンソルは 全結合した

68
00:03:37,590 --> 00:03:41,909
ReLu活性化への各層を介して

69
00:03:41,909 --> 00:03:44,264
そして ドロップアウトを介して渡されます

70
00:03:44,264 --> 00:03:48,629
最後は 出力層ですので
ここでは ドロップアウトは使用しません

71
00:03:48,629 --> 00:03:50,759
もう一つ注意する点があります

72
00:03:50,759 --> 00:03:53,799
実際に推論を行う場合

73
00:03:53,800 --> 00:03:55,700
ネットワークで予測しようとするときは

74
00:03:55,699 --> 00:03:59,389
すべてのユニットを
利用可能にしたいですよね？

75
00:03:59,389 --> 00:04:00,839
このような場合

76
00:04:00,840 --> 00:04:03,718
予測をしようとするときは 検証
テストの際にドロップアウトをオフにしておきます

77
00:04:03,718 --> 00:04:05,969
これには  model.eval.を実行します

78
00:04:05,969 --> 00:04:09,439
model.evalが ドロップアウトをオフにし

79
00:04:09,439 --> 00:04:16,269
これにより推論の際に ネットワークの最大の能力と
最高のパフォーマンスが得られます

80
00:04:16,269 --> 00:04:19,714
次に トレーニングモードに戻るには
model.trainを使用します

81
00:04:19,714 --> 00:04:22,609
検証パスは このようになっています

82
00:04:22,610 --> 00:04:25,780
まず 勾配をオフにします

83
00:04:25,779 --> 00:04:27,699
with torch.no_gradとして

84
00:04:27,699 --> 00:04:32,360
モデルを評価モードに設定し

85
00:04:32,360 --> 00:04:36,720
テストデータを介して 検証パスを実行します

86
00:04:36,720 --> 00:04:38,070
これらの後に

87
00:04:38,069 --> 00:04:42,545
モデルをトレーニングモードに戻すため
model.trainを実行します

88
00:04:42,545 --> 00:04:48,585
それでは それぞれ自分の
新しいモデルを作成してみましょう

89
00:04:48,584 --> 00:04:52,049
ドロップアウトの追加や ドロップアウトなしでの
トレーニングに挑戦してください

90
00:04:52,050 --> 00:04:58,470
その後 再度ドロップアウトを使用して
検証のトレーニングの進捗を確認してください

