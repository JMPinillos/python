1
00:00:00,000 --> 00:00:04,530
こんにちは

2
00:00:04,530 --> 00:00:08,879
PyTorchモデルのセーブと
ロードについて説明します

3
00:00:08,880 --> 00:00:10,170
モデルのトレーニングや
予測について見てきましたが

4
00:00:10,169 --> 00:00:13,469
トレーニング後に 予測を行ったり

5
00:00:13,470 --> 00:00:16,935
新しいデータで
やり直したりすることもあるでしょう

6
00:00:16,934 --> 00:00:19,649
ここではtrainモデルのセーブと

7
00:00:19,649 --> 00:00:21,779
ロードについて学びます

8
00:00:21,780 --> 00:00:23,925
まず モジュールのロードです

9
00:00:23,925 --> 00:00:25,903
既に見たtorchや torchivisionとほぼ同様です

10
00:00:25,903 --> 00:00:27,795
新しいのは このfc_modelです

11
00:00:27,795 --> 00:00:29,910
これは 完全に接続された分類機能を

12
00:00:29,910 --> 00:00:33,659
構築するためのモデルを
実装するモジュールです

13
00:00:33,659 --> 00:00:38,099
簡単なものですが

14
00:00:38,100 --> 00:00:39,414
このノートと動画のために作りました

15
00:00:39,414 --> 00:00:43,509
次に データセットのロードです

16
00:00:43,509 --> 00:00:46,039
ここでもFashionMNISTのトレーニングと
テストデータセットを使用しています

17
00:00:46,039 --> 00:00:51,049
これらの画像は

18
00:00:51,049 --> 00:00:53,809
28x28のグレースケールです

19
00:00:53,810 --> 00:00:56,240
これは財布のように見えます

20
00:00:56,240 --> 00:00:58,015
モジュールとデータがロードされ

21
00:00:58,015 --> 00:01:00,679
モデルのトレーニングを行います

22
00:01:00,679 --> 00:01:02,375
これは完全に接続されたネットワークです

23
00:01:02,375 --> 00:01:05,629
前述の通り 784の入力ユニットと

24
00:01:05,629 --> 00:01:07,159
10の出力ユニットがあります

25
00:01:07,159 --> 00:01:10,489
これは 隠れ層のサイズを含むリストです

26
00:01:10,489 --> 00:01:14,329
３つの隠れ層があり

27
00:01:14,329 --> 00:01:16,219
第１層は512ユニット

28
00:01:16,219 --> 00:01:17,959
第２層は256 第３層は128ユニットです

29
00:01:17,959 --> 00:01:21,604
その後 出力層に移動します

30
00:01:21,605 --> 00:01:23,855
ここでの出力層は logsoftmaxです

31
00:01:23,855 --> 00:01:26,285
つまり 損失の基準には

32
00:01:26,284 --> 00:01:28,609
負の対数尤度損失を使用します

33
00:01:28,609 --> 00:01:31,435
最後に Adam Optimizerでネットワークを
トレーニングし パラメータを更新します

34
00:01:31,435 --> 00:01:37,204
このモデルでは fc_moduleで

35
00:01:37,204 --> 00:01:40,149
trainメソッドを呼び
トレーニングを行います

36
00:01:40,150 --> 00:01:43,530
約２週間のトレーニング後

37
00:01:43,530 --> 00:01:45,989
約84%の精度を得られました

38
00:01:45,989 --> 00:01:48,844
トレーニングの続行で

39
00:01:48,844 --> 00:01:50,420
さらに上昇させられます

40
00:01:50,420 --> 00:01:52,489
しかし これは単なるデモです

41
00:01:52,489 --> 00:01:54,379
ネットワークのトレーニングが完了したので

42
00:01:54,379 --> 00:01:57,214
ファイルにセーブします

43
00:01:57,215 --> 00:01:59,075
後でロードして予測を行ったり

44
00:01:59,075 --> 00:02:01,939
トレーニングを続けたりするためです

45
00:02:01,939 --> 00:02:05,420
セーブとは モデルの全パラメータを含む

46
00:02:05,420 --> 00:02:08,909
state_dictを保存することです

47
00:02:08,909 --> 00:02:13,370
つまり weightと biasテンソル全てです

48
00:02:13,370 --> 00:02:16,509
これはモデルの出力です

49
00:02:16,509 --> 00:02:18,634
線形層の ３つの隠れ層が見られます

50
00:02:18,634 --> 00:02:22,715
出力用の線形層もあります

51
00:02:22,715 --> 00:02:25,120
これらは パラメータとしてweightと
biasのテンセルを持っています

52
00:02:25,120 --> 00:02:30,750
state_dictを見ると

53
00:02:30,750 --> 00:02:35,900
hidden_​​layers.0があり

54
00:02:35,900 --> 00:02:37,474
そのweightとbias

55
00:02:37,474 --> 00:02:38,704
そして第２層の

56
00:02:38,705 --> 00:02:41,420
weightとbias…という風に

57
00:02:41,419 --> 00:02:43,429
なっているのがわかります

58
00:02:43,430 --> 00:02:49,319
つまりここには weightとbiasが
すべて含まれています

59
00:02:49,319 --> 00:02:53,359
それらをセーブし 再びロードできます

60
00:02:53,360 --> 00:02:57,665
torch.saveでstate_dictを
保存するのが簡単です

61
00:02:57,664 --> 00:03:00,034
torch.saveを実行し

62
00:03:00,034 --> 00:03:05,319
モデルのstate_dictを渡して
ファイルに名前を付けます

63
00:03:05,319 --> 00:03:09,044
checkpoint.pthは
チェックポイントファイルです

64
00:03:09,044 --> 00:03:14,569
pthは PyTorchチェックポイントの
一般的な拡張子であり

65
00:03:14,569 --> 00:03:16,854
state_dictにリロードできます

66
00:03:16,854 --> 00:03:20,599
pthファイルからロードして キーを出力すると

67
00:03:20,599 --> 00:03:23,914
biasや隠れ層があることがわかります

68
00:03:23,914 --> 00:03:25,834
state_dictをロードしたら

69
00:03:25,835 --> 00:03:27,965
それをモデル自体にロードできます

70
00:03:27,965 --> 00:03:30,365
ここにはstate_dict自体を
ロードしてあります

71
00:03:30,365 --> 00:03:32,030
モデルにはまだロードしていません

72
00:03:32,030 --> 00:03:33,634
しかし今 load_state_dictでstate_dictを渡し

73
00:03:33,634 --> 00:03:35,405
モデル自体にロードすることができます

74
00:03:35,405 --> 00:03:39,000
これで モデルで予測等を行う準備ができました

75
00:03:39,000 --> 00:03:44,555
簡単に見えますが

76
00:03:44,555 --> 00:03:46,240
実際は もっと複雑です

77
00:03:46,240 --> 00:03:48,200
ロードしたstate_dictを

78
00:03:48,199 --> 00:03:50,989
別のアーキテクチャのモデルにロードすると

79
00:03:50,990 --> 00:03:54,469
エラーになります

80
00:03:54,469 --> 00:03:55,969
エラー内容を見ると

81
00:03:55,969 --> 00:03:57,604
このweightにサイズ不一致があります

82
00:03:57,604 --> 00:04:00,664
コピーしたい第１隠れ層のweightは

83
00:04:00,664 --> 00:04:05,329
512x784であり
現在のモデルは400x784です

84
00:04:05,330 --> 00:04:10,100
state_dictをモデルにロードする場合

85
00:04:10,099 --> 00:04:13,444
パラメータは 同形状である必要があります

86
00:04:13,444 --> 00:04:17,420
つまり トレーニング時と同一の

87
00:04:17,420 --> 00:04:20,330
モデルを再構築する必要があります

88
00:04:20,329 --> 00:04:24,009
従って pthには

89
00:04:24,009 --> 00:04:26,959
モデルのアーキテクチャ情報を含めます

90
00:04:26,959 --> 00:04:30,680
このpthは 単なるディクショナリです

91
00:04:30,680 --> 00:04:34,084
ここにアーキテクトを定義します

92
00:04:34,084 --> 00:04:35,899
入力サイズは784 出力は10です

93
00:04:35,899 --> 00:04:39,514
これが隠れ層のリストです

94
00:04:39,514 --> 00:04:42,709
各層を経て 出力層へ到達し

95
00:04:42,709 --> 00:04:46,000
層のサイズを得ます

96
00:04:46,000 --> 00:04:47,300
最後に state_dictのキーがあります

97
00:04:47,300 --> 00:04:50,750
このディクショナリに
モデルstate_dictを含められます

98
00:04:50,750 --> 00:04:54,894
ここで全体をpthに保存する必要があります

99
00:04:54,894 --> 00:04:58,519
pth自体が モデルアーキテクチャの情報を持っており

100
00:04:58,519 --> 00:05:00,529
チェックポイントロード用関数作成が
可能になります

101
00:05:00,529 --> 00:05:02,719
ファイルパスの指定でロードできます

102
00:05:02,720 --> 00:05:06,410
ディクショナリの位置は 記憶されています

103
00:05:06,410 --> 00:05:09,260
入力 出力サイズ

104
00:05:09,259 --> 00:05:12,529
隠れ層 state_dictがあります

105
00:05:12,529 --> 00:05:13,939
このチェックポイントを使用することで

106
00:05:13,939 --> 00:05:16,040
モデルをfc_model.Networkとして
再作成します

107
00:05:16,040 --> 00:05:18,694
チェックポイントからの

108
00:05:18,694 --> 00:05:21,680
パラメータを指定します

109
00:05:21,680 --> 00:05:23,704
これでモデルが作成されます

110
00:05:23,704 --> 00:05:26,000
トレーニングしたものと

111
00:05:26,000 --> 00:05:27,535
同じアーキテクチャで作成されます

112
00:05:27,535 --> 00:05:28,700
再作成されたモデルを

113
00:05:28,699 --> 00:05:30,889
load_state_dictで
state_dictにロードします

114
00:05:30,889 --> 00:05:32,389
これでモデルが戻りました

115
00:05:32,389 --> 00:05:35,854
モデルのセーブ リロードトレーニングモデルと同様の

116
00:05:35,855 --> 00:05:38,430
新モデルの作成に成功しました

117
00:05:38,430 --> 00:05:41,504
このメソッド 関数 load_checkpointは

118
00:05:41,504 --> 00:05:43,365
モデルの実装方法と同様

119
00:05:43,365 --> 00:05:45,775
すべて同じload_checkpoint関数を

120
00:05:45,774 --> 00:05:49,009
使用することはできません

121
00:05:49,009 --> 00:05:52,789
モデルアーキテクチャに合わせて

122
00:05:52,790 --> 00:05:54,379
構築する必要があります

123
00:05:54,379 --> 00:05:58,759
それでは また次回！

