1
00:00:00,000 --> 00:00:01,995
皆さん おかえりなさい

2
00:00:01,995 --> 00:00:05,235
この動画および このノートブックでは

3
00:00:05,235 --> 00:00:09,195
PyTorchで ニューラルネットワークを
どのように訓練するかをお見せします

4
00:00:09,195 --> 00:00:15,914
前回は nn.moduleを使って PyTorchで
ニューラルネットワークを定義する方法でした

5
00:00:15,914 --> 00:00:18,269
ここからは 定義したニューラルネットワークの
１つを実際に訓練していきます

6
00:00:18,269 --> 00:00:20,609
訓練とは ニューラルネットワークを
ユニバーサル関数近似器として使っていくということです

7
00:00:20,609 --> 00:00:22,800
つまり 基本的にどの関数でも

8
00:00:22,800 --> 00:00:26,414
望ましい入力というものがあるということです

9
00:00:26,414 --> 00:00:28,274
例えば 数字の４の画像

10
00:00:28,274 --> 00:00:30,375
この関数の望ましい出力があります

11
00:00:30,375 --> 00:00:33,439
この場合は 確率分布で

12
00:00:33,439 --> 00:00:35,839
さまざまな数字の可能性を教えてくれます

13
00:00:35,840 --> 00:00:38,450
この例で

14
00:00:38,450 --> 00:00:40,970
４の画像で渡して

15
00:00:40,969 --> 00:00:43,865
数字の４で確率が高くなっている

16
00:00:43,865 --> 00:00:45,185
確率分布を得たいと考えています

17
00:00:45,185 --> 00:00:46,609
ニューラルネットワークが素晴らしいのは

18
00:00:46,609 --> 00:00:49,250
非線型の活性化関数を使って

19
00:00:49,250 --> 00:00:52,240
正しくラベル付けされた画像の
正しいデータセットを得ている場合

20
00:00:52,240 --> 00:00:54,590
基本的に画像と正しい出力とラベル

21
00:00:54,590 --> 00:00:56,885
もしくは クラスで渡すと

22
00:00:56,884 --> 00:01:02,179
結果的に ニューラルネットワークが
この機能に近いものを構築して

23
00:01:02,179 --> 00:01:06,555
これらの画像を 確率分布に変換します

24
00:01:06,555 --> 00:01:08,665
それが ここでの目標です

25
00:01:08,665 --> 00:01:13,219
私たちが知りたいのはPyTorchにおいて

26
00:01:13,219 --> 00:01:17,969
ニューラルネットワークを構築する方法と
入力と出力を渡す方法

27
00:01:17,969 --> 00:01:21,504
ニューラルネットワークの重みを調整して
この関数で近似を求める方法です

28
00:01:21,504 --> 00:01:26,239
まず必要なのは 損失関数と呼ばれるものです

29
00:01:26,239 --> 00:01:31,314
コストと呼ばれることもあります

30
00:01:31,314 --> 00:01:34,939
これは何かというと
予測誤差を評価するものです

31
00:01:34,939 --> 00:01:37,069
４の画像を渡して

32
00:01:37,069 --> 00:01:40,129
ニューラルネットワークが違うものを予測したら

33
00:01:40,129 --> 00:01:42,994
それが誤差です

34
00:01:42,995 --> 00:01:46,510
ニューラルネットワークの予測と正しい
ラベルとの乖離度を評価したいので

35
00:01:46,510 --> 00:01:52,135
損失関数を使って評価します

36
00:01:52,135 --> 00:01:54,195
この場合 誤差を二乗する道具になります

37
00:01:54,194 --> 00:01:56,954
これを使うのは 回帰問題であることが多く

38
00:01:56,954 --> 00:01:59,959
ここにあるような分類問題では
他の損失関数を使うでしょう

39
00:01:59,959 --> 00:02:04,429
損失は ニューラルネットワークの出力 もしくは

40
00:02:04,430 --> 00:02:07,130
ニューラルネットワークが出す予測に 左右され

41
00:02:07,129 --> 00:02:11,060
ニューラルネットワークの出力は 重みに左右され

42
00:02:11,060 --> 00:02:14,080
ニューラルネットワークの
パラメータなどです

43
00:02:14,080 --> 00:02:15,785
損失を最小化するよう
重みを調整することも可能です

44
00:02:15,784 --> 00:02:20,783
損失が 最小化されると

45
00:02:20,783 --> 00:02:22,754
ニューラルネットワークが 最大限良い予測を
出していると分かります

46
00:02:22,754 --> 00:02:26,729
ニューラルネットワークのパラメータを
調整して 損失を最小化することが全体目標で

47
00:02:26,729 --> 00:02:33,139
勾配降下と呼ばれる
プロセスを使って行います

48
00:02:33,139 --> 00:02:36,349
勾配は パラメータに関する
損失関数の傾きです

49
00:02:36,349 --> 00:02:42,039
勾配は 常に一番変化が速い方向を
指しています

50
00:02:42,039 --> 00:02:45,379
例えば 山があるとします

51
00:02:45,379 --> 00:02:47,180
勾配は 常に山の頂上を指します

52
00:02:47,180 --> 00:02:49,610
損失関数は 高い損失が 頂上にあり

53
00:02:49,610 --> 00:02:52,820
低い損失は 麓にあるこの山で
イメージすることができます

54
00:02:52,819 --> 00:02:56,359
損失を最小化する際は 損失の
最小値を得たいと分かっていますので

55
00:02:56,360 --> 00:03:01,415
下に降りたいと考えます

56
00:03:01,414 --> 00:03:02,824
勾配は 上向きを指すので

57
00:03:02,824 --> 00:03:05,479
私たちは 反対方向に進みます

58
00:03:05,479 --> 00:03:07,099
つまり負の勾配の方向に進んで

59
00:03:07,099 --> 00:03:09,335
その通りに下り続ければ

60
00:03:09,335 --> 00:03:11,540
最終的に山の麓に着きます

61
00:03:11,539 --> 00:03:17,084
つまり 最小損失です

62
00:03:17,085 --> 00:03:18,680
多層ニューラルネットワークで

63
00:03:18,680 --> 00:03:21,395
バックプロパゲーションという アルゴリズムを使います

64
00:03:21,395 --> 00:03:26,015
バックプロパゲーションは 微積分の
連鎖律の応用に過ぎません

65
00:03:26,014 --> 00:03:29,834
何かデータで渡す場合 ニューラルネットワークへ入力し

66
00:03:29,835 --> 00:03:31,465
損失計算のための ニューラルネットワークを
経る このフォワードパスを通過します

67
00:03:31,465 --> 00:03:35,539
いくつかのデータ

68
00:03:35,539 --> 00:03:37,324
特徴ある入力ｘを渡し

69
00:03:37,324 --> 00:03:39,709
重みとバイアスに依存する 線型変換を通過します

70
00:03:39,710 --> 00:03:43,474
その後 シグモイドといった活性化関数を経て

71
00:03:43,474 --> 00:03:46,489
より多くの重みと バイアスを持つ別の線型変換を経て

72
00:03:46,490 --> 00:03:49,820
損失の計算が 可能になります

73
00:03:49,819 --> 00:03:50,930
この重みW₁に 小さな変更を加えた場合

74
00:03:50,930 --> 00:03:52,430
変更は ニューラルネットワーク内で伝わり

75
00:03:52,430 --> 00:03:55,985
最終的に 損失でも小さな変更となります

76
00:03:55,985 --> 00:03:58,910
これは 変更の連鎖と考えることができます

77
00:03:58,909 --> 00:04:02,645
W₁を変更したら L₁が変わります

78
00:04:02,645 --> 00:04:05,330
それはSにも L₂にも ℓまで伝わります

79
00:04:05,330 --> 00:04:07,850
バックプロパゲーションでは 同じ変更を使いますが

80
00:04:07,849 --> 00:04:09,739
逆の方向に進みます

81
00:04:09,740 --> 00:04:12,129
演算それぞれについて

82
00:04:12,129 --> 00:04:15,859
損失やシグモイド活性化関数への
線型変換などは

83
00:04:15,860 --> 00:04:17,830
常に何らかの微分や

84
00:04:17,829 --> 00:04:20,659
出力と入力の間の勾配になります

85
00:04:20,660 --> 00:04:23,600
私たちが行うことは これらの演算間の勾配をとり

86
00:04:23,600 --> 00:04:25,939
ニューラルネットワーク全体で 逆方向に渡すことです

87
00:04:25,939 --> 00:04:29,449
各ステップで入ってくる勾配と
演算自体の勾配を掛けます

88
00:04:29,449 --> 00:04:32,149
例えば 損失がある最後の部分から開始します

89
00:04:32,149 --> 00:04:35,929
この勾配 損失dℓ/dL₂を渡します

90
00:04:35,930 --> 00:04:42,040
これは ２つ目の線型変換に関する損失の勾配で

91
00:04:42,040 --> 00:04:45,650
L₂の出力を再び逆向きにＳに渡して
L₂の損失を掛けた場合

92
00:04:45,649 --> 00:04:48,739
これは線型変換で

93
00:04:48,740 --> 00:04:53,509
活性化関数の出力に関するものなので

94
00:04:53,509 --> 00:05:03,334
L₂の演算の勾配が得られます

95
00:05:03,334 --> 00:05:06,169
L₂の勾配に 損失ℓからの勾配を掛けたら

96
00:05:06,170 --> 00:05:08,840
L₂とℓの両方の全勾配が得られます

97
00:05:08,839 --> 00:05:10,974
この全勾配は ソフトマックス関数へ渡すことが可能です

98
00:05:10,975 --> 00:05:16,625
バックプロパゲーションのプロセス全体としては

99
00:05:16,625 --> 00:05:18,600
勾配をとって 一つ前の演算に
逆方向に渡して

100
00:05:18,600 --> 00:05:21,865
２つの演算の勾配を掛けて

101
00:05:21,865 --> 00:05:25,845
その全勾配を逆向きに渡します

102
00:05:25,845 --> 00:05:28,745
これをニューラルネットワークの中で
演算ごとに続けていくと

103
00:05:28,745 --> 00:05:30,110
結局 重みに戻ります

104
00:05:30,110 --> 00:05:31,879
何をするかというと

105
00:05:31,879 --> 00:05:35,629
重みに関する損失の勾配の計算を
可能にするのです

106
00:05:35,629 --> 00:05:38,165
先にお話ししたように

107
00:05:38,165 --> 00:05:40,985
勾配は 損失の中で一番速く変化する方向を示します

108
00:05:40,985 --> 00:05:44,480
つまり 損失を最大化します

109
00:05:44,480 --> 00:05:46,160
損失を最小化したい場合は

110
00:05:46,160 --> 00:05:52,590
重みから勾配を引くことができます

111
00:05:52,589 --> 00:05:53,894
そうすることで 新しい重みのセットが得られます

112
00:05:53,894 --> 00:05:55,484
通常は より小さな損失につながるでしょう

113
00:05:55,485 --> 00:05:59,819
バックプロパゲーションのアルゴリズムが
機能する方法によって

114
00:05:59,819 --> 00:06:03,110
ニューラルネットワーク全体を フォワードパスにして計算し

115
00:06:03,110 --> 00:06:06,965
損失が得られたら

116
00:06:06,964 --> 00:06:09,769
ニューラルネットワーク全体を
逆向きに進み 勾配を計算すると

117
00:06:09,769 --> 00:06:12,924
重みの勾配が 得られます

118
00:06:12,925 --> 00:06:14,675
そして 重みがアップデートされます

119
00:06:14,675 --> 00:06:17,030
また別のフォワードパスを行って

120
00:06:17,029 --> 00:06:19,174
損失を計算して

121
00:06:19,175 --> 00:06:20,485
バックワードパスを行って

122
00:06:20,485 --> 00:06:21,660
重みをアップデートする

123
00:06:21,660 --> 00:06:24,495
何度も 十分に最小化された
損失が得られるまで繰り返します

124
00:06:24,495 --> 00:06:25,935
勾配が得られたら 先に話したように

125
00:06:25,935 --> 00:06:29,519
重みから勾配を引くことができますが

126
00:06:29,519 --> 00:06:32,149
いわゆる学習率である αを使うこともできます

127
00:06:32,149 --> 00:06:34,219
勾配を増減させる方法に過ぎず

128
00:06:34,220 --> 00:06:38,770
繰り返しのプロセスの中では ステップはあまり大きくしません

129
00:06:38,769 --> 00:06:42,109
ステップを大きくアップデートすると
何が起こりうるかというと

130
00:06:42,110 --> 00:06:45,864
最小値の周りの底で
大きく跳ね回るような形になり

131
00:06:45,863 --> 00:06:48,839
損失の最小値には たどり着けないでしょう

132
00:06:48,839 --> 00:06:51,529
実際に PyTorchでどのように損失を
計算できるか見てみましょう

133
00:06:51,529 --> 00:06:55,250
再び nnモジュールを使って

134
00:06:55,250 --> 00:07:00,009
PyTorchでは 交差エントロピー損失も含めた
さまざまな損失が得られます

135
00:07:00,009 --> 00:07:02,279
損失は 分類問題を解く際に通常使うものです

136
00:07:02,279 --> 00:07:06,904
PyTorchでは 損失を変数criterionに
割り当てることになっています

137
00:07:06,904 --> 00:07:12,264
交差エントロピーを使いたい場合は

138
00:07:12,264 --> 00:07:17,479
criterion = nn.CrossEntropyLossとして
クラスを生成します

139
00:07:17,480 --> 00:07:19,805
ここで 注意すべきことは

140
00:07:19,805 --> 00:07:25,060
CrossEntropyLossの文書を見る場合

141
00:07:25,060 --> 00:07:26,689
分かることは その入力として

142
00:07:26,689 --> 00:07:30,485
ニューラルネットワークのロジットのような
スコアを求めていることです

143
00:07:30,485 --> 00:07:34,730
皆さんは これを良い確率分布をもたらす

144
00:07:34,730 --> 00:07:39,060
ソフトマックス関数のような出力と
一緒に使うでしょう

145
00:07:39,060 --> 00:07:43,310
ただし 計算上の理由で

146
00:07:43,310 --> 00:07:46,405
この損失の入力として
ソフトマックス関数の入力である

147
00:07:46,404 --> 00:07:48,974
ロジットを使った方が通常は良いでしょう

148
00:07:48,975 --> 00:07:51,680
入力には 各クラスのスコアが見込まれています

149
00:07:51,680 --> 00:07:55,704
確率自体ではありません

150
00:07:55,704 --> 00:07:58,834
そこでまず 必要なモジュールをここにインポートして

151
00:07:58,834 --> 00:08:02,239
データのダウンロードも行い

152
00:08:02,240 --> 00:08:06,500
先に見たように trainloaderの中に
生成してデータを取得できるようにします

153
00:08:06,500 --> 00:08:11,370
ここでモデルを定義しています

154
00:08:11,370 --> 00:08:13,410
nn.Sequentialを使ってます

155
00:08:13,410 --> 00:08:15,110
見たことがないという方は 前回の
ノートの一番最後を確認してください

156
00:08:15,110 --> 00:08:17,165
パート２の最後に nn.Sequentialの使い方をお見せします

157
00:08:17,165 --> 00:08:19,910
単純なフィードフォワードネットワークを
より簡潔に定義できる方法で

158
00:08:19,910 --> 00:08:21,860
ここで 単にロジッツと出力関数のスコアを返しているだけで

159
00:08:21,860 --> 00:08:23,139
ソフトマックスの出力そのものは
返していないと気づくでしょう

160
00:08:23,139 --> 00:08:25,844
ここで損失を定義することができます

161
00:08:25,845 --> 00:08:31,300
「criterion = nn.CrossEntropyLoss( )」
としています

162
00:08:31,300 --> 00:08:34,730
データを画像とラベルで得た後 フラット化して

163
00:08:34,730 --> 00:08:41,250
ロジットを得るためにmodel関数を通して

164
00:08:41,250 --> 00:08:42,779
logitsと真のlabelsに渡すことで
実際の損失を得ることができます

165
00:08:42,779 --> 00:08:45,899
ここでもlabelsは trainloaderから得ます

166
00:08:45,899 --> 00:08:48,139
実行すると 計算で出た損失があるのが分かります

167
00:08:48,139 --> 00:08:51,710
私の経験では 通常のソフトマックス関数の
代わりにLogSoftmax関数の出力を使って

168
00:08:51,710 --> 00:08:57,995
自分のモデルを構築した方が
やりやすいと思います

169
00:08:57,995 --> 00:09:00,710
LogSoftmaxの出力で 実際の確率を得るには

170
00:09:00,710 --> 00:09:03,920
torch.expへ渡すだけです

171
00:09:03,919 --> 00:09:06,995
これは指数関数です

172
00:09:06,995 --> 00:09:10,975
LogSoftmax関数の出力と一緒に

173
00:09:10,975 --> 00:09:13,860
負の対数尤度損失 もしくは
nn.NLLLossを使いたいと思うでしょう

174
00:09:13,860 --> 00:09:17,250
ここで やっていただきたいのは

175
00:09:17,250 --> 00:09:19,279
LogSoftmaxを出力として 返すモデルを構築して

176
00:09:19,279 --> 00:09:24,339
負の対数尤度損失を使って 損失を計算することです

177
00:09:24,340 --> 00:09:26,780
LogSoftmaxを使用する際は

178
00:09:26,779 --> 00:09:29,899
キーワード引数のdimに必ず注意してください

179
00:09:29,899 --> 00:09:33,980
dimを間違いなく正しく設定することで
出力が求めているものになります

180
00:09:33,980 --> 00:09:36,095
早速これを試して 私の解を自由にご確認ください

181
00:09:36,095 --> 00:09:40,040
解はノートにあり 困った時は次の動画にもあります

182
00:09:40,039 --> 00:09:45,199
それでは！

