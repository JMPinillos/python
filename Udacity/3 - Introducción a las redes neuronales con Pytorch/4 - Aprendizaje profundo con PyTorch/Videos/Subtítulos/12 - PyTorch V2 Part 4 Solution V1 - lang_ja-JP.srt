1
00:00:00,000 --> 00:00:01,080
こんにちは

2
00:00:01,080 --> 00:00:02,625
前回のビデオでは

3
00:00:02,625 --> 00:00:05,609
独自のニューラル
ネットワークを構築して

4
00:00:05,610 --> 00:00:09,015
このデータセットで
分類してみましょう

5
00:00:09,015 --> 00:00:12,900
方法の決め方など
私の解を紹介します

6
00:00:12,900 --> 00:00:14,685
まず ネットワークを
構築します

7
00:00:14,685 --> 00:00:18,269
ここでは PyTorchから通常の
モジュールをインポートします

8
00:00:18,269 --> 00:00:19,664
nnとoptimについては

9
00:00:19,664 --> 00:00:21,629
nnはネットワークを構築し

10
00:00:21,629 --> 00:00:23,564
optimは最適化手法です

11
00:00:23,565 --> 00:00:26,100
ReLUやlog softmaxなどの
関数を使えるように

12
00:00:26,100 --> 00:00:30,015
この関数モジュールを
インポートします

13
00:00:30,015 --> 00:00:33,810
この分類を使い
ネットワークアーキテクチャを定義します

14
00:00:33,810 --> 00:00:36,150
これをサブクラス化したのが
nn.モジュールで

15
00:00:36,149 --> 00:00:37,649
分類子と呼びます

16
00:00:37,649 --> 00:00:41,269
次に４種類の線型変換を
作成しました

17
00:00:41,270 --> 00:00:45,935
ここでは３つの隠れ層と
１つの出力層を用意しました

18
00:00:45,935 --> 00:00:49,190
第１の隠れ層には
256個のユニットがあり

19
00:00:49,189 --> 00:00:50,989
第２の隠れ層には
128個で

20
00:00:50,990 --> 00:00:52,560
その次は64です

21
00:00:52,560 --> 00:00:55,200
そして 出力は10ユニットです

22
00:00:55,200 --> 00:00:57,330
フォワードパスでは
少しやり方を変えました

23
00:00:57,329 --> 00:01:01,384
入力テンソルがフラット化
されていることを確認したので

24
00:01:01,384 --> 00:01:06,049
トレーニングループで
入力テンソルをフラット化する必要がなく

25
00:01:06,049 --> 00:01:08,334
フォワードパスの中で
処理されることになります

26
00:01:08,334 --> 00:01:10,304
これを行うには x.view.を行い

27
00:01:10,305 --> 00:01:12,645
これで形が変わります

28
00:01:12,644 --> 00:01:16,185
x.shape(0)がバッチサイズ
になります

29
00:01:16,185 --> 00:01:19,460
ここでの負の値は
基本的に

30
00:01:19,459 --> 00:01:21,589
エレメントの合計数を
維持するために

31
00:01:21,590 --> 00:01:24,320
必要な数のエレメントで
２番目の次元を埋めます

32
00:01:24,319 --> 00:01:27,529
これで入力テンソルを
フラット化した

33
00:01:27,530 --> 00:01:30,560
別のテンソルが生成されます

34
00:01:30,560 --> 00:01:33,454
これらを線型変換で処理し

35
00:01:33,454 --> 00:01:36,024
次にReLU活性化関数を実行します

36
00:01:36,025 --> 00:01:40,550
最後に 次元を１に設定した
log softmaxを出力として使用し

37
00:01:40,549 --> 00:01:43,204
forward関数から返しています

38
00:01:43,204 --> 00:01:44,584
モデルが定義されていれば

39
00:01:44,584 --> 00:01:46,969
model=classifierも可能です

40
00:01:46,969 --> 00:01:48,459
これで実際にモデルが
生成されます

41
00:01:48,459 --> 00:01:53,434
次に 負の対数尤度損失で
基準を定義します

42
00:01:53,435 --> 00:01:56,299
そこで モデルの出力として
log softmaxを使用するので

43
00:01:56,299 --> 00:02:00,009
NLLLossを基準にします

44
00:02:00,010 --> 00:02:03,150
ここではAdam最適化を
使用します

45
00:02:03,150 --> 00:02:07,070
これは基本的には
確率的勾配降下法と同じですが

46
00:02:07,069 --> 00:02:10,264
モーメンタムを利用して

47
00:02:10,264 --> 00:02:14,569
フィッティング処理を高速化する
優れた特性があります

48
00:02:14,569 --> 00:02:21,609
また モデルの各パラメータの
学習率を調整します

49
00:02:21,610 --> 00:02:24,465
ここでは
トレーニングループを書き

50
00:02:24,465 --> 00:02:26,674
５つのエポックを使用します

51
00:02:26,674 --> 00:02:28,670
エポックの範囲内のeに対して

52
00:02:28,669 --> 00:02:31,954
データセットを
５回ループさせます

53
00:02:31,955 --> 00:02:35,090
損失はランニングロスで
追跡し

54
00:02:35,090 --> 00:02:36,900
これをインスタンス化します

55
00:02:36,900 --> 00:02:38,564
次に 画像を取得します

56
00:02:38,564 --> 00:02:41,175
train Loaderで画像のラベルから

57
00:02:41,175 --> 00:02:45,635
モデルに画像を渡すことで
ログの確率を得ています

58
00:02:45,634 --> 00:02:48,019
ショートカットができることに
注目してください

59
00:02:48,020 --> 00:02:51,320
これらを関数のようにモデルに
渡すと

60
00:02:51,319 --> 00:02:53,810
forwardメソッドを実行します

61
00:02:53,810 --> 00:02:59,634
モデルのフォワードパスを実行する
ちょっとした早ワザです

62
00:02:59,634 --> 00:03:02,060
そして 対数確率とラベルを使って

63
00:03:02,060 --> 00:03:03,560
損失を計算します

64
00:03:03,560 --> 00:03:06,110
ここでは 勾配を０にします

65
00:03:06,110 --> 00:03:09,485
逆向きに減らして
勾配を計算し

66
00:03:09,485 --> 00:03:11,890
この勾配で 最適化の手順を
実行します

67
00:03:11,889 --> 00:03:14,209
最低でも この最初の５つの
エポックについては

68
00:03:14,210 --> 00:03:17,645
実際に損失が減少していること
がわかります

69
00:03:17,645 --> 00:03:18,860
これでネットワークの学習が
完了しました

70
00:03:18,860 --> 00:03:20,555
テストしてみましょう

71
00:03:20,555 --> 00:03:24,965
つまり データをモデルに渡し
確率を計算します

72
00:03:24,965 --> 00:03:28,129
ここでは
モデルのフォワードパスを行い

73
00:03:28,129 --> 00:03:31,509
実際の対数確率を求めています

74
00:03:31,509 --> 00:03:33,979
指数関数で 実際の確率を
求めることができます

75
00:03:33,979 --> 00:03:38,875
この画像を 私が書いた
view classify 関数に渡せば

76
00:03:38,875 --> 00:03:40,365
例えば

77
00:03:40,365 --> 00:03:42,110
シャツの画像なら

78
00:03:42,110 --> 00:03:44,195
それがシャツだと
教えてくれるようになります

79
00:03:44,194 --> 00:03:47,974
このネットワークは
データセットを

80
00:03:47,974 --> 00:03:50,799
かなり学習していることが
わかります

