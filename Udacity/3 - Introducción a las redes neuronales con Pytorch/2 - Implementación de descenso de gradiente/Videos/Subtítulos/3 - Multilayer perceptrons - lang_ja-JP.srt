1
00:00:00,250 --> 00:00:03,919
XORパーセプトロンを前に確認しました

2
00:00:03,919 --> 00:00:08,289
ユニットの第２層を追加すると

3
00:00:08,289 --> 00:00:13,079
線形分離不可能な問題の解決策を
モデルが見つけられるというものです

4
00:00:13,080 --> 00:00:16,530
ここには３つの入力ユニットのある
複数層パーセプトロンの例があります

5
00:00:16,530 --> 00:00:19,089
１つは出力ユニットで
２つは中央にあります

6
00:00:19,089 --> 00:00:22,960
この中央の層は 隠れ層と呼ばれます

7
00:00:22,960 --> 00:00:27,589
このネットワークの出力の計算は 以前と同じで

8
00:00:27,589 --> 00:00:30,370
隠れ層のアクティベーションが 出力層への
入力として使用されることだけ異なります

9
00:00:30,370 --> 00:00:34,469
隠れ層への入力は 以前と同じで

10
00:00:34,469 --> 00:00:39,329
重み x 入力値 + バイアス項です

11
00:00:39,329 --> 00:00:41,979
これも以前と同様 シグモイド関数のような活性化関数で

12
00:00:41,979 --> 00:00:44,619
隠れ層の出力を計算します

13
00:00:44,619 --> 00:00:46,329
隠れ層のアクティベーションは ２番目の重みのセットを通じて

14
00:00:46,329 --> 00:00:49,799
出力層に受け渡され

15
00:00:49,799 --> 00:00:50,539
再度活性化関数を利用して ネットワークの出力を得ます

16
00:00:50,539 --> 00:00:54,570
より多くの層を重ねると

17
00:00:54,570 --> 00:00:57,500
ネットワークが 複雑なパターンを学習できるようになります

18
00:00:57,500 --> 00:00:58,479
これが 深層学習の名前の由来で

19
00:00:58,479 --> 00:01:00,009
強力である所以です

