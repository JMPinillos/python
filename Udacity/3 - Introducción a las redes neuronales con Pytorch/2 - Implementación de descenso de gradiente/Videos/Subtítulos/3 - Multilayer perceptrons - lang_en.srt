1
00:00:00,000 --> 00:00:02,279
We saw before with the XOR perceptron,

2
00:00:02,279 --> 00:00:05,040
that adding a second layer of units allows the model to find

3
00:00:05,040 --> 00:00:08,085
solutions to linearly inseparable problems.

4
00:00:08,085 --> 00:00:12,795
Here is an example of a multilayer perceptron with three input units,

5
00:00:12,795 --> 00:00:16,305
one output unit, and two units in the middle.

6
00:00:16,305 --> 00:00:18,795
This middle layer is called the hidden layer.

7
00:00:18,795 --> 00:00:22,155
Calculating the output of this network is the same as before,

8
00:00:22,155 --> 00:00:24,555
except that now the activations of the hidden layer

9
00:00:24,555 --> 00:00:27,210
are used as the input to the output layer.

10
00:00:27,210 --> 00:00:30,000
The input to the hidden layer is the same as before,

11
00:00:30,000 --> 00:00:34,155
it's these weights times the input values plus some bias term.

12
00:00:34,155 --> 00:00:36,900
As before, again, you use

13
00:00:36,900 --> 00:00:41,665
an activation function such as a sigmoid to calculate the output of the hidden layer.

14
00:00:41,665 --> 00:00:43,640
The hidden layer activations are passed to

15
00:00:43,640 --> 00:00:46,085
the output layer through the second set of weights.

16
00:00:46,085 --> 00:00:49,655
Again, use an activation function to get the output of the network.

17
00:00:49,655 --> 00:00:54,325
Stacking more and more layers like this helps the network learn more complex patterns.

18
00:00:54,325 --> 00:00:58,280
This is where deep learning gets this name from and what makes it so powerful.

19
00:00:58,280 --> 00:01:01,170
Deep stacks of hidden layers.

