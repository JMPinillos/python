1
00:00:00,750 --> 00:00:05,030
単純なニューラルネットワークから
出力を得る方法は分かりましたね

2
00:00:05,030 --> 00:00:05,910
出力で予測を行いたいのですが

3
00:00:05,910 --> 00:00:09,690
正確な重みが不明でも 予測を行う
ニューラルネットワークを構築するには どうすればよいでしょう？

4
00:00:09,690 --> 00:00:14,030
できることは真だと分かっているデータと提示して

5
00:00:14,030 --> 00:00:17,530
モデルのパラメータである
データに合った重みを設定します

6
00:00:17,530 --> 00:00:21,640
まず 予測がどれほどできていないかの測定が必要です

7
00:00:21,640 --> 00:00:25,330
選択肢は明らかで 真のターゲット値であるｙと

8
00:00:25,330 --> 00:00:28,500
ネットワーク出力 ŷ との差を利用します

9
00:00:28,500 --> 00:00:32,220
予測が高すぎる場合の誤差は 負になり

10
00:00:32,220 --> 00:00:36,090
同じ値で予測が低すぎる場合は 正になります

11
00:00:36,090 --> 00:00:40,270
これらの誤差を同様に扱い

12
00:00:40,270 --> 00:00:42,370
どちらも正にするために 誤差を二乗します

13
00:00:43,420 --> 00:00:46,890
なぜ絶対値を取らないのか 疑問に思うかもしれません

14
00:00:46,890 --> 00:00:50,630
二乗を使う利点は 小さな誤差に比べて
異常値がより不利になることです

15
00:00:50,630 --> 00:00:53,250
誤差を二乗すると 計算がしやすくなります

16
00:00:53,250 --> 00:00:56,020
これは １つの予測の誤差に過ぎません

17
00:00:56,020 --> 00:00:59,690
データセット全体の誤差が 知りたいところです

18
00:00:59,690 --> 00:01:02,455
そこで ミューの合計によって示される

19
00:01:02,455 --> 00:01:06,860
各データレコードの誤差を単純に加算します

20
00:01:06,860 --> 00:01:12,000
μはｕのような形の記号です

21
00:01:13,010 --> 00:01:15,230
これで ネットワークのデータセット全体に
対する全誤差が出ました

22
00:01:15,230 --> 00:01:20,190
最後に 後で計算を整理するため
前に２/１を付けます

23
00:01:20,190 --> 00:01:20,990
この式は 二乗和誤差と呼ばれています

24
00:01:20,990 --> 00:01:25,100
英語の「The Sum of the 
Squared Errors」から SSEとも呼ばれます

25
00:01:25,100 --> 00:01:29,010
思い出してください ŷは重みの線型結合で

26
00:01:29,010 --> 00:01:34,550
入力は 活性化関数を経ています

27
00:01:35,590 --> 00:01:38,910
ŷを置換すると 誤差が重みwᵢと 
入力値xᵢに依存していることが分かります

28
00:01:38,910 --> 00:01:41,780
説明したように データレコードは
ギリシャ文字のμで表されます

29
00:01:41,780 --> 00:01:45,240
データは２つの表や列行列の
どれで考えても構いません

30
00:01:45,240 --> 00:01:48,500
１つは入力データｘを含み

31
00:01:49,580 --> 00:01:53,850
もう１つはターゲットｙを含んでいます

32
00:01:53,850 --> 00:01:57,360
各レコードは ここでは１つの行です

33
00:01:57,360 --> 00:01:59,180
μ = １が最初の行です

34
00:01:59,180 --> 00:02:04,360
次に全誤差を計算するために

35
00:02:04,360 --> 00:02:07,940
これらの列の各行を精査して
SSE(二乗和誤差)を計算します

36
00:02:08,940 --> 00:02:11,160
そして結果を全部合計します

37
00:02:11,160 --> 00:02:15,590
SSEは ネットワーク性能の評価です

38
00:02:15,590 --> 00:02:17,440
高い場合は ネットワークの予測が悪いことになり

39
00:02:18,580 --> 00:02:21,800
低い場合は 予測が良いということになります

40
00:02:21,800 --> 00:02:24,540
ですから できる限り小さくしたいのです

41
00:02:24,540 --> 00:02:27,800
ここからは 誤差を最小にする方法が分かりやすくなるように

42
00:02:27,800 --> 00:02:30,700
データレコードが１つだけの 単純な例で考えてみましょう

43
00:02:30,700 --> 00:02:35,220
単純なネットワークに関して
SSEは ターゲット値から予測を引いた

44
00:02:35,220 --> 00:02:38,620
1/2(y - ŷ)²です

45
00:02:38,620 --> 00:02:43,530
この予測を置換すると

46
00:02:43,530 --> 00:02:46,680
誤差は 重みの関数であることが分かります

47
00:02:46,680 --> 00:02:51,440
重みは ネットワークの予測を変更できる 
いわば調節用のつまみで

48
00:02:51,440 --> 00:02:54,880
誤差全体に影響を与えます

49
00:02:54,880 --> 00:02:57,930
ここでの目標は 誤差を最小化する重みを見つけることです

50
00:02:57,930 --> 00:03:00,919
ここに 重みが１つある誤差の単純な描出があります

51
00:03:01,950 --> 00:03:05,580
目標は ボウルの底の重みを見つけることです

52
00:03:05,580 --> 00:03:08,790
ランダムな重み付けから始め

53
00:03:08,790 --> 00:03:10,340
最小値へと進めたいのです

54
00:03:10,340 --> 00:03:14,130
この方向は 勾配つまり傾きとは逆です

55
00:03:14,130 --> 00:03:18,190
何歩も進むと 常に勾配を下っていきます

56
00:03:18,190 --> 00:03:21,580
最終的に 重みは誤差関数の最小値に達します

57
00:03:21,580 --> 00:03:25,410
これが勾配降下です

58
00:03:25,410 --> 00:03:27,900
重みをアップデートしたいので

59
00:03:27,900 --> 00:03:32,490
新しい重みwᵢは 古い重みのwᵢに
重みのステップを加えたΔwᵢです

60
00:03:32,490 --> 00:03:34,940
このギリシャ文字デルタ(Δ)は 一般的に変化を表します

61
00:03:36,140 --> 00:03:40,070
重みのステップは 勾配

62
00:03:40,070 --> 00:03:43,169
つまり 各重みwᵢに関連する誤差の偏微分に比例しています

63
00:03:43,169 --> 00:03:47,543
任意のスケーリングパラメータに追加でき

64
00:03:47,543 --> 00:03:51,367
勾配降下のステップのサイズが 設定可能になります

65
00:03:51,367 --> 00:03:53,780
これは学習率と呼ばれ ギリシャ文字イータ(η)で表されます

66
00:03:53,780 --> 00:03:56,720
ここでの勾配の計算には 多変数の微積分が必要です

67
00:03:57,910 --> 00:04:01,410
偏微分をやっているので 既にお分かりでしょう

68
00:04:01,410 --> 00:04:04,970
何をしているか理解できなくても 心配ありません

69
00:04:04,970 --> 00:04:08,890
勾配降下と 最終結果の概念を理解する方が

70
00:04:08,890 --> 00:04:12,540
より重要です

71
00:04:12,540 --> 00:04:14,380
復習が必要なら カーン・アカデミーの
多変数微積分のレッスンをお勧めします

72
00:04:14,380 --> 00:04:18,620
皆さんの便宜のためにリンクしておきます

73
00:04:18,620 --> 00:04:21,130
勾配を描き出すと 二乗した誤差の重みに関連する

74
00:04:21,130 --> 00:04:24,030
偏微分が得られます

75
00:04:24,030 --> 00:04:26,970
ネットワーク出力であるŷは 重みの関数です

76
00:04:26,970 --> 00:04:30,330
ここにあるのは 他の関数の関数で

77
00:04:30,330 --> 00:04:33,820
重みに依存しています

78
00:04:33,820 --> 00:04:35,750
微分を計算するために 連鎖律を必要とします

79
00:04:35,750 --> 00:04:39,080
連鎖律について軽くおさらいしましょう

80
00:04:40,210 --> 00:04:43,120
ｚに関連している 関数ｐの微分をとりたいとします

81
00:04:43,120 --> 00:04:46,340
ｐがｚに依存している もう１つの
関数ｑに依存している場合

82
00:04:46,340 --> 00:04:51,150
連鎖律が求めるのは
ｑに対するｐの微分をとることです

83
00:04:51,150 --> 00:04:55,442
それからｚに対するｑの微分を掛けます

84
00:04:55,442 --> 00:04:58,825
これを通常の分数として捉えたいと思います

85
00:04:58,825 --> 00:05:02,879
∂qは分母と分子で消せるので ∂p/∂zに戻ります

86
00:05:02,879 --> 00:05:07,535
これは私達の問題に関係し
ｑは誤差 つまりy - ŷに設定し

87
00:05:07,535 --> 00:05:11,877
ｐを誤差の二乗に設定できます

88
00:05:11,877 --> 00:05:14,811
そしてwᵢに関連する微分をとって

89
00:05:14,811 --> 00:05:18,842
ｑに対するｐの微分が 誤差そのものを返します

90
00:05:18,842 --> 00:05:23,211
指数の２は落とされて1/2を消します

91
00:05:23,211 --> 00:05:27,100
残ったのはwᵢに対する誤差の微分です

92
00:05:27,100 --> 00:05:31,140
ターゲット値ｙは 重みに依存しませんが ŷは依存します

93
00:05:32,290 --> 00:05:36,120
再び連鎖律でマイナス記号が 前に出てきて

94
00:05:37,130 --> 00:05:40,200
残ったのは ŷの微分です

95
00:05:40,200 --> 00:05:43,350
ŷは活性化関数ｈと等価だと 覚えていますか？

96
00:05:43,350 --> 00:05:47,650
ｈは重みと入力値の線型結合です

97
00:05:47,650 --> 00:05:51,810
ŷの微分をとって 連鎖律を再び使います

98
00:05:51,810 --> 00:05:55,520
活性化関数ｈの微分と線型結合の偏微分を

99
00:05:55,520 --> 00:05:59,010
掛けたものが得られます

100
00:05:59,010 --> 00:06:02,390
その和の中で各重みに依存している項が １つだけあります

101
00:06:02,390 --> 00:06:06,310
これを重み１について書き出すと

102
00:06:06,310 --> 00:06:07,090
x1と同じ１番目の項だけが 
重み１に依存していることが分かるでしょう

103
00:06:07,090 --> 00:06:12,410
重み１に対する和の偏微分は 正にx1です

104
00:06:12,410 --> 00:06:17,180
他の項は すべてゼロで

105
00:06:17,180 --> 00:06:19,520
wᵢに対する和の偏微分は 単にxᵢです

106
00:06:19,520 --> 00:06:24,860
最後に これを全部一緒にすると

107
00:06:24,860 --> 00:06:28,420
wᵢに対する誤差の二乗の勾配は

108
00:06:28,420 --> 00:06:31,540
負の誤差に活性化関数ｈの微分を掛けて
入力値xᵢを掛けたものになります

109
00:06:31,540 --> 00:06:37,410
重みのステップは 学習率ηと誤差

110
00:06:37,410 --> 00:06:40,820
活性化微分 入力値を掛けたのものです

111
00:06:40,820 --> 00:06:44,590
便宜上 また後で簡単になるよう

112
00:06:44,590 --> 00:06:48,730
誤差の項を小文字の δとします

113
00:06:48,730 --> 00:06:53,250
誤差に活性化関数微分ｈを掛けたものです

114
00:06:54,360 --> 00:06:58,660
これで重みのアップデートを

115
00:06:58,660 --> 00:07:03,320
 wᵢ＝wᵢ＋ηδxi が
入力ｉの値と書くことができます

116
00:07:04,350 --> 00:07:07,410
複数の出力ユニットに 対応することもあるかもしれません

117
00:07:07,410 --> 00:07:10,820
これは 単体の出力ネットワークからの
構造の積み上げとみなせます

118
00:07:10,820 --> 00:07:14,469
ただし 入力ユニットを
新しい出力ユニットにつないでいます

119
00:07:15,820 --> 00:07:19,940
全誤差には 各出力の
誤差を合計したものが含まれるでしょう

120
00:07:20,770 --> 00:07:25,030
勾配降下のステップは ｊで表された
各出力ユニットの誤差の項を計算することで

121
00:07:25,030 --> 00:07:27,148
複数の出力を持つ
ネットワークまで伸ばすことができます

122
00:07:27,148 --> 00:07:30,213
次は これをどうコードへ落とし込むかをお見せし

123
00:07:30,213 --> 00:07:33,000
PhthonやNumPyで
勾配降下を実装できるようになります

124
00:07:33,000 --> 00:07:35,267

