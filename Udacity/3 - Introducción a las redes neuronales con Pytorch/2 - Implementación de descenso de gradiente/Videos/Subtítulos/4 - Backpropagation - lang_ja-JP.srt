1
00:00:00,590 --> 00:00:03,080
複数の層を取り扱っていて

2
00:00:03,080 --> 00:00:05,950
ネットワークを勾配降下法で トレーニングするとします

3
00:00:05,950 --> 00:00:09,330
出力ノードで誤差を計算する方法は 分かっています

4
00:00:09,330 --> 00:00:11,919
この誤差を勾配降下法で利用して

5
00:00:11,919 --> 00:00:13,539
隠れ層を出力の重みに向けて トレーニングできます

6
00:00:13,539 --> 00:00:15,899
しかし 入力を隠れ重みに向けてトレーニングするには

7
00:00:15,900 --> 00:00:19,690
隠れ層のユニットにより生じた誤差を 知る必要があります

8
00:00:19,690 --> 00:00:22,800
勾配降下法の手順を使用する上で
この誤差をどう見つければ良いでしょうか？

9
00:00:22,800 --> 00:00:26,539
以前は ２乗誤差の微分係数を
利用することで 誤差を見つけました

10
00:00:26,539 --> 00:00:29,769
入力層と出力層の間の重みに関してです

11
00:00:29,769 --> 00:00:33,850
連鎖律で隠れ層で それを行うと 

12
00:00:33,850 --> 00:00:37,050
出力層の誤差に ユニット間の重みを掛けた

13
00:00:37,049 --> 00:00:38,820
ユニットの誤差が見つかります

14
00:00:38,820 --> 00:00:40,460
これは理にかなっています

15
00:00:40,460 --> 00:00:42,789
出力ノードとの強い結びつきのあるユニットが

16
00:00:42,789 --> 00:00:46,089
最終出力への誤差に 一番関わることになるのです

17
00:00:46,090 --> 00:00:48,430
ここでは 誤差 x 重みとなっています

18
00:00:48,429 --> 00:00:51,469
これはネットワークを通じて
入力を伝播するのと同じ方法です

19
00:00:51,469 --> 00:00:54,299
入力 x 複数の層の間の重みです

20
00:00:54,299 --> 00:00:56,599
入力を前に伝播するのではなく

21
00:00:56,600 --> 00:01:00,070
誤差をネットワークを通じて 後ろに伝播しています

22
00:01:00,070 --> 00:01:03,929
ネットワークをフリップし
誤差を入力として利用することで

23
00:01:03,929 --> 00:01:06,340
このプロセスを確認できます

24
00:01:06,340 --> 00:01:08,329
この方法は 誤差逆伝播法と呼ばれます

25
00:01:08,329 --> 00:01:11,450
このプロセスは 層を追加しても同様に機能します

26
00:01:11,450 --> 00:01:14,299
複数の層を通じて 誤差を伝播し続けるのです

27
00:01:14,299 --> 00:01:17,810
誤差逆伝播法は ニューラルネットワーク学習の基礎となります

28
00:01:17,810 --> 00:01:19,629
そのため 深層学習のモデルを構築するにあたり

29
00:01:19,629 --> 00:01:21,439
理解することは 本当に重要です

